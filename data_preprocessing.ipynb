{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess review & summary texts in Amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import glob\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_2_dataframe(path): \n",
    "    df = pd.read_json(path, compression='gzip', lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "    custom_infixes = ['\\.\\.\\.+', '(?<=[0-9])-(?=[0-9])', '[!&:,()]']\n",
    "    infix_re = spacy.util.compile_infix_regex(custom_infixes)\n",
    "\n",
    "    tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab,\n",
    "                                        nlp.Defaults.tokenizer_exceptions,\n",
    "                                        prefix_re.search,\n",
    "                                        suffix_re.search,\n",
    "                                        infix_re.finditer,\n",
    "                                        token_match=None)\n",
    "    return lambda text: tokenizer(text)\n",
    "\n",
    "def process_data_with_spacy(review_data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    return [text_to_seq(s, nlp) for s in review_data]\n",
    "\n",
    "def text_to_seq (s, nlp):\n",
    "    doc = nlp(s)\n",
    "    tokens = []\n",
    "    \n",
    "    for tok in doc:\n",
    "        if not tok.is_stop and not tok.is_punct and not tok.like_url and not tok.like_email:\n",
    "            tokens.append(tok.lemma_.lower().strip() if tok.lemma_ != '-PRON-' else tok.lower_)\n",
    "    return tokens\n",
    "\n",
    "def text_to_text(s, nlp):\n",
    "    return ' '.join(text_to_seq(s, nlp))\n",
    "\n",
    "def process_data_with_spacy_df(df):\n",
    "    \n",
    "    df['reviewTextProc'] = df.apply (lambda row: text_to_text(row['reviewText'], nlp), axis=1)\n",
    "    df['summaryProc'] = df.apply (lambda row: text_to_text(row['summary'], nlp), axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gzip_path = r'D:\\Datasets\\amazon_reviews\\gzips'\n",
    "ds_proc_path = r'D:\\Datasets\\amazon_reviews\\processed'\n",
    "\n",
    "files = [Path(f) for f in glob.glob(ds_gzip_path + r\"\\*.gz\", recursive=False)]\n",
    "files.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import cpu_count, Pool\n",
    " \n",
    "cores = cpu_count() - 2 #Number of CPU cores on your system\n",
    "partitions = cores #Define as many partitions as you want\n",
    " \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    print('DF is splitted to {} partitions'.format(partitions))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=cores) as executor:\n",
    "        data_proc = pd.concat(executor.map(func, data_split))\n",
    "        return data_proc\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing reviews_Kindle_Store_5.json\n",
      "Shape of DF: (982619, 9)\n",
      "DF is splitted to 10 partitions\n",
      "Shape of processed DF: (982619, 11)\n",
      "Processing of reviews_Kindle_Store_5.json is finished\n",
      "Start processing reviews_Home_and_Kitchen_5.json\n",
      "Shape of DF: (551682, 9)\n",
      "DF is splitted to 10 partitions\n",
      "Shape of processed DF: (551682, 11)\n",
      "Processing of reviews_Home_and_Kitchen_5.json is finished\n"
     ]
    }
   ],
   "source": [
    "files = [Path(ds_gzip_path + '\\\\reviews_Kindle_Store_5.json.gz'), Path(ds_gzip_path + '\\\\reviews_Home_and_Kitchen_5.json.gz')]\n",
    "\n",
    "for f in files:\n",
    "    print(\"Start processing \" + f.stem)\n",
    "\n",
    "    df = read_2_dataframe(str(f))\n",
    "    print(\"Shape of DF: \" + str(df.shape))\n",
    "\n",
    "    df_proc = parallelize(df, process_data_with_spacy_df);\n",
    "\n",
    "    print(\"Shape of processed DF: \" + str(df_proc.shape))\n",
    "    df_proc.to_json(ds_proc_path + \"/\" + f.stem)\n",
    "\n",
    "    print(\"Processing of \" + f.stem + \" is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
