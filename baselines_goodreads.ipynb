{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Matrix Factorization & Random Predictor Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all aux imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import csv\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "\n",
    "from helpful_stuff.utils_metrics import precision_recall_at_k_4ds\n",
    "from helpful_stuff.utils_xp_out import write_to_csv, XPDescription, XPResults\n",
    "\n",
    "# random seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# surplrise lib\n",
    "from surprise import Dataset, Reader, SVD, NormalPredictor\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# visualizaion\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "XP_NAME = 'baselines'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and run predictors per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [XPDescription(predictor = NormalPredictor(), label = 'Normal Predictor', nfactors = None),\n",
    "              XPDescription(predictor = SVD(n_factors = 100, lr_all=0.001, n_epochs=300, reg_all=0.1), label = 'SVD-100-300', nfactors = 100),\n",
    "              XPDescription(predictor = SVD(n_factors = 100, lr_all=0.001, n_epochs=350, reg_all=0.1), label = 'SVD-100-350', nfactors = 100),\n",
    "              XPDescription(predictor = SVD(n_factors = 100, lr_all=0.001, n_epochs=300, reg_all=0.1, biased = False), label = 'SVD-100-300-nobias', nfactors = 100),\n",
    "              XPDescription(predictor = SVD(n_factors = 100, lr_all=0.001, n_epochs=350, reg_all=0.1, biased = False), label = 'SVD-100-350-nobias', nfactors = 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_complete_ds(file):\n",
    "    return pd.read_json(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "    k_prec = {}\n",
    "    k_rec = {}\n",
    "\n",
    "    for k in range(0, 200):\n",
    "        precisions, recalls = precision_recall_at_k_4ds(predictions, k=k, threshold=3) \n",
    "        p_mean = np.mean(list(precisions.values()))\n",
    "        r_mean = np.mean(list(recalls.values()))\n",
    "        k_prec[k] = p_mean\n",
    "        k_rec[k] = r_mean\n",
    "        \n",
    "    return rmse, mae, k_prec, k_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for: children\n",
      "Run predictor Normal Predictor\n",
      "Write eval data to file\n",
      "Run predictor SVD-100-300\n",
      "Write eval data to file\n",
      "Run predictor SVD-100-350\n",
      "Write eval data to file\n",
      "Run predictor SVD-100-300-nobias\n",
      "Write eval data to file\n",
      "Run predictor SVD-100-350-nobias\n",
      "Write eval data to file\n"
     ]
    }
   ],
   "source": [
    "parent_path = r'D:/Datasets/goodreads_reviews/processed'\n",
    "files = [Path(f) for f in glob.glob(parent_path  + r\"\\*_interactions_*.json\", recursive=False)]\n",
    "\n",
    "for file in files:\n",
    "    label = ' '.join(file.stem.split('_')[2:])\n",
    "    print('Loading data for: ' + label)              \n",
    "    \n",
    "    complete_df = read_complete_ds(file)\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    ds = Dataset.load_from_df(complete_df[['user_id', 'book_id', 'rating']], reader)\n",
    "    trainset, testset = train_test_split(ds, random_state=42, test_size=0.3)\n",
    "    \n",
    "    for pred in predictors:\n",
    "        print(\"Run predictor %s\" % pred.label)\n",
    "        \n",
    "        pred.predictor.fit(trainset)\n",
    "        predictions = pred.predictor.test(testset)\n",
    "        \n",
    "        print(\"Write eval data to file\")\n",
    "        rmse, mae, k_prec, k_rec = evaluate(predictions)\n",
    "        row = XPResults(dataset=label, xpdata=pred, rmse=rmse, mae=mae, precision=k_prec, recall=k_rec)\n",
    "        write_to_csv(row, 'goodreads', XP_NAME)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = read_complete_ds(Path(r'D:/Datasets/goodreads_reviews/processed/goodreads_interactions_children.json'))\n",
    "trainset, testset = train_test_split(complete_df, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "ds = Dataset.load_from_df(trainset[['user_id', 'book_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing output cache (0 entries)\n"
     ]
    }
   ],
   "source": [
    "%reset out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  16 out of  16 | elapsed: 190.9min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_epochs': [300, 350], 'lr_all': [0.001], \n",
    "              'n_factors':[120, 150], 'reg_all': [0.1, 0.2]}\n",
    "\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=2, n_jobs=4, joblib_verbose=1)\n",
    "grid_search.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': 1.0043392096174895}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr_all': 0.001, 'n_epochs': 350, 'n_factors': 150, 'reg_all': 0.1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1e0ba5c7710>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = ds.build_full_trainset()\n",
    "algo = grid_search.best_estimator['rmse']\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9774895322733043, MAE: 0.7092989302429106\n"
     ]
    }
   ],
   "source": [
    "ts = Dataset.load_from_df(testset[['user_id', 'book_id', 'rating']], reader)\n",
    "predictions = algo.test(ds.construct_testset(ts.raw_ratings))\n",
    "rmse, mae, k_prec, k_rec = evaluate(predictions)\n",
    "print('RMSE: %s, MAE: %s' % (rmse, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat | rmse | rmse best | mae |\n",
    "--- | --- | --- | --- |\n",
    "children| 'lr_all': 0.001, 'n_epochs': 200, 'n_factors': 60, 'reg_all': 0.1 | 0.9838804226761576 | 0.7191162486499061|\n",
    "children (cont) | 'lr_all': 0.001, 'n_epochs': 300, 'n_factors': 100, 'reg_all': 0.1 | 0.9792371157644386 | 0.7116851908236864 |\n",
    "children (cont2) | 'lr_all': 0.001, 'n_epochs': 350, 'n_factors': 150, 'reg_all': 0.1 | 0.9774895322733043 | 0.7092989302429106 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
