{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.sparse import csr_matrix\n",
    "#init random seed\n",
    "np.random.seed(5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json('/home/neopux/UHH/datasets/Video_Games_5_proc.json')\n",
    "df = pd.read_json('D:/Datasets/amazon_reviews/Video_Games_5_proc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTextProc</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[8, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "      <td>instal game struggle game window live bugs).so...</td>\n",
       "      <td>07 9, 2012</td>\n",
       "      <td>A2HD75EMZR8QLN</td>\n",
       "      <td>123</td>\n",
       "      <td>Pay to unlock content? I don't think so.</td>\n",
       "      <td>1341792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "      <td>if like rally car game fun it orient 34;europe...</td>\n",
       "      <td>06 30, 2013</td>\n",
       "      <td>A3UR8NLLY1ZHCX</td>\n",
       "      <td>Alejandro Henao \"Electronic Junky\"</td>\n",
       "      <td>Good rally game</td>\n",
       "      <td>1372550400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  helpful  overall  \\\n",
       "0  0700099867  [8, 12]        1   \n",
       "1  0700099867   [0, 0]        4   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  Installing the game was a struggle (because of...   \n",
       "1  If you like rally cars get this game you will ...   \n",
       "\n",
       "                                      reviewTextProc   reviewTime  \\\n",
       "0  instal game struggle game window live bugs).so...   07 9, 2012   \n",
       "1  if like rally car game fun it orient 34;europe...  06 30, 2013   \n",
       "\n",
       "       reviewerID                        reviewerName  \\\n",
       "0  A2HD75EMZR8QLN                                 123   \n",
       "1  A3UR8NLLY1ZHCX  Alejandro Henao \"Electronic Junky\"   \n",
       "\n",
       "                                    summary  unixReviewTime  \n",
       "0  Pay to unlock content? I don't think so.      1341792000  \n",
       "1                           Good rally game      1372550400  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.3, stratify=df['reviewerID'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_train.groupby('asin').reviewTextProc.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "vectorizer.fit(reviews.values)\n",
    "\n",
    "item_infomation_matrix = vectorizer.transform(reviews.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10668, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(item_infomation_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inspiring', 4598),\n",
       " ('gentle', 3770),\n",
       " ('solar', 8157),\n",
       " ('sixaxis', 8008),\n",
       " ('bracket', 1234),\n",
       " ('walk', 9628),\n",
       " ('matt', 5420),\n",
       " ('tim', 8961),\n",
       " ('by', 1370),\n",
       " ('moving', 5750),\n",
       " ('outfits', 6226),\n",
       " ('tweak', 9200),\n",
       " ('glyph', 3845),\n",
       " ('carl', 1449),\n",
       " ('obey', 6054),\n",
       " ('waist', 9622),\n",
       " ('tournament', 9050),\n",
       " ('orb', 6177),\n",
       " ('ata', 762),\n",
       " ('asphalt', 726)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['benders',\n",
       " 'iterface',\n",
       " 'repeaditive',\n",
       " 'darkly',\n",
       " 'weaknessno',\n",
       " 'lia',\n",
       " 'smallest',\n",
       " 'tghing',\n",
       " 'dreamcastthough',\n",
       " 'uns2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.stop_words_)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_infomation_matrix = np.array(item_infomation_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build rating matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin = CategoricalDtype(sorted(df_train.asin.unique()), ordered=True)\n",
    "rev_id = CategoricalDtype(sorted(df_train.reviewerID.unique()), ordered=True)\n",
    "\n",
    "row_cat = df_train.reviewerID.astype(rev_id).cat\n",
    "col_cat = df_train.asin.astype(asin).cat\n",
    "\n",
    "row = row_cat.codes\n",
    "col = col_cat.codes\n",
    "\n",
    "sparse_matrix = csr_matrix((df_train[\"overall\"].values, (row, col)), \\\n",
    "                           shape=(rev_id.categories.size, asin.categories.size), dtype = 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save matrix by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:/Datasets/amazon_reviews/cdl_item_infomation_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(r'D:/Datasets/amazon_reviews/cdl_rating_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load matrix from pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:/Datasets/amazon_reviews/cdl_item_infomation_matrix.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)  \n",
    "    \n",
    "with open(r'D:/Datasets/amazon_reviews/cdl_rating_matrix.pickle', 'rb') as handle2:\n",
    "    rating_matrix = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix factorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "    def __init__(self , rating_matrix, k):\n",
    "        self.num_u = rating_matrix.shape[0] #5551\n",
    "        self.num_v = rating_matrix.shape[1] #16980\n",
    "        \n",
    "        self.u_lambda = 0.1\n",
    "        self.v_lambda = 0.1#10\n",
    "        \n",
    "        self.k = k #latent維度\n",
    "        self.a = 1\n",
    "        self.b = 0.01\n",
    "        \n",
    "        self.R = np.mat(rating_matrix)\n",
    "        \n",
    "        self.C = np.mat(np.ones(self.R.shape)) * self.b\n",
    "        self.C[np.where(self.R>0)] = self.a\n",
    "        \n",
    "        self.I_U = np.mat(np.eye(self.k) * self.u_lambda)\n",
    "        self.I_V = np.mat(np.eye(self.k) * self.v_lambda)\n",
    "        \n",
    "        self.Q = rating_matrix\n",
    "        self.non_zero_idx = self.Q > 0\n",
    "        \n",
    "        self.W = rating_matrix > 0.5\n",
    "        self.W = self.W.astype(np.float64, copy=False)\n",
    "        \n",
    "        self.lambda_x = 0.1 * np.eye(k)\n",
    "        self.lambda_y = 10.0 * np.eye(k)\n",
    "        \n",
    "        self.n_factors = k\n",
    "        self.m, self.n = self.Q.shape\n",
    "        \n",
    "        self.X = 5 * np.random.rand(self.m, self.n_factors)\n",
    "        self.Y = 5 * np.random.rand(self.n_factors, self.n)\n",
    "        \n",
    "        self.U = np.mat(np.random.normal(0 , 1/self.u_lambda , size=(self.k, self.num_u)))\n",
    "        self.V = np.mat(np.random.normal(0 , 1/self.v_lambda , size=(self.k, self.num_v)))\n",
    "                        \n",
    "\n",
    "    def test(self):\n",
    "        print( ((U_cut*self.R[np.ravel(np.where(self.R[:,j]>0)[1]),j] + self.v_lambda * self.V_sdae[j])).shape)\n",
    "        \n",
    "    \n",
    "    def ALS(self , V_sdae):\n",
    "        self.V_sdae = np.mat(V_sdae)\n",
    "        \n",
    "        V_sq = self.V * self.V.T * self.b\n",
    "        for i in range(self.num_u):\n",
    "            idx_a = np.ravel(np.where(self.R[i,:]>0)[1])\n",
    "            V_cut = self.V[:,idx_a]\n",
    "            self.U[:,i] = np.linalg.pinv(V_sq + V_cut * V_cut.T * (self.a-self.b) + self.I_U )*(V_cut*self.R[i,idx_a].T) #V_sq+V_cut*V_cut.T*a_m_b = VCV^T\n",
    "        \n",
    "        U_sq = self.U * self.U.T * self.b\n",
    "        for j in range(self.num_v):\n",
    "            idx_a = np.ravel(np.where(self.R[:,j]>0)[1])\n",
    "            U_cut = self.U[:,idx_a]\n",
    "            self.V[:,j] = np.linalg.pinv(U_sq +  U_cut * U_cut.T * (self.a-self.b)+self.I_V) * (U_cut*self.R[idx_a,j] + self.v_lambda * np.resize(self.V_sdae[j],(self.k,1)))\n",
    "        \n",
    "        return self.U ,self.V\n",
    "    \n",
    "    \n",
    "    def ALS_v2(self, V_sdae):\n",
    "        self.V_sdae = np.mat(V_sdae)\n",
    "        \n",
    "        for i in range(0, self.num_u):\n",
    "            #idx = nonZero[i,:]\n",
    "            idx_a = np.ravel(np.where(self.R[i,:]>0)[1])\n",
    "            #a = Y[idx,]\n",
    "            V_cut = self.V[:,idx_a]\n",
    "            \n",
    "            #b = np.dot(np.transpose(Y[idx,]), ratingsMatrix[i, idx])\n",
    "            b =  V_cut * self.R[i,idx_a].T\n",
    "            updateU = np.linalg.solve((V_cut * V_cut.T + self.I_U),  b)\n",
    "            #print(updateU)\n",
    "            self.U[:,i] = updateU\n",
    "    \n",
    "        for j in range(0, self.num_v):\n",
    "            #idx = nonZero[:,j]\n",
    "            idx_a = np.ravel(np.where(self.R[:,j]>0)[1])\n",
    "            \n",
    "            #a = X[idx,]\n",
    "            U_cut = self.U[:,idx_a]\n",
    "            \n",
    "            #b = np.dot(np.transpose(X[idx,]), ratingsMatrix[idx, j])\n",
    "            b = U_cut*self.R[idx_a,j] #+ self.v_lambda * np.resize(self.V_sdae[j],(self.k,1))\n",
    "            \n",
    "            updateV = np.linalg.solve((U_cut * U_cut.T +self.I_V), b)\n",
    "            self.V[:,j] = updateV\n",
    "                                                                  \n",
    "        return self.U ,self.V\n",
    "    \n",
    "    def ALS_v3_weighted(self, V_sdae, print_loss):\n",
    "        #print(\"ALS step \")\n",
    "        for u in range(self.W.shape[0]):\n",
    "            Wu = self.W[u]            \n",
    "            c = self.Y * Wu\n",
    "            \n",
    "            #print(\"Y\" + str(self.Y.shape))\n",
    "            #print(\"W_diag\" + str(Wu_diag.shape))\n",
    "            #print(\"Q_u\" + str(self.Q[u].shape))\n",
    "            \n",
    "            a = np.matmul(c, np.transpose(self.Q[u]))\n",
    "            d = np.matmul(c, np.transpose(self.Y)) + self.lambda_x\n",
    "            \n",
    "            #print(\"C:\" + str(c.shape))\n",
    "            #print(\"A:\" + str(a.shape))\n",
    "            #print(\"D:\" + str(d.shape))\n",
    "            \n",
    "            self.X[u] = np.linalg.solve(d, a).T\n",
    "            \n",
    "            #if u % 1000 == 0:\n",
    "            #    print(\"WOOOOOOOOOOOOOOOOOOOOOOOOOP:\" ,u)\n",
    "        \n",
    "        #print(\"ALS step 2\")    \n",
    "        for i in range(self.W.shape[1]):\n",
    "            Wi = self.W.T[i]\n",
    "            c = self.X.T * Wi\n",
    "            \n",
    "            a = np.matmul(c, self.Q[:, i]) \n",
    "            b = np.matmul(self.lambda_y, V_sdae[i])\n",
    "            \n",
    "            #print(\"B\", b.shape)\n",
    "            #print(\"A\", a.shape)\n",
    "            self.Y[:,i] = np.linalg.solve(np.matmul(c, self.X) + self.lambda_y, a + b)\n",
    "            \n",
    "        preds = np.dot(self.X, self.Y)\n",
    "        err = mean_squared_error(self.Q[self.non_zero_idx], preds[self.non_zero_idx]) ** 0.5\n",
    "           \n",
    "        print(\"ALS LOSS: %f\" % err)    \n",
    "        del preds\n",
    "        \n",
    "        return self.X, self.Y, err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 7],\n",
       "       [1, 5, 2, 1],\n",
       "       [1, 7, 2, 1],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rm > 0.5\n",
    "W = W.astype(np.float64, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.mat(5 * np.random.rand(4, 3))\n",
    "Y = np.mat(5 * np.random.rand(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wu = W[1]\n",
    "Wu_diag = np.mat(np.diag(Wu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04955666, 2.4480058 , 2.42686456, 4.29286235],\n",
       "       [2.24025944, 0.80691231, 1.8383078 , 1.93557249],\n",
       "       [1.51707224, 1.24571824, 2.71906757, 3.17993   ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Y) * np.array(Wu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.04955666, 2.4480058 , 2.42686456, 4.29286235],\n",
       "        [2.24025944, 0.80691231, 1.8383078 , 1.93557249],\n",
       "        [1.51707224, 1.24571824, 2.71906757, 3.17993   ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y * Wu_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7, 0.8, 0.9],\n",
       "       [1. , 0.2, 0.3],\n",
       "       [0.5, 0.6, 0.7]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (4, 3)\n",
      "Noising completed..:(4, 3)\n",
      "1 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 2.720730\n",
      "EPOCH 1 MODEL LOSS 110.523331\n",
      "2 / 1000\n",
      "ALS step \n",
      "EPOCH 2 MODEL LOSS 104.084976\n",
      "3 / 1000\n",
      "ALS step \n",
      "EPOCH 3 MODEL LOSS 99.165581\n",
      "4 / 1000\n",
      "ALS step \n",
      "EPOCH 4 MODEL LOSS 95.199127\n",
      "5 / 1000\n",
      "ALS step \n",
      "EPOCH 5 MODEL LOSS 91.383072\n",
      "6 / 1000\n",
      "ALS step \n",
      "EPOCH 6 MODEL LOSS 87.388420\n",
      "7 / 1000\n",
      "ALS step \n",
      "EPOCH 7 MODEL LOSS 83.545349\n",
      "8 / 1000\n",
      "ALS step \n",
      "EPOCH 8 MODEL LOSS 80.878616\n",
      "9 / 1000\n",
      "ALS step \n",
      "EPOCH 9 MODEL LOSS 77.144592\n",
      "10 / 1000\n",
      "ALS step \n",
      "EPOCH 10 MODEL LOSS 73.642151\n",
      "11 / 1000\n",
      "ALS step \n",
      "EPOCH 11 MODEL LOSS 70.703026\n",
      "12 / 1000\n",
      "ALS step \n",
      "EPOCH 12 MODEL LOSS 68.055199\n",
      "13 / 1000\n",
      "ALS step \n",
      "EPOCH 13 MODEL LOSS 65.062660\n",
      "14 / 1000\n",
      "ALS step \n",
      "EPOCH 14 MODEL LOSS 62.779819\n",
      "15 / 1000\n",
      "ALS step \n",
      "EPOCH 15 MODEL LOSS 60.966976\n",
      "16 / 1000\n",
      "ALS step \n",
      "EPOCH 16 MODEL LOSS 57.891499\n",
      "17 / 1000\n",
      "ALS step \n",
      "EPOCH 17 MODEL LOSS 56.504326\n",
      "18 / 1000\n",
      "ALS step \n",
      "EPOCH 18 MODEL LOSS 53.332420\n",
      "19 / 1000\n",
      "ALS step \n",
      "EPOCH 19 MODEL LOSS 51.142727\n",
      "20 / 1000\n",
      "ALS step \n",
      "EPOCH 20 MODEL LOSS 49.458061\n",
      "21 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.377251\n",
      "EPOCH 21 MODEL LOSS 47.469746\n",
      "22 / 1000\n",
      "ALS step \n",
      "EPOCH 22 MODEL LOSS 46.042530\n",
      "23 / 1000\n",
      "ALS step \n",
      "EPOCH 23 MODEL LOSS 43.782978\n",
      "24 / 1000\n",
      "ALS step \n",
      "EPOCH 24 MODEL LOSS 41.790123\n",
      "25 / 1000\n",
      "ALS step \n",
      "EPOCH 25 MODEL LOSS 40.569592\n",
      "26 / 1000\n",
      "ALS step \n",
      "EPOCH 26 MODEL LOSS 39.002018\n",
      "27 / 1000\n",
      "ALS step \n",
      "EPOCH 27 MODEL LOSS 37.018990\n",
      "28 / 1000\n",
      "ALS step \n",
      "EPOCH 28 MODEL LOSS 35.811611\n",
      "29 / 1000\n",
      "ALS step \n",
      "EPOCH 29 MODEL LOSS 34.564823\n",
      "30 / 1000\n",
      "ALS step \n",
      "EPOCH 30 MODEL LOSS 32.879417\n",
      "31 / 1000\n",
      "ALS step \n",
      "EPOCH 31 MODEL LOSS 32.317265\n",
      "32 / 1000\n",
      "ALS step \n",
      "EPOCH 32 MODEL LOSS 30.482315\n",
      "33 / 1000\n",
      "ALS step \n",
      "EPOCH 33 MODEL LOSS 29.380312\n",
      "34 / 1000\n",
      "ALS step \n",
      "EPOCH 34 MODEL LOSS 29.160936\n",
      "35 / 1000\n",
      "ALS step \n",
      "EPOCH 35 MODEL LOSS 27.295090\n",
      "36 / 1000\n",
      "ALS step \n",
      "EPOCH 36 MODEL LOSS 26.842319\n",
      "37 / 1000\n",
      "ALS step \n",
      "EPOCH 37 MODEL LOSS 25.315714\n",
      "38 / 1000\n",
      "ALS step \n",
      "EPOCH 38 MODEL LOSS 24.645725\n",
      "39 / 1000\n",
      "ALS step \n",
      "EPOCH 39 MODEL LOSS 23.604116\n",
      "40 / 1000\n",
      "ALS step \n",
      "EPOCH 40 MODEL LOSS 22.557579\n",
      "41 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.371160\n",
      "EPOCH 41 MODEL LOSS 21.541336\n",
      "42 / 1000\n",
      "ALS step \n",
      "EPOCH 42 MODEL LOSS 21.000751\n",
      "43 / 1000\n",
      "ALS step \n",
      "EPOCH 43 MODEL LOSS 20.499392\n",
      "44 / 1000\n",
      "ALS step \n",
      "EPOCH 44 MODEL LOSS 19.778334\n",
      "45 / 1000\n",
      "ALS step \n",
      "EPOCH 45 MODEL LOSS 18.604059\n",
      "46 / 1000\n",
      "ALS step \n",
      "EPOCH 46 MODEL LOSS 18.398346\n",
      "47 / 1000\n",
      "ALS step \n",
      "EPOCH 47 MODEL LOSS 17.452271\n",
      "48 / 1000\n",
      "ALS step \n",
      "EPOCH 48 MODEL LOSS 16.905134\n",
      "49 / 1000\n",
      "ALS step \n",
      "EPOCH 49 MODEL LOSS 16.253574\n",
      "50 / 1000\n",
      "ALS step \n",
      "EPOCH 50 MODEL LOSS 15.923725\n",
      "51 / 1000\n",
      "ALS step \n",
      "EPOCH 51 MODEL LOSS 15.416773\n",
      "52 / 1000\n",
      "ALS step \n",
      "EPOCH 52 MODEL LOSS 14.688913\n",
      "53 / 1000\n",
      "ALS step \n",
      "EPOCH 53 MODEL LOSS 14.125399\n",
      "54 / 1000\n",
      "ALS step \n",
      "EPOCH 54 MODEL LOSS 13.977396\n",
      "55 / 1000\n",
      "ALS step \n",
      "EPOCH 55 MODEL LOSS 13.256073\n",
      "56 / 1000\n",
      "ALS step \n",
      "EPOCH 56 MODEL LOSS 12.708852\n",
      "57 / 1000\n",
      "ALS step \n",
      "EPOCH 57 MODEL LOSS 12.013138\n",
      "58 / 1000\n",
      "ALS step \n",
      "EPOCH 58 MODEL LOSS 12.149158\n",
      "59 / 1000\n",
      "ALS step \n",
      "EPOCH 59 MODEL LOSS 11.448735\n",
      "60 / 1000\n",
      "ALS step \n",
      "EPOCH 60 MODEL LOSS 11.328242\n",
      "61 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.318521\n",
      "EPOCH 61 MODEL LOSS 11.185615\n",
      "62 / 1000\n",
      "ALS step \n",
      "EPOCH 62 MODEL LOSS 10.787792\n",
      "63 / 1000\n",
      "ALS step \n",
      "EPOCH 63 MODEL LOSS 10.060995\n",
      "64 / 1000\n",
      "ALS step \n",
      "EPOCH 64 MODEL LOSS 10.370254\n",
      "65 / 1000\n",
      "ALS step \n",
      "EPOCH 65 MODEL LOSS 9.947740\n",
      "66 / 1000\n",
      "ALS step \n",
      "EPOCH 66 MODEL LOSS 9.548116\n",
      "67 / 1000\n",
      "ALS step \n",
      "EPOCH 67 MODEL LOSS 8.957287\n",
      "68 / 1000\n",
      "ALS step \n",
      "EPOCH 68 MODEL LOSS 8.959795\n",
      "69 / 1000\n",
      "ALS step \n",
      "EPOCH 69 MODEL LOSS 8.749756\n",
      "70 / 1000\n",
      "ALS step \n",
      "EPOCH 70 MODEL LOSS 8.570539\n",
      "71 / 1000\n",
      "ALS step \n",
      "EPOCH 71 MODEL LOSS 8.719908\n",
      "72 / 1000\n",
      "ALS step \n",
      "EPOCH 72 MODEL LOSS 7.986979\n",
      "73 / 1000\n",
      "ALS step \n",
      "EPOCH 73 MODEL LOSS 8.003615\n",
      "74 / 1000\n",
      "ALS step \n",
      "EPOCH 74 MODEL LOSS 7.557094\n",
      "75 / 1000\n",
      "ALS step \n",
      "EPOCH 75 MODEL LOSS 8.463417\n",
      "76 / 1000\n",
      "ALS step \n",
      "EPOCH 76 MODEL LOSS 7.064744\n",
      "77 / 1000\n",
      "ALS step \n",
      "EPOCH 77 MODEL LOSS 6.593536\n",
      "78 / 1000\n",
      "ALS step \n",
      "EPOCH 78 MODEL LOSS 6.444678\n",
      "79 / 1000\n",
      "ALS step \n",
      "EPOCH 79 MODEL LOSS 6.262705\n",
      "80 / 1000\n",
      "ALS step \n",
      "EPOCH 80 MODEL LOSS 6.612082\n",
      "81 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.331938\n",
      "EPOCH 81 MODEL LOSS 5.655221\n",
      "82 / 1000\n",
      "ALS step \n",
      "EPOCH 82 MODEL LOSS 5.474995\n",
      "83 / 1000\n",
      "ALS step \n",
      "EPOCH 83 MODEL LOSS 6.544558\n",
      "84 / 1000\n",
      "ALS step \n",
      "EPOCH 84 MODEL LOSS 5.742662\n",
      "85 / 1000\n",
      "ALS step \n",
      "EPOCH 85 MODEL LOSS 5.487506\n",
      "86 / 1000\n",
      "ALS step \n",
      "EPOCH 86 MODEL LOSS 5.170355\n",
      "87 / 1000\n",
      "ALS step \n",
      "EPOCH 87 MODEL LOSS 5.206652\n",
      "88 / 1000\n",
      "ALS step \n",
      "EPOCH 88 MODEL LOSS 4.909816\n",
      "89 / 1000\n",
      "ALS step \n",
      "EPOCH 89 MODEL LOSS 4.452968\n",
      "90 / 1000\n",
      "ALS step \n",
      "EPOCH 90 MODEL LOSS 4.408383\n",
      "91 / 1000\n",
      "ALS step \n",
      "EPOCH 91 MODEL LOSS 4.572633\n",
      "92 / 1000\n",
      "ALS step \n",
      "EPOCH 92 MODEL LOSS 4.491076\n",
      "93 / 1000\n",
      "ALS step \n",
      "EPOCH 93 MODEL LOSS 4.167543\n",
      "94 / 1000\n",
      "ALS step \n",
      "EPOCH 94 MODEL LOSS 4.369862\n",
      "95 / 1000\n",
      "ALS step \n",
      "EPOCH 95 MODEL LOSS 3.805291\n",
      "96 / 1000\n",
      "ALS step \n",
      "EPOCH 96 MODEL LOSS 3.891722\n",
      "97 / 1000\n",
      "ALS step \n",
      "EPOCH 97 MODEL LOSS 3.928315\n",
      "98 / 1000\n",
      "ALS step \n",
      "EPOCH 98 MODEL LOSS 3.981077\n",
      "99 / 1000\n",
      "ALS step \n",
      "EPOCH 99 MODEL LOSS 4.157866\n",
      "100 / 1000\n",
      "ALS step \n",
      "EPOCH 100 MODEL LOSS 3.883935\n",
      "101 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.290150\n",
      "EPOCH 101 MODEL LOSS 4.082802\n",
      "102 / 1000\n",
      "ALS step \n",
      "EPOCH 102 MODEL LOSS 3.499636\n",
      "103 / 1000\n",
      "ALS step \n",
      "EPOCH 103 MODEL LOSS 3.146079\n",
      "104 / 1000\n",
      "ALS step \n",
      "EPOCH 104 MODEL LOSS 3.143296\n",
      "105 / 1000\n",
      "ALS step \n",
      "EPOCH 105 MODEL LOSS 2.958345\n",
      "106 / 1000\n",
      "ALS step \n",
      "EPOCH 106 MODEL LOSS 3.098881\n",
      "107 / 1000\n",
      "ALS step \n",
      "EPOCH 107 MODEL LOSS 3.726945\n",
      "108 / 1000\n",
      "ALS step \n",
      "EPOCH 108 MODEL LOSS 2.819151\n",
      "109 / 1000\n",
      "ALS step \n",
      "EPOCH 109 MODEL LOSS 3.015884\n",
      "110 / 1000\n",
      "ALS step \n",
      "EPOCH 110 MODEL LOSS 3.244829\n",
      "111 / 1000\n",
      "ALS step \n",
      "EPOCH 111 MODEL LOSS 2.895518\n",
      "112 / 1000\n",
      "ALS step \n",
      "EPOCH 112 MODEL LOSS 3.432925\n",
      "113 / 1000\n",
      "ALS step \n",
      "EPOCH 113 MODEL LOSS 2.562412\n",
      "114 / 1000\n",
      "ALS step \n",
      "EPOCH 114 MODEL LOSS 3.503748\n",
      "115 / 1000\n",
      "ALS step \n",
      "EPOCH 115 MODEL LOSS 2.429785\n",
      "116 / 1000\n",
      "ALS step \n",
      "EPOCH 116 MODEL LOSS 3.088339\n",
      "117 / 1000\n",
      "ALS step \n",
      "EPOCH 117 MODEL LOSS 3.015676\n",
      "118 / 1000\n",
      "ALS step \n",
      "EPOCH 118 MODEL LOSS 2.765448\n",
      "119 / 1000\n",
      "ALS step \n",
      "EPOCH 119 MODEL LOSS 2.548246\n",
      "120 / 1000\n",
      "ALS step \n",
      "EPOCH 120 MODEL LOSS 2.851089\n",
      "121 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.359884\n",
      "EPOCH 121 MODEL LOSS 3.323623\n",
      "122 / 1000\n",
      "ALS step \n",
      "EPOCH 122 MODEL LOSS 2.669001\n",
      "123 / 1000\n",
      "ALS step \n",
      "EPOCH 123 MODEL LOSS 2.318737\n",
      "124 / 1000\n",
      "ALS step \n",
      "EPOCH 124 MODEL LOSS 2.389221\n",
      "125 / 1000\n",
      "ALS step \n",
      "EPOCH 125 MODEL LOSS 2.208810\n",
      "126 / 1000\n",
      "ALS step \n",
      "EPOCH 126 MODEL LOSS 2.387659\n",
      "127 / 1000\n",
      "ALS step \n",
      "EPOCH 127 MODEL LOSS 2.192710\n",
      "128 / 1000\n",
      "ALS step \n",
      "EPOCH 128 MODEL LOSS 2.054234\n",
      "129 / 1000\n",
      "ALS step \n",
      "EPOCH 129 MODEL LOSS 2.256755\n",
      "130 / 1000\n",
      "ALS step \n",
      "EPOCH 130 MODEL LOSS 2.107214\n",
      "131 / 1000\n",
      "ALS step \n",
      "EPOCH 131 MODEL LOSS 2.567064\n",
      "132 / 1000\n",
      "ALS step \n",
      "EPOCH 132 MODEL LOSS 2.626430\n",
      "133 / 1000\n",
      "ALS step \n",
      "EPOCH 133 MODEL LOSS 2.350488\n",
      "134 / 1000\n",
      "ALS step \n",
      "EPOCH 134 MODEL LOSS 2.543088\n",
      "135 / 1000\n",
      "ALS step \n",
      "EPOCH 135 MODEL LOSS 1.775317\n",
      "136 / 1000\n",
      "ALS step \n",
      "EPOCH 136 MODEL LOSS 2.051899\n",
      "137 / 1000\n",
      "ALS step \n",
      "EPOCH 137 MODEL LOSS 1.801892\n",
      "138 / 1000\n",
      "ALS step \n",
      "EPOCH 138 MODEL LOSS 1.626646\n",
      "139 / 1000\n",
      "ALS step \n",
      "EPOCH 139 MODEL LOSS 2.355982\n",
      "140 / 1000\n",
      "ALS step \n",
      "EPOCH 140 MODEL LOSS 2.194083\n",
      "141 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.328719\n",
      "EPOCH 141 MODEL LOSS 1.961141\n",
      "142 / 1000\n",
      "ALS step \n",
      "EPOCH 142 MODEL LOSS 2.057308\n",
      "143 / 1000\n",
      "ALS step \n",
      "EPOCH 143 MODEL LOSS 1.694936\n",
      "144 / 1000\n",
      "ALS step \n",
      "EPOCH 144 MODEL LOSS 2.076997\n",
      "145 / 1000\n",
      "ALS step \n",
      "EPOCH 145 MODEL LOSS 2.026763\n",
      "146 / 1000\n",
      "ALS step \n",
      "EPOCH 146 MODEL LOSS 1.834369\n",
      "147 / 1000\n",
      "ALS step \n",
      "EPOCH 147 MODEL LOSS 2.173652\n",
      "148 / 1000\n",
      "ALS step \n",
      "EPOCH 148 MODEL LOSS 2.029472\n",
      "149 / 1000\n",
      "ALS step \n",
      "EPOCH 149 MODEL LOSS 2.830424\n",
      "150 / 1000\n",
      "ALS step \n",
      "EPOCH 150 MODEL LOSS 1.847197\n",
      "151 / 1000\n",
      "ALS step \n",
      "EPOCH 151 MODEL LOSS 1.941508\n",
      "152 / 1000\n",
      "ALS step \n",
      "EPOCH 152 MODEL LOSS 1.818981\n",
      "153 / 1000\n",
      "ALS step \n",
      "EPOCH 153 MODEL LOSS 1.931707\n",
      "154 / 1000\n",
      "ALS step \n",
      "EPOCH 154 MODEL LOSS 1.502399\n",
      "155 / 1000\n",
      "ALS step \n",
      "EPOCH 155 MODEL LOSS 2.172720\n",
      "156 / 1000\n",
      "ALS step \n",
      "EPOCH 156 MODEL LOSS 1.495121\n",
      "157 / 1000\n",
      "ALS step \n",
      "EPOCH 157 MODEL LOSS 1.504948\n",
      "158 / 1000\n",
      "ALS step \n",
      "EPOCH 158 MODEL LOSS 1.450250\n",
      "159 / 1000\n",
      "ALS step \n",
      "EPOCH 159 MODEL LOSS 1.446738\n",
      "160 / 1000\n",
      "ALS step \n",
      "EPOCH 160 MODEL LOSS 1.549824\n",
      "161 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.348262\n",
      "EPOCH 161 MODEL LOSS 1.867159\n",
      "162 / 1000\n",
      "ALS step \n",
      "EPOCH 162 MODEL LOSS 1.585739\n",
      "163 / 1000\n",
      "ALS step \n",
      "EPOCH 163 MODEL LOSS 1.633071\n",
      "164 / 1000\n",
      "ALS step \n",
      "EPOCH 164 MODEL LOSS 1.489363\n",
      "165 / 1000\n",
      "ALS step \n",
      "EPOCH 165 MODEL LOSS 1.733550\n",
      "166 / 1000\n",
      "ALS step \n",
      "EPOCH 166 MODEL LOSS 1.739524\n",
      "167 / 1000\n",
      "ALS step \n",
      "EPOCH 167 MODEL LOSS 2.026929\n",
      "168 / 1000\n",
      "ALS step \n",
      "EPOCH 168 MODEL LOSS 1.609592\n",
      "169 / 1000\n",
      "ALS step \n",
      "EPOCH 169 MODEL LOSS 1.520323\n",
      "170 / 1000\n",
      "ALS step \n",
      "EPOCH 170 MODEL LOSS 1.808200\n",
      "171 / 1000\n",
      "ALS step \n",
      "EPOCH 171 MODEL LOSS 1.710552\n",
      "172 / 1000\n",
      "ALS step \n",
      "EPOCH 172 MODEL LOSS 1.352360\n",
      "173 / 1000\n",
      "ALS step \n",
      "EPOCH 173 MODEL LOSS 1.920097\n",
      "174 / 1000\n",
      "ALS step \n",
      "EPOCH 174 MODEL LOSS 1.593596\n",
      "175 / 1000\n",
      "ALS step \n",
      "EPOCH 175 MODEL LOSS 1.498221\n",
      "176 / 1000\n",
      "ALS step \n",
      "EPOCH 176 MODEL LOSS 1.447264\n",
      "177 / 1000\n",
      "ALS step \n",
      "EPOCH 177 MODEL LOSS 1.225645\n",
      "178 / 1000\n",
      "ALS step \n",
      "EPOCH 178 MODEL LOSS 1.616928\n",
      "179 / 1000\n",
      "ALS step \n",
      "EPOCH 179 MODEL LOSS 1.722343\n",
      "180 / 1000\n",
      "ALS step \n",
      "EPOCH 180 MODEL LOSS 1.250358\n",
      "181 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.343668\n",
      "EPOCH 181 MODEL LOSS 1.310641\n",
      "182 / 1000\n",
      "ALS step \n",
      "EPOCH 182 MODEL LOSS 1.679918\n",
      "183 / 1000\n",
      "ALS step \n",
      "EPOCH 183 MODEL LOSS 1.768138\n",
      "184 / 1000\n",
      "ALS step \n",
      "EPOCH 184 MODEL LOSS 1.261100\n",
      "185 / 1000\n",
      "ALS step \n",
      "EPOCH 185 MODEL LOSS 1.301060\n",
      "186 / 1000\n",
      "ALS step \n",
      "EPOCH 186 MODEL LOSS 1.493775\n",
      "187 / 1000\n",
      "ALS step \n",
      "EPOCH 187 MODEL LOSS 1.494167\n",
      "188 / 1000\n",
      "ALS step \n",
      "EPOCH 188 MODEL LOSS 1.527565\n",
      "189 / 1000\n",
      "ALS step \n",
      "EPOCH 189 MODEL LOSS 1.634872\n",
      "190 / 1000\n",
      "ALS step \n",
      "EPOCH 190 MODEL LOSS 1.565742\n",
      "191 / 1000\n",
      "ALS step \n",
      "EPOCH 191 MODEL LOSS 1.211771\n",
      "192 / 1000\n",
      "ALS step \n",
      "EPOCH 192 MODEL LOSS 1.502498\n",
      "193 / 1000\n",
      "ALS step \n",
      "EPOCH 193 MODEL LOSS 1.218423\n",
      "194 / 1000\n",
      "ALS step \n",
      "EPOCH 194 MODEL LOSS 1.378268\n",
      "195 / 1000\n",
      "ALS step \n",
      "EPOCH 195 MODEL LOSS 1.525782\n",
      "196 / 1000\n",
      "ALS step \n",
      "EPOCH 196 MODEL LOSS 1.176109\n",
      "197 / 1000\n",
      "ALS step \n",
      "EPOCH 197 MODEL LOSS 1.461358\n",
      "198 / 1000\n",
      "ALS step \n",
      "EPOCH 198 MODEL LOSS 1.173852\n",
      "199 / 1000\n",
      "ALS step \n",
      "EPOCH 199 MODEL LOSS 1.556868\n",
      "200 / 1000\n",
      "ALS step \n",
      "EPOCH 200 MODEL LOSS 1.962588\n",
      "201 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.361738\n",
      "EPOCH 201 MODEL LOSS 1.605788\n",
      "202 / 1000\n",
      "ALS step \n",
      "EPOCH 202 MODEL LOSS 2.727422\n",
      "203 / 1000\n",
      "ALS step \n",
      "EPOCH 203 MODEL LOSS 1.480931\n",
      "204 / 1000\n",
      "ALS step \n",
      "EPOCH 204 MODEL LOSS 1.192563\n",
      "205 / 1000\n",
      "ALS step \n",
      "EPOCH 205 MODEL LOSS 1.315457\n",
      "206 / 1000\n",
      "ALS step \n",
      "EPOCH 206 MODEL LOSS 1.820497\n",
      "207 / 1000\n",
      "ALS step \n",
      "EPOCH 207 MODEL LOSS 1.484349\n",
      "208 / 1000\n",
      "ALS step \n",
      "EPOCH 208 MODEL LOSS 1.267308\n",
      "209 / 1000\n",
      "ALS step \n",
      "EPOCH 209 MODEL LOSS 1.129871\n",
      "210 / 1000\n",
      "ALS step \n",
      "EPOCH 210 MODEL LOSS 1.806817\n",
      "211 / 1000\n",
      "ALS step \n",
      "EPOCH 211 MODEL LOSS 1.545346\n",
      "212 / 1000\n",
      "ALS step \n",
      "EPOCH 212 MODEL LOSS 1.411722\n",
      "213 / 1000\n",
      "ALS step \n",
      "EPOCH 213 MODEL LOSS 1.580950\n",
      "214 / 1000\n",
      "ALS step \n",
      "EPOCH 214 MODEL LOSS 1.961025\n",
      "215 / 1000\n",
      "ALS step \n",
      "EPOCH 215 MODEL LOSS 1.469676\n",
      "216 / 1000\n",
      "ALS step \n",
      "EPOCH 216 MODEL LOSS 1.183897\n",
      "217 / 1000\n",
      "ALS step \n",
      "EPOCH 217 MODEL LOSS 1.946915\n",
      "218 / 1000\n",
      "ALS step \n",
      "EPOCH 218 MODEL LOSS 1.614381\n",
      "219 / 1000\n",
      "ALS step \n",
      "EPOCH 219 MODEL LOSS 1.427518\n",
      "220 / 1000\n",
      "ALS step \n",
      "EPOCH 220 MODEL LOSS 1.340794\n",
      "221 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.348028\n",
      "EPOCH 221 MODEL LOSS 1.347313\n",
      "222 / 1000\n",
      "ALS step \n",
      "EPOCH 222 MODEL LOSS 1.420184\n",
      "223 / 1000\n",
      "ALS step \n",
      "EPOCH 223 MODEL LOSS 1.098323\n",
      "224 / 1000\n",
      "ALS step \n",
      "EPOCH 224 MODEL LOSS 1.371340\n",
      "225 / 1000\n",
      "ALS step \n",
      "EPOCH 225 MODEL LOSS 2.038466\n",
      "226 / 1000\n",
      "ALS step \n",
      "EPOCH 226 MODEL LOSS 1.528539\n",
      "227 / 1000\n",
      "ALS step \n",
      "EPOCH 227 MODEL LOSS 1.490180\n",
      "228 / 1000\n",
      "ALS step \n",
      "EPOCH 228 MODEL LOSS 1.476438\n",
      "229 / 1000\n",
      "ALS step \n",
      "EPOCH 229 MODEL LOSS 1.519964\n",
      "230 / 1000\n",
      "ALS step \n",
      "EPOCH 230 MODEL LOSS 1.292322\n",
      "231 / 1000\n",
      "ALS step \n",
      "EPOCH 231 MODEL LOSS 1.347347\n",
      "232 / 1000\n",
      "ALS step \n",
      "EPOCH 232 MODEL LOSS 1.129096\n",
      "233 / 1000\n",
      "ALS step \n",
      "EPOCH 233 MODEL LOSS 1.356090\n",
      "234 / 1000\n",
      "ALS step \n",
      "EPOCH 234 MODEL LOSS 1.344723\n",
      "235 / 1000\n",
      "ALS step \n",
      "EPOCH 235 MODEL LOSS 1.231499\n",
      "236 / 1000\n",
      "ALS step \n",
      "EPOCH 236 MODEL LOSS 1.572784\n",
      "237 / 1000\n",
      "ALS step \n",
      "EPOCH 237 MODEL LOSS 1.562930\n",
      "238 / 1000\n",
      "ALS step \n",
      "EPOCH 238 MODEL LOSS 1.820738\n",
      "239 / 1000\n",
      "ALS step \n",
      "EPOCH 239 MODEL LOSS 1.131096\n",
      "240 / 1000\n",
      "ALS step \n",
      "EPOCH 240 MODEL LOSS 1.053687\n",
      "241 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.377734\n",
      "EPOCH 241 MODEL LOSS 1.351232\n",
      "242 / 1000\n",
      "ALS step \n",
      "EPOCH 242 MODEL LOSS 1.304932\n",
      "243 / 1000\n",
      "ALS step \n",
      "EPOCH 243 MODEL LOSS 1.408800\n",
      "244 / 1000\n",
      "ALS step \n",
      "EPOCH 244 MODEL LOSS 1.740719\n",
      "245 / 1000\n",
      "ALS step \n",
      "EPOCH 245 MODEL LOSS 1.210340\n",
      "246 / 1000\n",
      "ALS step \n",
      "EPOCH 246 MODEL LOSS 1.347794\n",
      "247 / 1000\n",
      "ALS step \n",
      "EPOCH 247 MODEL LOSS 1.123124\n",
      "248 / 1000\n",
      "ALS step \n",
      "EPOCH 248 MODEL LOSS 1.129954\n",
      "249 / 1000\n",
      "ALS step \n",
      "EPOCH 249 MODEL LOSS 1.672729\n",
      "250 / 1000\n",
      "ALS step \n",
      "EPOCH 250 MODEL LOSS 1.828711\n",
      "251 / 1000\n",
      "ALS step \n",
      "EPOCH 251 MODEL LOSS 1.847039\n",
      "252 / 1000\n",
      "ALS step \n",
      "EPOCH 252 MODEL LOSS 1.357294\n",
      "253 / 1000\n",
      "ALS step \n",
      "EPOCH 253 MODEL LOSS 1.282410\n",
      "254 / 1000\n",
      "ALS step \n",
      "EPOCH 254 MODEL LOSS 1.023523\n",
      "255 / 1000\n",
      "ALS step \n",
      "EPOCH 255 MODEL LOSS 1.710330\n",
      "256 / 1000\n",
      "ALS step \n",
      "EPOCH 256 MODEL LOSS 1.173078\n",
      "257 / 1000\n",
      "ALS step \n",
      "EPOCH 257 MODEL LOSS 1.599195\n",
      "258 / 1000\n",
      "ALS step \n",
      "EPOCH 258 MODEL LOSS 1.371067\n",
      "259 / 1000\n",
      "ALS step \n",
      "EPOCH 259 MODEL LOSS 1.061217\n",
      "260 / 1000\n",
      "ALS step \n",
      "EPOCH 260 MODEL LOSS 1.798844\n",
      "261 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.371027\n",
      "EPOCH 261 MODEL LOSS 1.206919\n",
      "262 / 1000\n",
      "ALS step \n",
      "EPOCH 262 MODEL LOSS 1.366807\n",
      "263 / 1000\n",
      "ALS step \n",
      "EPOCH 263 MODEL LOSS 0.981433\n",
      "264 / 1000\n",
      "ALS step \n",
      "EPOCH 264 MODEL LOSS 1.420328\n",
      "265 / 1000\n",
      "ALS step \n",
      "EPOCH 265 MODEL LOSS 1.901004\n",
      "266 / 1000\n",
      "ALS step \n",
      "EPOCH 266 MODEL LOSS 1.285118\n",
      "267 / 1000\n",
      "ALS step \n",
      "EPOCH 267 MODEL LOSS 1.305182\n",
      "268 / 1000\n",
      "ALS step \n",
      "EPOCH 268 MODEL LOSS 1.231274\n",
      "269 / 1000\n",
      "ALS step \n",
      "EPOCH 269 MODEL LOSS 1.399752\n",
      "270 / 1000\n",
      "ALS step \n",
      "EPOCH 270 MODEL LOSS 1.721354\n",
      "271 / 1000\n",
      "ALS step \n",
      "EPOCH 271 MODEL LOSS 1.457094\n",
      "272 / 1000\n",
      "ALS step \n",
      "EPOCH 272 MODEL LOSS 1.613979\n",
      "273 / 1000\n",
      "ALS step \n",
      "EPOCH 273 MODEL LOSS 1.579689\n",
      "274 / 1000\n",
      "ALS step \n",
      "EPOCH 274 MODEL LOSS 1.421352\n",
      "275 / 1000\n",
      "ALS step \n",
      "EPOCH 275 MODEL LOSS 1.819729\n",
      "276 / 1000\n",
      "ALS step \n",
      "EPOCH 276 MODEL LOSS 1.601304\n",
      "277 / 1000\n",
      "ALS step \n",
      "EPOCH 277 MODEL LOSS 1.772487\n",
      "278 / 1000\n",
      "ALS step \n",
      "EPOCH 278 MODEL LOSS 1.598300\n",
      "279 / 1000\n",
      "ALS step \n",
      "EPOCH 279 MODEL LOSS 1.377275\n",
      "280 / 1000\n",
      "ALS step \n",
      "EPOCH 280 MODEL LOSS 1.613932\n",
      "281 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.413281\n",
      "EPOCH 281 MODEL LOSS 1.480501\n",
      "282 / 1000\n",
      "ALS step \n",
      "EPOCH 282 MODEL LOSS 1.291858\n",
      "283 / 1000\n",
      "ALS step \n",
      "EPOCH 283 MODEL LOSS 1.800686\n",
      "284 / 1000\n",
      "ALS step \n",
      "EPOCH 284 MODEL LOSS 1.665695\n",
      "285 / 1000\n",
      "ALS step \n",
      "EPOCH 285 MODEL LOSS 1.765619\n",
      "286 / 1000\n",
      "ALS step \n",
      "EPOCH 286 MODEL LOSS 1.392466\n",
      "287 / 1000\n",
      "ALS step \n",
      "EPOCH 287 MODEL LOSS 1.615565\n",
      "288 / 1000\n",
      "ALS step \n",
      "EPOCH 288 MODEL LOSS 1.473994\n",
      "289 / 1000\n",
      "ALS step \n",
      "EPOCH 289 MODEL LOSS 1.538536\n",
      "290 / 1000\n",
      "ALS step \n",
      "EPOCH 290 MODEL LOSS 1.642674\n",
      "291 / 1000\n",
      "ALS step \n",
      "EPOCH 291 MODEL LOSS 1.235933\n",
      "292 / 1000\n",
      "ALS step \n",
      "EPOCH 292 MODEL LOSS 1.207669\n",
      "293 / 1000\n",
      "ALS step \n",
      "EPOCH 293 MODEL LOSS 2.015306\n",
      "294 / 1000\n",
      "ALS step \n",
      "EPOCH 294 MODEL LOSS 1.529063\n",
      "295 / 1000\n",
      "ALS step \n",
      "EPOCH 295 MODEL LOSS 1.123358\n",
      "296 / 1000\n",
      "ALS step \n",
      "EPOCH 296 MODEL LOSS 1.847618\n",
      "297 / 1000\n",
      "ALS step \n",
      "EPOCH 297 MODEL LOSS 1.639267\n",
      "298 / 1000\n",
      "ALS step \n",
      "EPOCH 298 MODEL LOSS 1.823505\n",
      "299 / 1000\n",
      "ALS step \n",
      "EPOCH 299 MODEL LOSS 1.279949\n",
      "300 / 1000\n",
      "ALS step \n",
      "EPOCH 300 MODEL LOSS 1.177796\n",
      "301 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.418618\n",
      "EPOCH 301 MODEL LOSS 1.493220\n",
      "302 / 1000\n",
      "ALS step \n",
      "EPOCH 302 MODEL LOSS 1.168522\n",
      "303 / 1000\n",
      "ALS step \n",
      "EPOCH 303 MODEL LOSS 1.046377\n",
      "304 / 1000\n",
      "ALS step \n",
      "EPOCH 304 MODEL LOSS 1.280569\n",
      "305 / 1000\n",
      "ALS step \n",
      "EPOCH 305 MODEL LOSS 1.505387\n",
      "306 / 1000\n",
      "ALS step \n",
      "EPOCH 306 MODEL LOSS 1.232307\n",
      "307 / 1000\n",
      "ALS step \n",
      "EPOCH 307 MODEL LOSS 1.647777\n",
      "308 / 1000\n",
      "ALS step \n",
      "EPOCH 308 MODEL LOSS 1.927519\n",
      "309 / 1000\n",
      "ALS step \n",
      "EPOCH 309 MODEL LOSS 1.628415\n",
      "310 / 1000\n",
      "ALS step \n",
      "EPOCH 310 MODEL LOSS 1.992561\n",
      "311 / 1000\n",
      "ALS step \n",
      "EPOCH 311 MODEL LOSS 1.187805\n",
      "312 / 1000\n",
      "ALS step \n",
      "EPOCH 312 MODEL LOSS 1.128811\n",
      "313 / 1000\n",
      "ALS step \n",
      "EPOCH 313 MODEL LOSS 1.386213\n",
      "314 / 1000\n",
      "ALS step \n",
      "EPOCH 314 MODEL LOSS 1.141567\n",
      "315 / 1000\n",
      "ALS step \n",
      "EPOCH 315 MODEL LOSS 1.691298\n",
      "316 / 1000\n",
      "ALS step \n",
      "EPOCH 316 MODEL LOSS 1.557160\n",
      "317 / 1000\n",
      "ALS step \n",
      "EPOCH 317 MODEL LOSS 1.120621\n",
      "318 / 1000\n",
      "ALS step \n",
      "EPOCH 318 MODEL LOSS 1.607086\n",
      "319 / 1000\n",
      "ALS step \n",
      "EPOCH 319 MODEL LOSS 1.116879\n",
      "320 / 1000\n",
      "ALS step \n",
      "EPOCH 320 MODEL LOSS 1.339400\n",
      "321 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.368296\n",
      "EPOCH 321 MODEL LOSS 1.086788\n",
      "322 / 1000\n",
      "ALS step \n",
      "EPOCH 322 MODEL LOSS 1.148233\n",
      "323 / 1000\n",
      "ALS step \n",
      "EPOCH 323 MODEL LOSS 1.748591\n",
      "324 / 1000\n",
      "ALS step \n",
      "EPOCH 324 MODEL LOSS 1.382200\n",
      "325 / 1000\n",
      "ALS step \n",
      "EPOCH 325 MODEL LOSS 1.534715\n",
      "326 / 1000\n",
      "ALS step \n",
      "EPOCH 326 MODEL LOSS 1.328668\n",
      "327 / 1000\n",
      "ALS step \n",
      "EPOCH 327 MODEL LOSS 1.183042\n",
      "328 / 1000\n",
      "ALS step \n",
      "EPOCH 328 MODEL LOSS 1.756655\n",
      "329 / 1000\n",
      "ALS step \n",
      "EPOCH 329 MODEL LOSS 1.117990\n",
      "330 / 1000\n",
      "ALS step \n",
      "EPOCH 330 MODEL LOSS 1.389726\n",
      "331 / 1000\n",
      "ALS step \n",
      "EPOCH 331 MODEL LOSS 1.655264\n",
      "332 / 1000\n",
      "ALS step \n",
      "EPOCH 332 MODEL LOSS 2.017782\n",
      "333 / 1000\n",
      "ALS step \n",
      "EPOCH 333 MODEL LOSS 1.366382\n",
      "334 / 1000\n",
      "ALS step \n",
      "EPOCH 334 MODEL LOSS 1.652568\n",
      "335 / 1000\n",
      "ALS step \n",
      "EPOCH 335 MODEL LOSS 1.512772\n",
      "336 / 1000\n",
      "ALS step \n",
      "EPOCH 336 MODEL LOSS 1.967886\n",
      "337 / 1000\n",
      "ALS step \n",
      "EPOCH 337 MODEL LOSS 1.712764\n",
      "338 / 1000\n",
      "ALS step \n",
      "EPOCH 338 MODEL LOSS 1.792611\n",
      "339 / 1000\n",
      "ALS step \n",
      "EPOCH 339 MODEL LOSS 1.215174\n",
      "340 / 1000\n",
      "ALS step \n",
      "EPOCH 340 MODEL LOSS 1.794668\n",
      "341 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.435640\n",
      "EPOCH 341 MODEL LOSS 1.174856\n",
      "342 / 1000\n",
      "ALS step \n",
      "EPOCH 342 MODEL LOSS 1.108561\n",
      "343 / 1000\n",
      "ALS step \n",
      "EPOCH 343 MODEL LOSS 1.547365\n",
      "344 / 1000\n",
      "ALS step \n",
      "EPOCH 344 MODEL LOSS 1.206564\n",
      "345 / 1000\n",
      "ALS step \n",
      "EPOCH 345 MODEL LOSS 1.433941\n",
      "346 / 1000\n",
      "ALS step \n",
      "EPOCH 346 MODEL LOSS 1.436487\n",
      "347 / 1000\n",
      "ALS step \n",
      "EPOCH 347 MODEL LOSS 1.456952\n",
      "348 / 1000\n",
      "ALS step \n",
      "EPOCH 348 MODEL LOSS 1.261597\n",
      "349 / 1000\n",
      "ALS step \n",
      "EPOCH 349 MODEL LOSS 1.981349\n",
      "350 / 1000\n",
      "ALS step \n",
      "EPOCH 350 MODEL LOSS 1.733784\n",
      "351 / 1000\n",
      "ALS step \n",
      "EPOCH 351 MODEL LOSS 1.556193\n",
      "352 / 1000\n",
      "ALS step \n",
      "EPOCH 352 MODEL LOSS 1.502392\n",
      "353 / 1000\n",
      "ALS step \n",
      "EPOCH 353 MODEL LOSS 1.048104\n",
      "354 / 1000\n",
      "ALS step \n",
      "EPOCH 354 MODEL LOSS 1.343620\n",
      "355 / 1000\n",
      "ALS step \n",
      "EPOCH 355 MODEL LOSS 1.401496\n",
      "356 / 1000\n",
      "ALS step \n",
      "EPOCH 356 MODEL LOSS 1.426030\n",
      "357 / 1000\n",
      "ALS step \n",
      "EPOCH 357 MODEL LOSS 1.230527\n",
      "358 / 1000\n",
      "ALS step \n",
      "EPOCH 358 MODEL LOSS 2.118255\n",
      "359 / 1000\n",
      "ALS step \n",
      "EPOCH 359 MODEL LOSS 1.885510\n",
      "360 / 1000\n",
      "ALS step \n",
      "EPOCH 360 MODEL LOSS 1.344497\n",
      "361 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.366652\n",
      "EPOCH 361 MODEL LOSS 1.078719\n",
      "362 / 1000\n",
      "ALS step \n",
      "EPOCH 362 MODEL LOSS 1.293380\n",
      "363 / 1000\n",
      "ALS step \n",
      "EPOCH 363 MODEL LOSS 1.286277\n",
      "364 / 1000\n",
      "ALS step \n",
      "EPOCH 364 MODEL LOSS 1.112045\n",
      "365 / 1000\n",
      "ALS step \n",
      "EPOCH 365 MODEL LOSS 1.146700\n",
      "366 / 1000\n",
      "ALS step \n",
      "EPOCH 366 MODEL LOSS 1.540233\n",
      "367 / 1000\n",
      "ALS step \n",
      "EPOCH 367 MODEL LOSS 1.316098\n",
      "368 / 1000\n",
      "ALS step \n",
      "EPOCH 368 MODEL LOSS 1.253813\n",
      "369 / 1000\n",
      "ALS step \n",
      "EPOCH 369 MODEL LOSS 1.108624\n",
      "370 / 1000\n",
      "ALS step \n",
      "EPOCH 370 MODEL LOSS 1.942628\n",
      "371 / 1000\n",
      "ALS step \n",
      "EPOCH 371 MODEL LOSS 1.178139\n",
      "372 / 1000\n",
      "ALS step \n",
      "EPOCH 372 MODEL LOSS 1.579596\n",
      "373 / 1000\n",
      "ALS step \n",
      "EPOCH 373 MODEL LOSS 1.495173\n",
      "374 / 1000\n",
      "ALS step \n",
      "EPOCH 374 MODEL LOSS 1.371541\n",
      "375 / 1000\n",
      "ALS step \n",
      "EPOCH 375 MODEL LOSS 1.914624\n",
      "376 / 1000\n",
      "ALS step \n",
      "EPOCH 376 MODEL LOSS 1.599390\n",
      "377 / 1000\n",
      "ALS step \n",
      "EPOCH 377 MODEL LOSS 2.040521\n",
      "378 / 1000\n",
      "ALS step \n",
      "EPOCH 378 MODEL LOSS 1.204856\n",
      "379 / 1000\n",
      "ALS step \n",
      "EPOCH 379 MODEL LOSS 1.076342\n",
      "380 / 1000\n",
      "ALS step \n",
      "EPOCH 380 MODEL LOSS 1.151564\n",
      "381 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.379746\n",
      "EPOCH 381 MODEL LOSS 1.698962\n",
      "382 / 1000\n",
      "ALS step \n",
      "EPOCH 382 MODEL LOSS 1.315166\n",
      "383 / 1000\n",
      "ALS step \n",
      "EPOCH 383 MODEL LOSS 1.003574\n",
      "384 / 1000\n",
      "ALS step \n",
      "EPOCH 384 MODEL LOSS 1.392867\n",
      "385 / 1000\n",
      "ALS step \n",
      "EPOCH 385 MODEL LOSS 0.985904\n",
      "386 / 1000\n",
      "ALS step \n",
      "EPOCH 386 MODEL LOSS 1.099920\n",
      "387 / 1000\n",
      "ALS step \n",
      "EPOCH 387 MODEL LOSS 1.654428\n",
      "388 / 1000\n",
      "ALS step \n",
      "EPOCH 388 MODEL LOSS 1.910915\n",
      "389 / 1000\n",
      "ALS step \n",
      "EPOCH 389 MODEL LOSS 1.761796\n",
      "390 / 1000\n",
      "ALS step \n",
      "EPOCH 390 MODEL LOSS 2.093853\n",
      "391 / 1000\n",
      "ALS step \n",
      "EPOCH 391 MODEL LOSS 1.110872\n",
      "392 / 1000\n",
      "ALS step \n",
      "EPOCH 392 MODEL LOSS 1.185624\n",
      "393 / 1000\n",
      "ALS step \n",
      "EPOCH 393 MODEL LOSS 1.353289\n",
      "394 / 1000\n",
      "ALS step \n",
      "EPOCH 394 MODEL LOSS 2.227651\n",
      "395 / 1000\n",
      "ALS step \n",
      "EPOCH 395 MODEL LOSS 1.187377\n",
      "396 / 1000\n",
      "ALS step \n",
      "EPOCH 396 MODEL LOSS 1.580318\n",
      "397 / 1000\n",
      "ALS step \n",
      "EPOCH 397 MODEL LOSS 1.254836\n",
      "398 / 1000\n",
      "ALS step \n",
      "EPOCH 398 MODEL LOSS 1.376221\n",
      "399 / 1000\n",
      "ALS step \n",
      "EPOCH 399 MODEL LOSS 1.303851\n",
      "400 / 1000\n",
      "ALS step \n",
      "EPOCH 400 MODEL LOSS 1.255891\n",
      "401 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.364547\n",
      "EPOCH 401 MODEL LOSS 1.231561\n",
      "402 / 1000\n",
      "ALS step \n",
      "EPOCH 402 MODEL LOSS 1.354892\n",
      "403 / 1000\n",
      "ALS step \n",
      "EPOCH 403 MODEL LOSS 1.376147\n",
      "404 / 1000\n",
      "ALS step \n",
      "EPOCH 404 MODEL LOSS 1.308160\n",
      "405 / 1000\n",
      "ALS step \n",
      "EPOCH 405 MODEL LOSS 1.322364\n",
      "406 / 1000\n",
      "ALS step \n",
      "EPOCH 406 MODEL LOSS 2.230525\n",
      "407 / 1000\n",
      "ALS step \n",
      "EPOCH 407 MODEL LOSS 1.546245\n",
      "408 / 1000\n",
      "ALS step \n",
      "EPOCH 408 MODEL LOSS 1.484774\n",
      "409 / 1000\n",
      "ALS step \n",
      "EPOCH 409 MODEL LOSS 1.232587\n",
      "410 / 1000\n",
      "ALS step \n",
      "EPOCH 410 MODEL LOSS 1.307475\n",
      "411 / 1000\n",
      "ALS step \n",
      "EPOCH 411 MODEL LOSS 1.789493\n",
      "412 / 1000\n",
      "ALS step \n",
      "EPOCH 412 MODEL LOSS 1.391093\n",
      "413 / 1000\n",
      "ALS step \n",
      "EPOCH 413 MODEL LOSS 1.091083\n",
      "414 / 1000\n",
      "ALS step \n",
      "EPOCH 414 MODEL LOSS 1.433210\n",
      "415 / 1000\n",
      "ALS step \n",
      "EPOCH 415 MODEL LOSS 1.364595\n",
      "416 / 1000\n",
      "ALS step \n",
      "EPOCH 416 MODEL LOSS 1.215732\n",
      "417 / 1000\n",
      "ALS step \n",
      "EPOCH 417 MODEL LOSS 2.019170\n",
      "418 / 1000\n",
      "ALS step \n",
      "EPOCH 418 MODEL LOSS 1.343322\n",
      "419 / 1000\n",
      "ALS step \n",
      "EPOCH 419 MODEL LOSS 1.062755\n",
      "420 / 1000\n",
      "ALS step \n",
      "EPOCH 420 MODEL LOSS 1.788206\n",
      "421 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.364549\n",
      "EPOCH 421 MODEL LOSS 1.666314\n",
      "422 / 1000\n",
      "ALS step \n",
      "EPOCH 422 MODEL LOSS 1.650556\n",
      "423 / 1000\n",
      "ALS step \n",
      "EPOCH 423 MODEL LOSS 1.385209\n",
      "424 / 1000\n",
      "ALS step \n",
      "EPOCH 424 MODEL LOSS 1.286872\n",
      "425 / 1000\n",
      "ALS step \n",
      "EPOCH 425 MODEL LOSS 1.362249\n",
      "426 / 1000\n",
      "ALS step \n",
      "EPOCH 426 MODEL LOSS 1.423385\n",
      "427 / 1000\n",
      "ALS step \n",
      "EPOCH 427 MODEL LOSS 1.599599\n",
      "428 / 1000\n",
      "ALS step \n",
      "EPOCH 428 MODEL LOSS 1.680149\n",
      "429 / 1000\n",
      "ALS step \n",
      "EPOCH 429 MODEL LOSS 1.183649\n",
      "430 / 1000\n",
      "ALS step \n",
      "EPOCH 430 MODEL LOSS 1.397278\n",
      "431 / 1000\n",
      "ALS step \n",
      "EPOCH 431 MODEL LOSS 1.440756\n",
      "432 / 1000\n",
      "ALS step \n",
      "EPOCH 432 MODEL LOSS 2.049484\n",
      "433 / 1000\n",
      "ALS step \n",
      "EPOCH 433 MODEL LOSS 1.201488\n",
      "434 / 1000\n",
      "ALS step \n",
      "EPOCH 434 MODEL LOSS 1.413303\n",
      "435 / 1000\n",
      "ALS step \n",
      "EPOCH 435 MODEL LOSS 1.371017\n",
      "436 / 1000\n",
      "ALS step \n",
      "EPOCH 436 MODEL LOSS 1.450381\n",
      "437 / 1000\n",
      "ALS step \n",
      "EPOCH 437 MODEL LOSS 1.268300\n",
      "438 / 1000\n",
      "ALS step \n",
      "EPOCH 438 MODEL LOSS 1.380516\n",
      "439 / 1000\n",
      "ALS step \n",
      "EPOCH 439 MODEL LOSS 1.513973\n",
      "440 / 1000\n",
      "ALS step \n",
      "EPOCH 440 MODEL LOSS 1.173635\n",
      "441 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.365396\n",
      "EPOCH 441 MODEL LOSS 1.516457\n",
      "442 / 1000\n",
      "ALS step \n",
      "EPOCH 442 MODEL LOSS 2.121286\n",
      "443 / 1000\n",
      "ALS step \n",
      "EPOCH 443 MODEL LOSS 1.313683\n",
      "444 / 1000\n",
      "ALS step \n",
      "EPOCH 444 MODEL LOSS 1.323951\n",
      "445 / 1000\n",
      "ALS step \n",
      "EPOCH 445 MODEL LOSS 1.094831\n",
      "446 / 1000\n",
      "ALS step \n",
      "EPOCH 446 MODEL LOSS 1.159098\n",
      "447 / 1000\n",
      "ALS step \n",
      "EPOCH 447 MODEL LOSS 1.408244\n",
      "448 / 1000\n",
      "ALS step \n",
      "EPOCH 448 MODEL LOSS 1.246723\n",
      "449 / 1000\n",
      "ALS step \n",
      "EPOCH 449 MODEL LOSS 1.443004\n",
      "450 / 1000\n",
      "ALS step \n",
      "EPOCH 450 MODEL LOSS 1.702914\n",
      "451 / 1000\n",
      "ALS step \n",
      "EPOCH 451 MODEL LOSS 1.858472\n",
      "452 / 1000\n",
      "ALS step \n",
      "EPOCH 452 MODEL LOSS 1.537770\n",
      "453 / 1000\n",
      "ALS step \n",
      "EPOCH 453 MODEL LOSS 1.038954\n",
      "454 / 1000\n",
      "ALS step \n",
      "EPOCH 454 MODEL LOSS 1.062968\n",
      "455 / 1000\n",
      "ALS step \n",
      "EPOCH 455 MODEL LOSS 1.379598\n",
      "456 / 1000\n",
      "ALS step \n",
      "EPOCH 456 MODEL LOSS 1.890223\n",
      "457 / 1000\n",
      "ALS step \n",
      "EPOCH 457 MODEL LOSS 1.742747\n",
      "458 / 1000\n",
      "ALS step \n",
      "EPOCH 458 MODEL LOSS 1.585070\n",
      "459 / 1000\n",
      "ALS step \n",
      "EPOCH 459 MODEL LOSS 1.901494\n",
      "460 / 1000\n",
      "ALS step \n",
      "EPOCH 460 MODEL LOSS 1.174449\n",
      "461 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.436138\n",
      "EPOCH 461 MODEL LOSS 1.165293\n",
      "462 / 1000\n",
      "ALS step \n",
      "EPOCH 462 MODEL LOSS 1.656705\n",
      "463 / 1000\n",
      "ALS step \n",
      "EPOCH 463 MODEL LOSS 1.315968\n",
      "464 / 1000\n",
      "ALS step \n",
      "EPOCH 464 MODEL LOSS 1.877868\n",
      "465 / 1000\n",
      "ALS step \n",
      "EPOCH 465 MODEL LOSS 1.313003\n",
      "466 / 1000\n",
      "ALS step \n",
      "EPOCH 466 MODEL LOSS 1.432575\n",
      "467 / 1000\n",
      "ALS step \n",
      "EPOCH 467 MODEL LOSS 1.527736\n",
      "468 / 1000\n",
      "ALS step \n",
      "EPOCH 468 MODEL LOSS 1.649847\n",
      "469 / 1000\n",
      "ALS step \n",
      "EPOCH 469 MODEL LOSS 1.370291\n",
      "470 / 1000\n",
      "ALS step \n",
      "EPOCH 470 MODEL LOSS 1.092815\n",
      "471 / 1000\n",
      "ALS step \n",
      "EPOCH 471 MODEL LOSS 1.898237\n",
      "472 / 1000\n",
      "ALS step \n",
      "EPOCH 472 MODEL LOSS 1.265093\n",
      "473 / 1000\n",
      "ALS step \n",
      "EPOCH 473 MODEL LOSS 1.412020\n",
      "474 / 1000\n",
      "ALS step \n",
      "EPOCH 474 MODEL LOSS 1.464038\n",
      "475 / 1000\n",
      "ALS step \n",
      "EPOCH 475 MODEL LOSS 1.518680\n",
      "476 / 1000\n",
      "ALS step \n",
      "EPOCH 476 MODEL LOSS 1.155808\n",
      "477 / 1000\n",
      "ALS step \n",
      "EPOCH 477 MODEL LOSS 1.718139\n",
      "478 / 1000\n",
      "ALS step \n",
      "EPOCH 478 MODEL LOSS 1.691295\n",
      "479 / 1000\n",
      "ALS step \n",
      "EPOCH 479 MODEL LOSS 1.130863\n",
      "480 / 1000\n",
      "ALS step \n",
      "EPOCH 480 MODEL LOSS 1.160519\n",
      "481 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.382354\n",
      "EPOCH 481 MODEL LOSS 1.140685\n",
      "482 / 1000\n",
      "ALS step \n",
      "EPOCH 482 MODEL LOSS 1.133763\n",
      "483 / 1000\n",
      "ALS step \n",
      "EPOCH 483 MODEL LOSS 2.014680\n",
      "484 / 1000\n",
      "ALS step \n",
      "EPOCH 484 MODEL LOSS 1.510654\n",
      "485 / 1000\n",
      "ALS step \n",
      "EPOCH 485 MODEL LOSS 1.372336\n",
      "486 / 1000\n",
      "ALS step \n",
      "EPOCH 486 MODEL LOSS 1.334942\n",
      "487 / 1000\n",
      "ALS step \n",
      "EPOCH 487 MODEL LOSS 1.125432\n",
      "488 / 1000\n",
      "ALS step \n",
      "EPOCH 488 MODEL LOSS 1.674105\n",
      "489 / 1000\n",
      "ALS step \n",
      "EPOCH 489 MODEL LOSS 1.078216\n",
      "490 / 1000\n",
      "ALS step \n",
      "EPOCH 490 MODEL LOSS 1.902995\n",
      "491 / 1000\n",
      "ALS step \n",
      "EPOCH 491 MODEL LOSS 1.634903\n",
      "492 / 1000\n",
      "ALS step \n",
      "EPOCH 492 MODEL LOSS 1.459645\n",
      "493 / 1000\n",
      "ALS step \n",
      "EPOCH 493 MODEL LOSS 1.456070\n",
      "494 / 1000\n",
      "ALS step \n",
      "EPOCH 494 MODEL LOSS 1.461558\n",
      "495 / 1000\n",
      "ALS step \n",
      "EPOCH 495 MODEL LOSS 1.476698\n",
      "496 / 1000\n",
      "ALS step \n",
      "EPOCH 496 MODEL LOSS 1.370203\n",
      "497 / 1000\n",
      "ALS step \n",
      "EPOCH 497 MODEL LOSS 1.432371\n",
      "498 / 1000\n",
      "ALS step \n",
      "EPOCH 498 MODEL LOSS 1.484862\n",
      "499 / 1000\n",
      "ALS step \n",
      "EPOCH 499 MODEL LOSS 1.285522\n",
      "500 / 1000\n",
      "ALS step \n",
      "EPOCH 500 MODEL LOSS 1.598811\n",
      "501 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.382065\n",
      "EPOCH 501 MODEL LOSS 1.166463\n",
      "502 / 1000\n",
      "ALS step \n",
      "EPOCH 502 MODEL LOSS 2.171759\n",
      "503 / 1000\n",
      "ALS step \n",
      "EPOCH 503 MODEL LOSS 1.604215\n",
      "504 / 1000\n",
      "ALS step \n",
      "EPOCH 504 MODEL LOSS 1.281282\n",
      "505 / 1000\n",
      "ALS step \n",
      "EPOCH 505 MODEL LOSS 1.428422\n",
      "506 / 1000\n",
      "ALS step \n",
      "EPOCH 506 MODEL LOSS 1.604844\n",
      "507 / 1000\n",
      "ALS step \n",
      "EPOCH 507 MODEL LOSS 1.217163\n",
      "508 / 1000\n",
      "ALS step \n",
      "EPOCH 508 MODEL LOSS 1.735175\n",
      "509 / 1000\n",
      "ALS step \n",
      "EPOCH 509 MODEL LOSS 1.091188\n",
      "510 / 1000\n",
      "ALS step \n",
      "EPOCH 510 MODEL LOSS 1.540289\n",
      "511 / 1000\n",
      "ALS step \n",
      "EPOCH 511 MODEL LOSS 1.475827\n",
      "512 / 1000\n",
      "ALS step \n",
      "EPOCH 512 MODEL LOSS 1.559961\n",
      "513 / 1000\n",
      "ALS step \n",
      "EPOCH 513 MODEL LOSS 1.201893\n",
      "514 / 1000\n",
      "ALS step \n",
      "EPOCH 514 MODEL LOSS 1.475707\n",
      "515 / 1000\n",
      "ALS step \n",
      "EPOCH 515 MODEL LOSS 1.828502\n",
      "516 / 1000\n",
      "ALS step \n",
      "EPOCH 516 MODEL LOSS 1.250039\n",
      "517 / 1000\n",
      "ALS step \n",
      "EPOCH 517 MODEL LOSS 1.305339\n",
      "518 / 1000\n",
      "ALS step \n",
      "EPOCH 518 MODEL LOSS 1.524938\n",
      "519 / 1000\n",
      "ALS step \n",
      "EPOCH 519 MODEL LOSS 1.268442\n",
      "520 / 1000\n",
      "ALS step \n",
      "EPOCH 520 MODEL LOSS 2.379156\n",
      "521 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.379769\n",
      "EPOCH 521 MODEL LOSS 1.177890\n",
      "522 / 1000\n",
      "ALS step \n",
      "EPOCH 522 MODEL LOSS 1.785285\n",
      "523 / 1000\n",
      "ALS step \n",
      "EPOCH 523 MODEL LOSS 1.105164\n",
      "524 / 1000\n",
      "ALS step \n",
      "EPOCH 524 MODEL LOSS 1.773072\n",
      "525 / 1000\n",
      "ALS step \n",
      "EPOCH 525 MODEL LOSS 1.152760\n",
      "526 / 1000\n",
      "ALS step \n",
      "EPOCH 526 MODEL LOSS 2.095443\n",
      "527 / 1000\n",
      "ALS step \n",
      "EPOCH 527 MODEL LOSS 1.113781\n",
      "528 / 1000\n",
      "ALS step \n",
      "EPOCH 528 MODEL LOSS 1.429141\n",
      "529 / 1000\n",
      "ALS step \n",
      "EPOCH 529 MODEL LOSS 1.257876\n",
      "530 / 1000\n",
      "ALS step \n",
      "EPOCH 530 MODEL LOSS 1.906210\n",
      "531 / 1000\n",
      "ALS step \n",
      "EPOCH 531 MODEL LOSS 1.103354\n",
      "532 / 1000\n",
      "ALS step \n",
      "EPOCH 532 MODEL LOSS 1.174270\n",
      "533 / 1000\n",
      "ALS step \n",
      "EPOCH 533 MODEL LOSS 1.625088\n",
      "534 / 1000\n",
      "ALS step \n",
      "EPOCH 534 MODEL LOSS 1.481004\n",
      "535 / 1000\n",
      "ALS step \n",
      "EPOCH 535 MODEL LOSS 1.308391\n",
      "536 / 1000\n",
      "ALS step \n",
      "EPOCH 536 MODEL LOSS 1.370103\n",
      "537 / 1000\n",
      "ALS step \n",
      "EPOCH 537 MODEL LOSS 1.280032\n",
      "538 / 1000\n",
      "ALS step \n",
      "EPOCH 538 MODEL LOSS 1.480303\n",
      "539 / 1000\n",
      "ALS step \n",
      "EPOCH 539 MODEL LOSS 1.331322\n",
      "540 / 1000\n",
      "ALS step \n",
      "EPOCH 540 MODEL LOSS 1.806094\n",
      "541 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.375929\n",
      "EPOCH 541 MODEL LOSS 1.284134\n",
      "542 / 1000\n",
      "ALS step \n",
      "EPOCH 542 MODEL LOSS 1.370336\n",
      "543 / 1000\n",
      "ALS step \n",
      "EPOCH 543 MODEL LOSS 1.282710\n",
      "544 / 1000\n",
      "ALS step \n",
      "EPOCH 544 MODEL LOSS 1.147379\n",
      "545 / 1000\n",
      "ALS step \n",
      "EPOCH 545 MODEL LOSS 1.533876\n",
      "546 / 1000\n",
      "ALS step \n",
      "EPOCH 546 MODEL LOSS 1.437078\n",
      "547 / 1000\n",
      "ALS step \n",
      "EPOCH 547 MODEL LOSS 1.464790\n",
      "548 / 1000\n",
      "ALS step \n",
      "EPOCH 548 MODEL LOSS 1.657918\n",
      "549 / 1000\n",
      "ALS step \n",
      "EPOCH 549 MODEL LOSS 1.160685\n",
      "550 / 1000\n",
      "ALS step \n",
      "EPOCH 550 MODEL LOSS 1.539228\n",
      "551 / 1000\n",
      "ALS step \n",
      "EPOCH 551 MODEL LOSS 1.378366\n",
      "552 / 1000\n",
      "ALS step \n",
      "EPOCH 552 MODEL LOSS 1.343987\n",
      "553 / 1000\n",
      "ALS step \n",
      "EPOCH 553 MODEL LOSS 1.715947\n",
      "554 / 1000\n",
      "ALS step \n",
      "EPOCH 554 MODEL LOSS 1.678164\n",
      "555 / 1000\n",
      "ALS step \n",
      "EPOCH 555 MODEL LOSS 1.924082\n",
      "556 / 1000\n",
      "ALS step \n",
      "EPOCH 556 MODEL LOSS 1.518445\n",
      "557 / 1000\n",
      "ALS step \n",
      "EPOCH 557 MODEL LOSS 1.500481\n",
      "558 / 1000\n",
      "ALS step \n",
      "EPOCH 558 MODEL LOSS 1.562415\n",
      "559 / 1000\n",
      "ALS step \n",
      "EPOCH 559 MODEL LOSS 1.255967\n",
      "560 / 1000\n",
      "ALS step \n",
      "EPOCH 560 MODEL LOSS 1.560311\n",
      "561 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.437995\n",
      "EPOCH 561 MODEL LOSS 1.603501\n",
      "562 / 1000\n",
      "ALS step \n",
      "EPOCH 562 MODEL LOSS 1.396692\n",
      "563 / 1000\n",
      "ALS step \n",
      "EPOCH 563 MODEL LOSS 2.111391\n",
      "564 / 1000\n",
      "ALS step \n",
      "EPOCH 564 MODEL LOSS 1.323428\n",
      "565 / 1000\n",
      "ALS step \n",
      "EPOCH 565 MODEL LOSS 1.719698\n",
      "566 / 1000\n",
      "ALS step \n",
      "EPOCH 566 MODEL LOSS 1.413111\n",
      "567 / 1000\n",
      "ALS step \n",
      "EPOCH 567 MODEL LOSS 1.833384\n",
      "568 / 1000\n",
      "ALS step \n",
      "EPOCH 568 MODEL LOSS 1.564351\n",
      "569 / 1000\n",
      "ALS step \n",
      "EPOCH 569 MODEL LOSS 1.655699\n",
      "570 / 1000\n",
      "ALS step \n",
      "EPOCH 570 MODEL LOSS 1.961837\n",
      "571 / 1000\n",
      "ALS step \n",
      "EPOCH 571 MODEL LOSS 2.066793\n",
      "572 / 1000\n",
      "ALS step \n",
      "EPOCH 572 MODEL LOSS 1.673651\n",
      "573 / 1000\n",
      "ALS step \n",
      "EPOCH 573 MODEL LOSS 1.171327\n",
      "574 / 1000\n",
      "ALS step \n",
      "EPOCH 574 MODEL LOSS 1.207793\n",
      "575 / 1000\n",
      "ALS step \n",
      "EPOCH 575 MODEL LOSS 1.172622\n",
      "576 / 1000\n",
      "ALS step \n",
      "EPOCH 576 MODEL LOSS 1.452745\n",
      "577 / 1000\n",
      "ALS step \n",
      "EPOCH 577 MODEL LOSS 2.046821\n",
      "578 / 1000\n",
      "ALS step \n",
      "EPOCH 578 MODEL LOSS 1.176896\n",
      "579 / 1000\n",
      "ALS step \n",
      "EPOCH 579 MODEL LOSS 0.994809\n",
      "580 / 1000\n",
      "ALS step \n",
      "EPOCH 580 MODEL LOSS 1.104626\n",
      "581 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.356741\n",
      "EPOCH 581 MODEL LOSS 1.939182\n",
      "582 / 1000\n",
      "ALS step \n",
      "EPOCH 582 MODEL LOSS 1.422812\n",
      "583 / 1000\n",
      "ALS step \n",
      "EPOCH 583 MODEL LOSS 1.762163\n",
      "584 / 1000\n",
      "ALS step \n",
      "EPOCH 584 MODEL LOSS 1.117347\n",
      "585 / 1000\n",
      "ALS step \n",
      "EPOCH 585 MODEL LOSS 1.421370\n",
      "586 / 1000\n",
      "ALS step \n",
      "EPOCH 586 MODEL LOSS 1.501555\n",
      "587 / 1000\n",
      "ALS step \n",
      "EPOCH 587 MODEL LOSS 1.899983\n",
      "588 / 1000\n",
      "ALS step \n",
      "EPOCH 588 MODEL LOSS 1.331811\n",
      "589 / 1000\n",
      "ALS step \n",
      "EPOCH 589 MODEL LOSS 1.164882\n",
      "590 / 1000\n",
      "ALS step \n",
      "EPOCH 590 MODEL LOSS 1.161580\n",
      "591 / 1000\n",
      "ALS step \n",
      "EPOCH 591 MODEL LOSS 1.343519\n",
      "592 / 1000\n",
      "ALS step \n",
      "EPOCH 592 MODEL LOSS 1.676885\n",
      "593 / 1000\n",
      "ALS step \n",
      "EPOCH 593 MODEL LOSS 2.184815\n",
      "594 / 1000\n",
      "ALS step \n",
      "EPOCH 594 MODEL LOSS 1.439630\n",
      "595 / 1000\n",
      "ALS step \n",
      "EPOCH 595 MODEL LOSS 1.660931\n",
      "596 / 1000\n",
      "ALS step \n",
      "EPOCH 596 MODEL LOSS 1.461278\n",
      "597 / 1000\n",
      "ALS step \n",
      "EPOCH 597 MODEL LOSS 1.392107\n",
      "598 / 1000\n",
      "ALS step \n",
      "EPOCH 598 MODEL LOSS 1.191611\n",
      "599 / 1000\n",
      "ALS step \n",
      "EPOCH 599 MODEL LOSS 1.115604\n",
      "600 / 1000\n",
      "ALS step \n",
      "EPOCH 600 MODEL LOSS 1.392646\n",
      "601 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.462287\n",
      "EPOCH 601 MODEL LOSS 1.518755\n",
      "602 / 1000\n",
      "ALS step \n",
      "EPOCH 602 MODEL LOSS 1.632692\n",
      "603 / 1000\n",
      "ALS step \n",
      "EPOCH 603 MODEL LOSS 1.280974\n",
      "604 / 1000\n",
      "ALS step \n",
      "EPOCH 604 MODEL LOSS 1.345610\n",
      "605 / 1000\n",
      "ALS step \n",
      "EPOCH 605 MODEL LOSS 1.138466\n",
      "606 / 1000\n",
      "ALS step \n",
      "EPOCH 606 MODEL LOSS 1.197675\n",
      "607 / 1000\n",
      "ALS step \n",
      "EPOCH 607 MODEL LOSS 1.423312\n",
      "608 / 1000\n",
      "ALS step \n",
      "EPOCH 608 MODEL LOSS 2.021609\n",
      "609 / 1000\n",
      "ALS step \n",
      "EPOCH 609 MODEL LOSS 1.728633\n",
      "610 / 1000\n",
      "ALS step \n",
      "EPOCH 610 MODEL LOSS 1.434536\n",
      "611 / 1000\n",
      "ALS step \n",
      "EPOCH 611 MODEL LOSS 1.196021\n",
      "612 / 1000\n",
      "ALS step \n",
      "EPOCH 612 MODEL LOSS 1.485554\n",
      "613 / 1000\n",
      "ALS step \n",
      "EPOCH 613 MODEL LOSS 1.540409\n",
      "614 / 1000\n",
      "ALS step \n",
      "EPOCH 614 MODEL LOSS 1.457835\n",
      "615 / 1000\n",
      "ALS step \n",
      "EPOCH 615 MODEL LOSS 1.226418\n",
      "616 / 1000\n",
      "ALS step \n",
      "EPOCH 616 MODEL LOSS 1.379395\n",
      "617 / 1000\n",
      "ALS step \n",
      "EPOCH 617 MODEL LOSS 1.328225\n",
      "618 / 1000\n",
      "ALS step \n",
      "EPOCH 618 MODEL LOSS 1.260213\n",
      "619 / 1000\n",
      "ALS step \n",
      "EPOCH 619 MODEL LOSS 1.594388\n",
      "620 / 1000\n",
      "ALS step \n",
      "EPOCH 620 MODEL LOSS 1.795134\n",
      "621 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.369797\n",
      "EPOCH 621 MODEL LOSS 1.524718\n",
      "622 / 1000\n",
      "ALS step \n",
      "EPOCH 622 MODEL LOSS 1.514664\n",
      "623 / 1000\n",
      "ALS step \n",
      "EPOCH 623 MODEL LOSS 1.593414\n",
      "624 / 1000\n",
      "ALS step \n",
      "EPOCH 624 MODEL LOSS 1.084424\n",
      "625 / 1000\n",
      "ALS step \n",
      "EPOCH 625 MODEL LOSS 1.213860\n",
      "626 / 1000\n",
      "ALS step \n",
      "EPOCH 626 MODEL LOSS 1.426322\n",
      "627 / 1000\n",
      "ALS step \n",
      "EPOCH 627 MODEL LOSS 1.281808\n",
      "628 / 1000\n",
      "ALS step \n",
      "EPOCH 628 MODEL LOSS 1.787732\n",
      "629 / 1000\n",
      "ALS step \n",
      "EPOCH 629 MODEL LOSS 1.691187\n",
      "630 / 1000\n",
      "ALS step \n",
      "EPOCH 630 MODEL LOSS 1.736108\n",
      "631 / 1000\n",
      "ALS step \n",
      "EPOCH 631 MODEL LOSS 1.494293\n",
      "632 / 1000\n",
      "ALS step \n",
      "EPOCH 632 MODEL LOSS 1.270216\n",
      "633 / 1000\n",
      "ALS step \n",
      "EPOCH 633 MODEL LOSS 1.645809\n",
      "634 / 1000\n",
      "ALS step \n",
      "EPOCH 634 MODEL LOSS 1.402425\n",
      "635 / 1000\n",
      "ALS step \n",
      "EPOCH 635 MODEL LOSS 1.407602\n",
      "636 / 1000\n",
      "ALS step \n",
      "EPOCH 636 MODEL LOSS 1.556422\n",
      "637 / 1000\n",
      "ALS step \n",
      "EPOCH 637 MODEL LOSS 1.191381\n",
      "638 / 1000\n",
      "ALS step \n",
      "EPOCH 638 MODEL LOSS 1.544368\n",
      "639 / 1000\n",
      "ALS step \n",
      "EPOCH 639 MODEL LOSS 1.457762\n",
      "640 / 1000\n",
      "ALS step \n",
      "EPOCH 640 MODEL LOSS 1.563529\n",
      "641 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.396846\n",
      "EPOCH 641 MODEL LOSS 1.065765\n",
      "642 / 1000\n",
      "ALS step \n",
      "EPOCH 642 MODEL LOSS 1.296701\n",
      "643 / 1000\n",
      "ALS step \n",
      "EPOCH 643 MODEL LOSS 1.481146\n",
      "644 / 1000\n",
      "ALS step \n",
      "EPOCH 644 MODEL LOSS 1.333369\n",
      "645 / 1000\n",
      "ALS step \n",
      "EPOCH 645 MODEL LOSS 2.460300\n",
      "646 / 1000\n",
      "ALS step \n",
      "EPOCH 646 MODEL LOSS 1.011996\n",
      "647 / 1000\n",
      "ALS step \n",
      "EPOCH 647 MODEL LOSS 1.686165\n",
      "648 / 1000\n",
      "ALS step \n",
      "EPOCH 648 MODEL LOSS 0.968044\n",
      "649 / 1000\n",
      "ALS step \n",
      "EPOCH 649 MODEL LOSS 1.483687\n",
      "650 / 1000\n",
      "ALS step \n",
      "EPOCH 650 MODEL LOSS 1.819797\n",
      "651 / 1000\n",
      "ALS step \n",
      "EPOCH 651 MODEL LOSS 2.226537\n",
      "652 / 1000\n",
      "ALS step \n",
      "EPOCH 652 MODEL LOSS 1.353275\n",
      "653 / 1000\n",
      "ALS step \n",
      "EPOCH 653 MODEL LOSS 1.719158\n",
      "654 / 1000\n",
      "ALS step \n",
      "EPOCH 654 MODEL LOSS 1.649010\n",
      "655 / 1000\n",
      "ALS step \n",
      "EPOCH 655 MODEL LOSS 1.455783\n",
      "656 / 1000\n",
      "ALS step \n",
      "EPOCH 656 MODEL LOSS 1.334871\n",
      "657 / 1000\n",
      "ALS step \n",
      "EPOCH 657 MODEL LOSS 1.617366\n",
      "658 / 1000\n",
      "ALS step \n",
      "EPOCH 658 MODEL LOSS 1.507242\n",
      "659 / 1000\n",
      "ALS step \n",
      "EPOCH 659 MODEL LOSS 1.399445\n",
      "660 / 1000\n",
      "ALS step \n",
      "EPOCH 660 MODEL LOSS 1.129002\n",
      "661 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.406420\n",
      "EPOCH 661 MODEL LOSS 1.210608\n",
      "662 / 1000\n",
      "ALS step \n",
      "EPOCH 662 MODEL LOSS 1.221443\n",
      "663 / 1000\n",
      "ALS step \n",
      "EPOCH 663 MODEL LOSS 1.525646\n",
      "664 / 1000\n",
      "ALS step \n",
      "EPOCH 664 MODEL LOSS 1.359518\n",
      "665 / 1000\n",
      "ALS step \n",
      "EPOCH 665 MODEL LOSS 1.573449\n",
      "666 / 1000\n",
      "ALS step \n",
      "EPOCH 666 MODEL LOSS 1.113124\n",
      "667 / 1000\n",
      "ALS step \n",
      "EPOCH 667 MODEL LOSS 1.339869\n",
      "668 / 1000\n",
      "ALS step \n",
      "EPOCH 668 MODEL LOSS 1.616693\n",
      "669 / 1000\n",
      "ALS step \n",
      "EPOCH 669 MODEL LOSS 1.740390\n",
      "670 / 1000\n",
      "ALS step \n",
      "EPOCH 670 MODEL LOSS 1.405248\n",
      "671 / 1000\n",
      "ALS step \n",
      "EPOCH 671 MODEL LOSS 1.392810\n",
      "672 / 1000\n",
      "ALS step \n",
      "EPOCH 672 MODEL LOSS 1.606521\n",
      "673 / 1000\n",
      "ALS step \n",
      "EPOCH 673 MODEL LOSS 1.225636\n",
      "674 / 1000\n",
      "ALS step \n",
      "EPOCH 674 MODEL LOSS 1.634226\n",
      "675 / 1000\n",
      "ALS step \n",
      "EPOCH 675 MODEL LOSS 1.523975\n",
      "676 / 1000\n",
      "ALS step \n",
      "EPOCH 676 MODEL LOSS 1.206498\n",
      "677 / 1000\n",
      "ALS step \n",
      "EPOCH 677 MODEL LOSS 1.210180\n",
      "678 / 1000\n",
      "ALS step \n",
      "EPOCH 678 MODEL LOSS 1.559044\n",
      "679 / 1000\n",
      "ALS step \n",
      "EPOCH 679 MODEL LOSS 1.711447\n",
      "680 / 1000\n",
      "ALS step \n",
      "EPOCH 680 MODEL LOSS 1.435638\n",
      "681 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.367895\n",
      "EPOCH 681 MODEL LOSS 1.772415\n",
      "682 / 1000\n",
      "ALS step \n",
      "EPOCH 682 MODEL LOSS 2.092063\n",
      "683 / 1000\n",
      "ALS step \n",
      "EPOCH 683 MODEL LOSS 1.804973\n",
      "684 / 1000\n",
      "ALS step \n",
      "EPOCH 684 MODEL LOSS 1.116476\n",
      "685 / 1000\n",
      "ALS step \n",
      "EPOCH 685 MODEL LOSS 1.146269\n",
      "686 / 1000\n",
      "ALS step \n",
      "EPOCH 686 MODEL LOSS 1.109701\n",
      "687 / 1000\n",
      "ALS step \n",
      "EPOCH 687 MODEL LOSS 1.333595\n",
      "688 / 1000\n",
      "ALS step \n",
      "EPOCH 688 MODEL LOSS 1.664525\n",
      "689 / 1000\n",
      "ALS step \n",
      "EPOCH 689 MODEL LOSS 1.656984\n",
      "690 / 1000\n",
      "ALS step \n",
      "EPOCH 690 MODEL LOSS 1.806948\n",
      "691 / 1000\n",
      "ALS step \n",
      "EPOCH 691 MODEL LOSS 1.341730\n",
      "692 / 1000\n",
      "ALS step \n",
      "EPOCH 692 MODEL LOSS 1.863068\n",
      "693 / 1000\n",
      "ALS step \n",
      "EPOCH 693 MODEL LOSS 1.297236\n",
      "694 / 1000\n",
      "ALS step \n",
      "EPOCH 694 MODEL LOSS 1.337516\n",
      "695 / 1000\n",
      "ALS step \n",
      "EPOCH 695 MODEL LOSS 1.421994\n",
      "696 / 1000\n",
      "ALS step \n",
      "EPOCH 696 MODEL LOSS 1.292109\n",
      "697 / 1000\n",
      "ALS step \n",
      "EPOCH 697 MODEL LOSS 1.288520\n",
      "698 / 1000\n",
      "ALS step \n",
      "EPOCH 698 MODEL LOSS 1.817935\n",
      "699 / 1000\n",
      "ALS step \n",
      "EPOCH 699 MODEL LOSS 1.851065\n",
      "700 / 1000\n",
      "ALS step \n",
      "EPOCH 700 MODEL LOSS 1.753781\n",
      "701 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.353901\n",
      "EPOCH 701 MODEL LOSS 2.000214\n",
      "702 / 1000\n",
      "ALS step \n",
      "EPOCH 702 MODEL LOSS 1.558132\n",
      "703 / 1000\n",
      "ALS step \n",
      "EPOCH 703 MODEL LOSS 2.237331\n",
      "704 / 1000\n",
      "ALS step \n",
      "EPOCH 704 MODEL LOSS 2.065011\n",
      "705 / 1000\n",
      "ALS step \n",
      "EPOCH 705 MODEL LOSS 1.552793\n",
      "706 / 1000\n",
      "ALS step \n",
      "EPOCH 706 MODEL LOSS 1.827520\n",
      "707 / 1000\n",
      "ALS step \n",
      "EPOCH 707 MODEL LOSS 1.208129\n",
      "708 / 1000\n",
      "ALS step \n",
      "EPOCH 708 MODEL LOSS 1.272377\n",
      "709 / 1000\n",
      "ALS step \n",
      "EPOCH 709 MODEL LOSS 1.439772\n",
      "710 / 1000\n",
      "ALS step \n",
      "EPOCH 710 MODEL LOSS 1.178037\n",
      "711 / 1000\n",
      "ALS step \n",
      "EPOCH 711 MODEL LOSS 1.697274\n",
      "712 / 1000\n",
      "ALS step \n",
      "EPOCH 712 MODEL LOSS 1.504806\n",
      "713 / 1000\n",
      "ALS step \n",
      "EPOCH 713 MODEL LOSS 1.782454\n",
      "714 / 1000\n",
      "ALS step \n",
      "EPOCH 714 MODEL LOSS 1.113459\n",
      "715 / 1000\n",
      "ALS step \n",
      "EPOCH 715 MODEL LOSS 1.362836\n",
      "716 / 1000\n",
      "ALS step \n",
      "EPOCH 716 MODEL LOSS 1.463849\n",
      "717 / 1000\n",
      "ALS step \n",
      "EPOCH 717 MODEL LOSS 1.534774\n",
      "718 / 1000\n",
      "ALS step \n",
      "EPOCH 718 MODEL LOSS 1.319272\n",
      "719 / 1000\n",
      "ALS step \n",
      "EPOCH 719 MODEL LOSS 1.105110\n",
      "720 / 1000\n",
      "ALS step \n",
      "EPOCH 720 MODEL LOSS 1.772334\n",
      "721 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.391443\n",
      "EPOCH 721 MODEL LOSS 1.306490\n",
      "722 / 1000\n",
      "ALS step \n",
      "EPOCH 722 MODEL LOSS 1.818756\n",
      "723 / 1000\n",
      "ALS step \n",
      "EPOCH 723 MODEL LOSS 1.189314\n",
      "724 / 1000\n",
      "ALS step \n",
      "EPOCH 724 MODEL LOSS 1.348198\n",
      "725 / 1000\n",
      "ALS step \n",
      "EPOCH 725 MODEL LOSS 1.392135\n",
      "726 / 1000\n",
      "ALS step \n",
      "EPOCH 726 MODEL LOSS 1.237679\n",
      "727 / 1000\n",
      "ALS step \n",
      "EPOCH 727 MODEL LOSS 1.229000\n",
      "728 / 1000\n",
      "ALS step \n",
      "EPOCH 728 MODEL LOSS 1.772426\n",
      "729 / 1000\n",
      "ALS step \n",
      "EPOCH 729 MODEL LOSS 1.616457\n",
      "730 / 1000\n",
      "ALS step \n",
      "EPOCH 730 MODEL LOSS 1.485420\n",
      "731 / 1000\n",
      "ALS step \n",
      "EPOCH 731 MODEL LOSS 1.772807\n",
      "732 / 1000\n",
      "ALS step \n",
      "EPOCH 732 MODEL LOSS 1.479738\n",
      "733 / 1000\n",
      "ALS step \n",
      "EPOCH 733 MODEL LOSS 1.051571\n",
      "734 / 1000\n",
      "ALS step \n",
      "EPOCH 734 MODEL LOSS 1.030925\n",
      "735 / 1000\n",
      "ALS step \n",
      "EPOCH 735 MODEL LOSS 2.018934\n",
      "736 / 1000\n",
      "ALS step \n",
      "EPOCH 736 MODEL LOSS 1.503340\n",
      "737 / 1000\n",
      "ALS step \n",
      "EPOCH 737 MODEL LOSS 1.460889\n",
      "738 / 1000\n",
      "ALS step \n",
      "EPOCH 738 MODEL LOSS 1.619939\n",
      "739 / 1000\n",
      "ALS step \n",
      "EPOCH 739 MODEL LOSS 1.349955\n",
      "740 / 1000\n",
      "ALS step \n",
      "EPOCH 740 MODEL LOSS 1.083084\n",
      "741 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.374645\n",
      "EPOCH 741 MODEL LOSS 1.430025\n",
      "742 / 1000\n",
      "ALS step \n",
      "EPOCH 742 MODEL LOSS 1.217190\n",
      "743 / 1000\n",
      "ALS step \n",
      "EPOCH 743 MODEL LOSS 1.710251\n",
      "744 / 1000\n",
      "ALS step \n",
      "EPOCH 744 MODEL LOSS 1.331240\n",
      "745 / 1000\n",
      "ALS step \n",
      "EPOCH 745 MODEL LOSS 1.709654\n",
      "746 / 1000\n",
      "ALS step \n",
      "EPOCH 746 MODEL LOSS 1.422483\n",
      "747 / 1000\n",
      "ALS step \n",
      "EPOCH 747 MODEL LOSS 1.123933\n",
      "748 / 1000\n",
      "ALS step \n",
      "EPOCH 748 MODEL LOSS 1.160541\n",
      "749 / 1000\n",
      "ALS step \n",
      "EPOCH 749 MODEL LOSS 1.175878\n",
      "750 / 1000\n",
      "ALS step \n",
      "EPOCH 750 MODEL LOSS 1.114789\n",
      "751 / 1000\n",
      "ALS step \n",
      "EPOCH 751 MODEL LOSS 1.147474\n",
      "752 / 1000\n",
      "ALS step \n",
      "EPOCH 752 MODEL LOSS 1.235346\n",
      "753 / 1000\n",
      "ALS step \n",
      "EPOCH 753 MODEL LOSS 1.801233\n",
      "754 / 1000\n",
      "ALS step \n",
      "EPOCH 754 MODEL LOSS 1.820721\n",
      "755 / 1000\n",
      "ALS step \n",
      "EPOCH 755 MODEL LOSS 1.159498\n",
      "756 / 1000\n",
      "ALS step \n",
      "EPOCH 756 MODEL LOSS 1.760140\n",
      "757 / 1000\n",
      "ALS step \n",
      "EPOCH 757 MODEL LOSS 1.383784\n",
      "758 / 1000\n",
      "ALS step \n",
      "EPOCH 758 MODEL LOSS 1.591471\n",
      "759 / 1000\n",
      "ALS step \n",
      "EPOCH 759 MODEL LOSS 1.407427\n",
      "760 / 1000\n",
      "ALS step \n",
      "EPOCH 760 MODEL LOSS 1.506244\n",
      "761 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.392156\n",
      "EPOCH 761 MODEL LOSS 1.222197\n",
      "762 / 1000\n",
      "ALS step \n",
      "EPOCH 762 MODEL LOSS 1.055143\n",
      "763 / 1000\n",
      "ALS step \n",
      "EPOCH 763 MODEL LOSS 1.719615\n",
      "764 / 1000\n",
      "ALS step \n",
      "EPOCH 764 MODEL LOSS 1.150107\n",
      "765 / 1000\n",
      "ALS step \n",
      "EPOCH 765 MODEL LOSS 1.297131\n",
      "766 / 1000\n",
      "ALS step \n",
      "EPOCH 766 MODEL LOSS 1.523054\n",
      "767 / 1000\n",
      "ALS step \n",
      "EPOCH 767 MODEL LOSS 1.478559\n",
      "768 / 1000\n",
      "ALS step \n",
      "EPOCH 768 MODEL LOSS 1.048154\n",
      "769 / 1000\n",
      "ALS step \n",
      "EPOCH 769 MODEL LOSS 2.140415\n",
      "770 / 1000\n",
      "ALS step \n",
      "EPOCH 770 MODEL LOSS 1.384747\n",
      "771 / 1000\n",
      "ALS step \n",
      "EPOCH 771 MODEL LOSS 1.523801\n",
      "772 / 1000\n",
      "ALS step \n",
      "EPOCH 772 MODEL LOSS 1.727461\n",
      "773 / 1000\n",
      "ALS step \n",
      "EPOCH 773 MODEL LOSS 1.109018\n",
      "774 / 1000\n",
      "ALS step \n",
      "EPOCH 774 MODEL LOSS 1.574546\n",
      "775 / 1000\n",
      "ALS step \n",
      "EPOCH 775 MODEL LOSS 1.671535\n",
      "776 / 1000\n",
      "ALS step \n",
      "EPOCH 776 MODEL LOSS 1.488725\n",
      "777 / 1000\n",
      "ALS step \n",
      "EPOCH 777 MODEL LOSS 1.558803\n",
      "778 / 1000\n",
      "ALS step \n",
      "EPOCH 778 MODEL LOSS 1.338316\n",
      "779 / 1000\n",
      "ALS step \n",
      "EPOCH 779 MODEL LOSS 1.696661\n",
      "780 / 1000\n",
      "ALS step \n",
      "EPOCH 780 MODEL LOSS 1.860775\n",
      "781 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.371515\n",
      "EPOCH 781 MODEL LOSS 1.138342\n",
      "782 / 1000\n",
      "ALS step \n",
      "EPOCH 782 MODEL LOSS 1.125240\n",
      "783 / 1000\n",
      "ALS step \n",
      "EPOCH 783 MODEL LOSS 1.177141\n",
      "784 / 1000\n",
      "ALS step \n",
      "EPOCH 784 MODEL LOSS 1.580795\n",
      "785 / 1000\n",
      "ALS step \n",
      "EPOCH 785 MODEL LOSS 1.409775\n",
      "786 / 1000\n",
      "ALS step \n",
      "EPOCH 786 MODEL LOSS 1.567637\n",
      "787 / 1000\n",
      "ALS step \n",
      "EPOCH 787 MODEL LOSS 1.648194\n",
      "788 / 1000\n",
      "ALS step \n",
      "EPOCH 788 MODEL LOSS 1.722700\n",
      "789 / 1000\n",
      "ALS step \n",
      "EPOCH 789 MODEL LOSS 0.984308\n",
      "790 / 1000\n",
      "ALS step \n",
      "EPOCH 790 MODEL LOSS 1.838063\n",
      "791 / 1000\n",
      "ALS step \n",
      "EPOCH 791 MODEL LOSS 1.384060\n",
      "792 / 1000\n",
      "ALS step \n",
      "EPOCH 792 MODEL LOSS 1.487628\n",
      "793 / 1000\n",
      "ALS step \n",
      "EPOCH 793 MODEL LOSS 1.188628\n",
      "794 / 1000\n",
      "ALS step \n",
      "EPOCH 794 MODEL LOSS 1.179504\n",
      "795 / 1000\n",
      "ALS step \n",
      "EPOCH 795 MODEL LOSS 1.477380\n",
      "796 / 1000\n",
      "ALS step \n",
      "EPOCH 796 MODEL LOSS 1.593575\n",
      "797 / 1000\n",
      "ALS step \n",
      "EPOCH 797 MODEL LOSS 1.411119\n",
      "798 / 1000\n",
      "ALS step \n",
      "EPOCH 798 MODEL LOSS 1.452438\n",
      "799 / 1000\n",
      "ALS step \n",
      "EPOCH 799 MODEL LOSS 1.699171\n",
      "800 / 1000\n",
      "ALS step \n",
      "EPOCH 800 MODEL LOSS 1.293050\n",
      "801 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.365430\n",
      "EPOCH 801 MODEL LOSS 1.590186\n",
      "802 / 1000\n",
      "ALS step \n",
      "EPOCH 802 MODEL LOSS 1.218847\n",
      "803 / 1000\n",
      "ALS step \n",
      "EPOCH 803 MODEL LOSS 1.371866\n",
      "804 / 1000\n",
      "ALS step \n",
      "EPOCH 804 MODEL LOSS 1.148767\n",
      "805 / 1000\n",
      "ALS step \n",
      "EPOCH 805 MODEL LOSS 1.289576\n",
      "806 / 1000\n",
      "ALS step \n",
      "EPOCH 806 MODEL LOSS 1.720078\n",
      "807 / 1000\n",
      "ALS step \n",
      "EPOCH 807 MODEL LOSS 1.082929\n",
      "808 / 1000\n",
      "ALS step \n",
      "EPOCH 808 MODEL LOSS 1.290309\n",
      "809 / 1000\n",
      "ALS step \n",
      "EPOCH 809 MODEL LOSS 1.685556\n",
      "810 / 1000\n",
      "ALS step \n",
      "EPOCH 810 MODEL LOSS 1.780450\n",
      "811 / 1000\n",
      "ALS step \n",
      "EPOCH 811 MODEL LOSS 1.414574\n",
      "812 / 1000\n",
      "ALS step \n",
      "EPOCH 812 MODEL LOSS 1.037502\n",
      "813 / 1000\n",
      "ALS step \n",
      "EPOCH 813 MODEL LOSS 1.575082\n",
      "814 / 1000\n",
      "ALS step \n",
      "EPOCH 814 MODEL LOSS 1.976923\n",
      "815 / 1000\n",
      "ALS step \n",
      "EPOCH 815 MODEL LOSS 1.570735\n",
      "816 / 1000\n",
      "ALS step \n",
      "EPOCH 816 MODEL LOSS 1.090263\n",
      "817 / 1000\n",
      "ALS step \n",
      "EPOCH 817 MODEL LOSS 1.270850\n",
      "818 / 1000\n",
      "ALS step \n",
      "EPOCH 818 MODEL LOSS 1.374946\n",
      "819 / 1000\n",
      "ALS step \n",
      "EPOCH 819 MODEL LOSS 1.180938\n",
      "820 / 1000\n",
      "ALS step \n",
      "EPOCH 820 MODEL LOSS 1.598680\n",
      "821 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.413837\n",
      "EPOCH 821 MODEL LOSS 1.055270\n",
      "822 / 1000\n",
      "ALS step \n",
      "EPOCH 822 MODEL LOSS 2.239164\n",
      "823 / 1000\n",
      "ALS step \n",
      "EPOCH 823 MODEL LOSS 1.280839\n",
      "824 / 1000\n",
      "ALS step \n",
      "EPOCH 824 MODEL LOSS 1.257359\n",
      "825 / 1000\n",
      "ALS step \n",
      "EPOCH 825 MODEL LOSS 1.223591\n",
      "826 / 1000\n",
      "ALS step \n",
      "EPOCH 826 MODEL LOSS 1.359027\n",
      "827 / 1000\n",
      "ALS step \n",
      "EPOCH 827 MODEL LOSS 1.558780\n",
      "828 / 1000\n",
      "ALS step \n",
      "EPOCH 828 MODEL LOSS 1.114607\n",
      "829 / 1000\n",
      "ALS step \n",
      "EPOCH 829 MODEL LOSS 1.562430\n",
      "830 / 1000\n",
      "ALS step \n",
      "EPOCH 830 MODEL LOSS 1.756207\n",
      "831 / 1000\n",
      "ALS step \n",
      "EPOCH 831 MODEL LOSS 1.157549\n",
      "832 / 1000\n",
      "ALS step \n",
      "EPOCH 832 MODEL LOSS 1.403239\n",
      "833 / 1000\n",
      "ALS step \n",
      "EPOCH 833 MODEL LOSS 1.882029\n",
      "834 / 1000\n",
      "ALS step \n",
      "EPOCH 834 MODEL LOSS 2.233099\n",
      "835 / 1000\n",
      "ALS step \n",
      "EPOCH 835 MODEL LOSS 1.640940\n",
      "836 / 1000\n",
      "ALS step \n",
      "EPOCH 836 MODEL LOSS 1.626336\n",
      "837 / 1000\n",
      "ALS step \n",
      "EPOCH 837 MODEL LOSS 1.922764\n",
      "838 / 1000\n",
      "ALS step \n",
      "EPOCH 838 MODEL LOSS 1.376884\n",
      "839 / 1000\n",
      "ALS step \n",
      "EPOCH 839 MODEL LOSS 1.530028\n",
      "840 / 1000\n",
      "ALS step \n",
      "EPOCH 840 MODEL LOSS 1.447780\n",
      "841 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.398930\n",
      "EPOCH 841 MODEL LOSS 1.149927\n",
      "842 / 1000\n",
      "ALS step \n",
      "EPOCH 842 MODEL LOSS 1.437328\n",
      "843 / 1000\n",
      "ALS step \n",
      "EPOCH 843 MODEL LOSS 1.299022\n",
      "844 / 1000\n",
      "ALS step \n",
      "EPOCH 844 MODEL LOSS 1.825499\n",
      "845 / 1000\n",
      "ALS step \n",
      "EPOCH 845 MODEL LOSS 1.268291\n",
      "846 / 1000\n",
      "ALS step \n",
      "EPOCH 846 MODEL LOSS 1.578863\n",
      "847 / 1000\n",
      "ALS step \n",
      "EPOCH 847 MODEL LOSS 1.736314\n",
      "848 / 1000\n",
      "ALS step \n",
      "EPOCH 848 MODEL LOSS 1.613397\n",
      "849 / 1000\n",
      "ALS step \n",
      "EPOCH 849 MODEL LOSS 1.079528\n",
      "850 / 1000\n",
      "ALS step \n",
      "EPOCH 850 MODEL LOSS 1.336659\n",
      "851 / 1000\n",
      "ALS step \n",
      "EPOCH 851 MODEL LOSS 1.222319\n",
      "852 / 1000\n",
      "ALS step \n",
      "EPOCH 852 MODEL LOSS 1.560503\n",
      "853 / 1000\n",
      "ALS step \n",
      "EPOCH 853 MODEL LOSS 1.039936\n",
      "854 / 1000\n",
      "ALS step \n",
      "EPOCH 854 MODEL LOSS 1.013508\n",
      "855 / 1000\n",
      "ALS step \n",
      "EPOCH 855 MODEL LOSS 1.108141\n",
      "856 / 1000\n",
      "ALS step \n",
      "EPOCH 856 MODEL LOSS 1.158734\n",
      "857 / 1000\n",
      "ALS step \n",
      "EPOCH 857 MODEL LOSS 1.183629\n",
      "858 / 1000\n",
      "ALS step \n",
      "EPOCH 858 MODEL LOSS 1.494411\n",
      "859 / 1000\n",
      "ALS step \n",
      "EPOCH 859 MODEL LOSS 1.853949\n",
      "860 / 1000\n",
      "ALS step \n",
      "EPOCH 860 MODEL LOSS 1.460116\n",
      "861 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.374341\n",
      "EPOCH 861 MODEL LOSS 1.528163\n",
      "862 / 1000\n",
      "ALS step \n",
      "EPOCH 862 MODEL LOSS 1.078193\n",
      "863 / 1000\n",
      "ALS step \n",
      "EPOCH 863 MODEL LOSS 1.685214\n",
      "864 / 1000\n",
      "ALS step \n",
      "EPOCH 864 MODEL LOSS 1.137217\n",
      "865 / 1000\n",
      "ALS step \n",
      "EPOCH 865 MODEL LOSS 1.563956\n",
      "866 / 1000\n",
      "ALS step \n",
      "EPOCH 866 MODEL LOSS 1.424562\n",
      "867 / 1000\n",
      "ALS step \n",
      "EPOCH 867 MODEL LOSS 1.305394\n",
      "868 / 1000\n",
      "ALS step \n",
      "EPOCH 868 MODEL LOSS 1.640224\n",
      "869 / 1000\n",
      "ALS step \n",
      "EPOCH 869 MODEL LOSS 1.415216\n",
      "870 / 1000\n",
      "ALS step \n",
      "EPOCH 870 MODEL LOSS 1.339401\n",
      "871 / 1000\n",
      "ALS step \n",
      "EPOCH 871 MODEL LOSS 1.611105\n",
      "872 / 1000\n",
      "ALS step \n",
      "EPOCH 872 MODEL LOSS 1.338131\n",
      "873 / 1000\n",
      "ALS step \n",
      "EPOCH 873 MODEL LOSS 1.112884\n",
      "874 / 1000\n",
      "ALS step \n",
      "EPOCH 874 MODEL LOSS 1.443531\n",
      "875 / 1000\n",
      "ALS step \n",
      "EPOCH 875 MODEL LOSS 1.596675\n",
      "876 / 1000\n",
      "ALS step \n",
      "EPOCH 876 MODEL LOSS 1.502761\n",
      "877 / 1000\n",
      "ALS step \n",
      "EPOCH 877 MODEL LOSS 1.409142\n",
      "878 / 1000\n",
      "ALS step \n",
      "EPOCH 878 MODEL LOSS 1.693051\n",
      "879 / 1000\n",
      "ALS step \n",
      "EPOCH 879 MODEL LOSS 1.688939\n",
      "880 / 1000\n",
      "ALS step \n",
      "EPOCH 880 MODEL LOSS 1.350456\n",
      "881 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.338248\n",
      "EPOCH 881 MODEL LOSS 1.617005\n",
      "882 / 1000\n",
      "ALS step \n",
      "EPOCH 882 MODEL LOSS 1.336903\n",
      "883 / 1000\n",
      "ALS step \n",
      "EPOCH 883 MODEL LOSS 1.249222\n",
      "884 / 1000\n",
      "ALS step \n",
      "EPOCH 884 MODEL LOSS 1.044896\n",
      "885 / 1000\n",
      "ALS step \n",
      "EPOCH 885 MODEL LOSS 1.648215\n",
      "886 / 1000\n",
      "ALS step \n",
      "EPOCH 886 MODEL LOSS 1.182230\n",
      "887 / 1000\n",
      "ALS step \n",
      "EPOCH 887 MODEL LOSS 1.192419\n",
      "888 / 1000\n",
      "ALS step \n",
      "EPOCH 888 MODEL LOSS 1.220516\n",
      "889 / 1000\n",
      "ALS step \n",
      "EPOCH 889 MODEL LOSS 1.113539\n",
      "890 / 1000\n",
      "ALS step \n",
      "EPOCH 890 MODEL LOSS 1.220042\n",
      "891 / 1000\n",
      "ALS step \n",
      "EPOCH 891 MODEL LOSS 1.101491\n",
      "892 / 1000\n",
      "ALS step \n",
      "EPOCH 892 MODEL LOSS 1.165865\n",
      "893 / 1000\n",
      "ALS step \n",
      "EPOCH 893 MODEL LOSS 1.347243\n",
      "894 / 1000\n",
      "ALS step \n",
      "EPOCH 894 MODEL LOSS 1.040816\n",
      "895 / 1000\n",
      "ALS step \n",
      "EPOCH 895 MODEL LOSS 1.052382\n",
      "896 / 1000\n",
      "ALS step \n",
      "EPOCH 896 MODEL LOSS 1.381889\n",
      "897 / 1000\n",
      "ALS step \n",
      "EPOCH 897 MODEL LOSS 1.065391\n",
      "898 / 1000\n",
      "ALS step \n",
      "EPOCH 898 MODEL LOSS 1.140922\n",
      "899 / 1000\n",
      "ALS step \n",
      "EPOCH 899 MODEL LOSS 1.587057\n",
      "900 / 1000\n",
      "ALS step \n",
      "EPOCH 900 MODEL LOSS 1.204331\n",
      "901 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.411499\n",
      "EPOCH 901 MODEL LOSS 1.692842\n",
      "902 / 1000\n",
      "ALS step \n",
      "EPOCH 902 MODEL LOSS 1.366807\n",
      "903 / 1000\n",
      "ALS step \n",
      "EPOCH 903 MODEL LOSS 1.306447\n",
      "904 / 1000\n",
      "ALS step \n",
      "EPOCH 904 MODEL LOSS 1.457292\n",
      "905 / 1000\n",
      "ALS step \n",
      "EPOCH 905 MODEL LOSS 1.186102\n",
      "906 / 1000\n",
      "ALS step \n",
      "EPOCH 906 MODEL LOSS 1.381275\n",
      "907 / 1000\n",
      "ALS step \n",
      "EPOCH 907 MODEL LOSS 1.183898\n",
      "908 / 1000\n",
      "ALS step \n",
      "EPOCH 908 MODEL LOSS 1.738588\n",
      "909 / 1000\n",
      "ALS step \n",
      "EPOCH 909 MODEL LOSS 1.272684\n",
      "910 / 1000\n",
      "ALS step \n",
      "EPOCH 910 MODEL LOSS 1.392347\n",
      "911 / 1000\n",
      "ALS step \n",
      "EPOCH 911 MODEL LOSS 1.390685\n",
      "912 / 1000\n",
      "ALS step \n",
      "EPOCH 912 MODEL LOSS 1.686563\n",
      "913 / 1000\n",
      "ALS step \n",
      "EPOCH 913 MODEL LOSS 1.830369\n",
      "914 / 1000\n",
      "ALS step \n",
      "EPOCH 914 MODEL LOSS 1.340358\n",
      "915 / 1000\n",
      "ALS step \n",
      "EPOCH 915 MODEL LOSS 1.791678\n",
      "916 / 1000\n",
      "ALS step \n",
      "EPOCH 916 MODEL LOSS 1.154475\n",
      "917 / 1000\n",
      "ALS step \n",
      "EPOCH 917 MODEL LOSS 1.430697\n",
      "918 / 1000\n",
      "ALS step \n",
      "EPOCH 918 MODEL LOSS 1.298166\n",
      "919 / 1000\n",
      "ALS step \n",
      "EPOCH 919 MODEL LOSS 1.293710\n",
      "920 / 1000\n",
      "ALS step \n",
      "EPOCH 920 MODEL LOSS 1.941242\n",
      "921 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.412639\n",
      "EPOCH 921 MODEL LOSS 1.546459\n",
      "922 / 1000\n",
      "ALS step \n",
      "EPOCH 922 MODEL LOSS 1.131154\n",
      "923 / 1000\n",
      "ALS step \n",
      "EPOCH 923 MODEL LOSS 1.168628\n",
      "924 / 1000\n",
      "ALS step \n",
      "EPOCH 924 MODEL LOSS 1.248037\n",
      "925 / 1000\n",
      "ALS step \n",
      "EPOCH 925 MODEL LOSS 1.213171\n",
      "926 / 1000\n",
      "ALS step \n",
      "EPOCH 926 MODEL LOSS 1.250068\n",
      "927 / 1000\n",
      "ALS step \n",
      "EPOCH 927 MODEL LOSS 1.277157\n",
      "928 / 1000\n",
      "ALS step \n",
      "EPOCH 928 MODEL LOSS 1.440449\n",
      "929 / 1000\n",
      "ALS step \n",
      "EPOCH 929 MODEL LOSS 1.432097\n",
      "930 / 1000\n",
      "ALS step \n",
      "EPOCH 930 MODEL LOSS 1.648683\n",
      "931 / 1000\n",
      "ALS step \n",
      "EPOCH 931 MODEL LOSS 1.268705\n",
      "932 / 1000\n",
      "ALS step \n",
      "EPOCH 932 MODEL LOSS 1.335065\n",
      "933 / 1000\n",
      "ALS step \n",
      "EPOCH 933 MODEL LOSS 2.003563\n",
      "934 / 1000\n",
      "ALS step \n",
      "EPOCH 934 MODEL LOSS 1.328122\n",
      "935 / 1000\n",
      "ALS step \n",
      "EPOCH 935 MODEL LOSS 1.670444\n",
      "936 / 1000\n",
      "ALS step \n",
      "EPOCH 936 MODEL LOSS 1.541188\n",
      "937 / 1000\n",
      "ALS step \n",
      "EPOCH 937 MODEL LOSS 1.392318\n",
      "938 / 1000\n",
      "ALS step \n",
      "EPOCH 938 MODEL LOSS 2.090314\n",
      "939 / 1000\n",
      "ALS step \n",
      "EPOCH 939 MODEL LOSS 1.654417\n",
      "940 / 1000\n",
      "ALS step \n",
      "EPOCH 940 MODEL LOSS 1.815233\n",
      "941 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.372150\n",
      "EPOCH 941 MODEL LOSS 1.310418\n",
      "942 / 1000\n",
      "ALS step \n",
      "EPOCH 942 MODEL LOSS 1.481023\n",
      "943 / 1000\n",
      "ALS step \n",
      "EPOCH 943 MODEL LOSS 1.392302\n",
      "944 / 1000\n",
      "ALS step \n",
      "EPOCH 944 MODEL LOSS 1.344338\n",
      "945 / 1000\n",
      "ALS step \n",
      "EPOCH 945 MODEL LOSS 1.263639\n",
      "946 / 1000\n",
      "ALS step \n",
      "EPOCH 946 MODEL LOSS 1.221988\n",
      "947 / 1000\n",
      "ALS step \n",
      "EPOCH 947 MODEL LOSS 1.153031\n",
      "948 / 1000\n",
      "ALS step \n",
      "EPOCH 948 MODEL LOSS 1.589518\n",
      "949 / 1000\n",
      "ALS step \n",
      "EPOCH 949 MODEL LOSS 1.910460\n",
      "950 / 1000\n",
      "ALS step \n",
      "EPOCH 950 MODEL LOSS 1.191119\n",
      "951 / 1000\n",
      "ALS step \n",
      "EPOCH 951 MODEL LOSS 1.107568\n",
      "952 / 1000\n",
      "ALS step \n",
      "EPOCH 952 MODEL LOSS 1.286160\n",
      "953 / 1000\n",
      "ALS step \n",
      "EPOCH 953 MODEL LOSS 1.469467\n",
      "954 / 1000\n",
      "ALS step \n",
      "EPOCH 954 MODEL LOSS 1.481877\n",
      "955 / 1000\n",
      "ALS step \n",
      "EPOCH 955 MODEL LOSS 1.055277\n",
      "956 / 1000\n",
      "ALS step \n",
      "EPOCH 956 MODEL LOSS 1.092151\n",
      "957 / 1000\n",
      "ALS step \n",
      "EPOCH 957 MODEL LOSS 1.097543\n",
      "958 / 1000\n",
      "ALS step \n",
      "EPOCH 958 MODEL LOSS 1.478140\n",
      "959 / 1000\n",
      "ALS step \n",
      "EPOCH 959 MODEL LOSS 1.777048\n",
      "960 / 1000\n",
      "ALS step \n",
      "EPOCH 960 MODEL LOSS 1.069395\n",
      "961 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.395989\n",
      "EPOCH 961 MODEL LOSS 1.693978\n",
      "962 / 1000\n",
      "ALS step \n",
      "EPOCH 962 MODEL LOSS 1.437876\n",
      "963 / 1000\n",
      "ALS step \n",
      "EPOCH 963 MODEL LOSS 1.293666\n",
      "964 / 1000\n",
      "ALS step \n",
      "EPOCH 964 MODEL LOSS 1.210468\n",
      "965 / 1000\n",
      "ALS step \n",
      "EPOCH 965 MODEL LOSS 1.635688\n",
      "966 / 1000\n",
      "ALS step \n",
      "EPOCH 966 MODEL LOSS 1.303012\n",
      "967 / 1000\n",
      "ALS step \n",
      "EPOCH 967 MODEL LOSS 1.313315\n",
      "968 / 1000\n",
      "ALS step \n",
      "EPOCH 968 MODEL LOSS 1.802709\n",
      "969 / 1000\n",
      "ALS step \n",
      "EPOCH 969 MODEL LOSS 1.302136\n",
      "970 / 1000\n",
      "ALS step \n",
      "EPOCH 970 MODEL LOSS 1.090087\n",
      "971 / 1000\n",
      "ALS step \n",
      "EPOCH 971 MODEL LOSS 1.458935\n",
      "972 / 1000\n",
      "ALS step \n",
      "EPOCH 972 MODEL LOSS 1.382651\n",
      "973 / 1000\n",
      "ALS step \n",
      "EPOCH 973 MODEL LOSS 1.805350\n",
      "974 / 1000\n",
      "ALS step \n",
      "EPOCH 974 MODEL LOSS 1.792961\n",
      "975 / 1000\n",
      "ALS step \n",
      "EPOCH 975 MODEL LOSS 1.302185\n",
      "976 / 1000\n",
      "ALS step \n",
      "EPOCH 976 MODEL LOSS 1.296343\n",
      "977 / 1000\n",
      "ALS step \n",
      "EPOCH 977 MODEL LOSS 1.809960\n",
      "978 / 1000\n",
      "ALS step \n",
      "EPOCH 978 MODEL LOSS 2.032660\n",
      "979 / 1000\n",
      "ALS step \n",
      "EPOCH 979 MODEL LOSS 1.331277\n",
      "980 / 1000\n",
      "ALS step \n",
      "EPOCH 980 MODEL LOSS 1.411039\n",
      "981 / 1000\n",
      "ALS step \n",
      "ALS LOSS: 0.379981\n",
      "EPOCH 981 MODEL LOSS 1.689873\n",
      "982 / 1000\n",
      "ALS step \n",
      "EPOCH 982 MODEL LOSS 1.153181\n",
      "983 / 1000\n",
      "ALS step \n",
      "EPOCH 983 MODEL LOSS 1.087366\n",
      "984 / 1000\n",
      "ALS step \n",
      "EPOCH 984 MODEL LOSS 1.127574\n",
      "985 / 1000\n",
      "ALS step \n",
      "EPOCH 985 MODEL LOSS 1.062430\n",
      "986 / 1000\n",
      "ALS step \n",
      "EPOCH 986 MODEL LOSS 1.390790\n",
      "987 / 1000\n",
      "ALS step \n",
      "EPOCH 987 MODEL LOSS 1.125028\n",
      "988 / 1000\n",
      "ALS step \n",
      "EPOCH 988 MODEL LOSS 1.549403\n",
      "989 / 1000\n",
      "ALS step \n",
      "EPOCH 989 MODEL LOSS 2.005490\n",
      "990 / 1000\n",
      "ALS step \n",
      "EPOCH 990 MODEL LOSS 1.091702\n",
      "991 / 1000\n",
      "ALS step \n",
      "EPOCH 991 MODEL LOSS 1.390022\n",
      "992 / 1000\n",
      "ALS step \n",
      "EPOCH 992 MODEL LOSS 1.430380\n",
      "993 / 1000\n",
      "ALS step \n",
      "EPOCH 993 MODEL LOSS 1.211949\n",
      "994 / 1000\n",
      "ALS step \n",
      "EPOCH 994 MODEL LOSS 1.586498\n",
      "995 / 1000\n",
      "ALS step \n",
      "EPOCH 995 MODEL LOSS 1.141089\n",
      "996 / 1000\n",
      "ALS step \n",
      "EPOCH 996 MODEL LOSS 1.096367\n",
      "997 / 1000\n",
      "ALS step \n",
      "EPOCH 997 MODEL LOSS 1.741675\n",
      "998 / 1000\n",
      "ALS step \n",
      "EPOCH 998 MODEL LOSS 1.521353\n",
      "999 / 1000\n",
      "ALS step \n",
      "EPOCH 999 MODEL LOSS 1.496235\n",
      "1000 / 1000\n",
      "ALS step \n",
      "EPOCH 1000 MODEL LOSS 1.723467\n"
     ]
    }
   ],
   "source": [
    "cdl = CDL(rm , iim)#item_infomation_matrix)\n",
    "U, V = cdl.training() #188910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(corruption_level, shape):\n",
    "    mask = np.random.binomial(1, 1 - corruption_level, shape)\n",
    "    return mask\n",
    "\n",
    "def add_noise(x , corruption_level ):\n",
    "    mask_ = mask(corruption_level , x.shape)\n",
    "    print(\"Mask shape: \" + str(mask_.shape))\n",
    "    x = np.multiply(x, mask_)\n",
    "    print(\"Noising completed..:\" + str(x.shape))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDL():\n",
    "    def __init__(self , rating_matrix , item_infomation_matrix):\n",
    "        self.xp_name = 'test5_lr00001_k50_250n_withoutput3'\n",
    "        self.base_dir = 'D:/Models/master/'\n",
    "        \n",
    "        os.mkdir('%s/%s/' % (self.base_dir, self.xp_name))\n",
    "        os.mkdir('%s/%s/tf/' % (self.base_dir, self.xp_name))\n",
    "        os.mkdir('%s/%s/pickles/' % (self.base_dir, self.xp_name))\n",
    "        \n",
    "        self.k = 50\n",
    "        self.n_input = item_infomation_matrix.shape[1] # dimensionality of text representations - 1000\n",
    "        self.n_hidden1 = 250 # 200\n",
    "        self.n_hidden2 = self.k\n",
    "        \n",
    "        \n",
    "        self.lambda_w = 1\n",
    "        self.lambda_n = 1\n",
    "        self.lambda_u = 1\n",
    "        self.lambda_v = 1\n",
    "        \n",
    "        self.drop_ratio = 0.1\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.num_u = rating_matrix.shape[0]\n",
    "        self.num_v = rating_matrix.shape[1]\n",
    "        intializer = tf.variance_scaling_initializer()\n",
    "        self.non_zero_idx = rating_matrix > 0\n",
    "        \n",
    "        self.Weights = {\n",
    "            #'w1' : tf.Variable(tf.random_normal( [self.n_input , self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            #'w2' : tf.Variable(tf.random_normal( [self.n_hidden1 , self.n_hidden2] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            #'w3' : tf.Variable(tf.random_normal( [self.n_hidden2 , self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            #'w4' : tf.Variable(tf.random_normal( [self.n_hidden1 , self.n_input] , mean=0.0, stddev=1 / self.lambda_w ))   \n",
    "            'w1' : tf.Variable(intializer([self.n_input, self.n_hidden1]), dtype=tf.float32),\n",
    "            'w2' : tf.Variable(intializer([self.n_hidden1, self.n_hidden2]), dtype=tf.float32),\n",
    "            'w3' : tf.Variable(intializer([self.n_hidden2, self.n_hidden1]), dtype=tf.float32),\n",
    "            'w4' : tf.Variable(intializer([self.n_hidden1, self.n_input]), dtype=tf.float32)   \n",
    "        }\n",
    "        self.Biases = {\n",
    "            'b1' : tf.Variable(tf.random_normal( [self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b2' : tf.Variable(tf.random_normal( [self.n_hidden2] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b3' : tf.Variable(tf.random_normal( [self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b4' : tf.Variable(tf.random_normal( [self.n_input] , mean=0.0, stddev=1 / self.lambda_w ))\n",
    "            #'b1' : tf.Variable(tf.zeros(self.n_hidden1)),\n",
    "            #'b2' : tf.Variable(tf.zeros(self.n_hidden2)),\n",
    "            #'b3' : tf.Variable(tf.zeros(self.n_hidden1)),\n",
    "            #'b4' : tf.Variable(tf.zeros(self.n_input))\n",
    "        }\n",
    "        \n",
    "        self.item_infomation_matrix = item_infomation_matrix\n",
    "        self.rating_matrix = rating_matrix\n",
    "    \n",
    "        self.build_model()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def encoder(self , x , drop_ratio):\n",
    "        w1 = self.Weights['w1']\n",
    "        b1 = self.Biases['b1']\n",
    "        L1 = tf.nn.relu(tf.matmul(x,w1) + b1)\n",
    "        L1 = tf.nn.dropout( L1 , keep_prob= 1 - drop_ratio )\n",
    "        \n",
    "        w2 = self.Weights['w2']\n",
    "        b2 = self.Biases['b2']\n",
    "        L2 = tf.nn.relu(tf.matmul(L1,w2) + b2)\n",
    "        L2 = tf.nn.dropout(L2 , keep_prob= 1 - drop_ratio)\n",
    "        \n",
    "        return L2\n",
    "    \n",
    "    def decoder(self , x , drop_ratio):\n",
    "        w3 = self.Weights['w3']\n",
    "        b3 = self.Biases['b3']\n",
    "        L3 = tf.nn.relu(tf.matmul(x,w3) + b3)\n",
    "        L3 = tf.nn.dropout(L3 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        w4 = self.Weights['w4']\n",
    "        b4 = self.Biases['b4']\n",
    "        L4 = tf.nn.relu(tf.matmul(L3,w4) + b4)\n",
    "        L4 = tf.nn.dropout(L4 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        return L4\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model_X_0 = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.model_X_c = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.model_V = tf.placeholder(tf.float32 , shape=(None , self.k))\n",
    "        \n",
    "        self.model_drop_ratio = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.V_sdae = self.encoder(self.model_X_0 , self.model_drop_ratio)\n",
    "        self.y_pred = self.decoder(self.V_sdae , self.model_drop_ratio)\n",
    "        \n",
    "        self.Regularization = tf.reduce_sum([tf.nn.l2_loss(w) + tf.nn.l2_loss(b) \n",
    "                                             for w,b in zip(self.Weights.values() , self.Biases.values())])\n",
    "        loss_r =1/2 * self.lambda_w * self.Regularization\n",
    "        self.loss_a =1/2 * self.lambda_n * tf.reduce_sum(tf.pow( self.model_X_c - self.y_pred , 2 ))\n",
    "        loss_v =1/2 * self.lambda_v * tf.reduce_sum(tf.pow( self.model_V - self.V_sdae , 2 ))\n",
    "        \n",
    "        self.Loss = loss_r + self.loss_a + loss_v\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.Loss)\n",
    "        \n",
    "    \n",
    "    def training(self):\n",
    "        #np.random.shuffle(self.item_infomation_matrix) #random index of train data\n",
    "        \n",
    "        self.item_infomation_matrix_noise = add_noise(self.item_infomation_matrix , 0.3)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        ## define dirs for tensorboard\n",
    "        train_writer = tf.summary.FileWriter('%s/%s/train'%(self.base_dir, self.xp_name), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('%s/%s/test'%(self.base_dir, self.xp_name))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        mf = MF(self.rating_matrix, self.k)\n",
    "        \n",
    "        for epoch in range(0, self.epochs):\n",
    "            print(\"%d / %d\"%(epoch + 1, self.epochs))\n",
    "            \n",
    "            V_sdae = sess.run(self.V_sdae , feed_dict={self.model_X_0 : self.item_infomation_matrix_noise , \n",
    "                                                       self.model_drop_ratio : 0.1})\n",
    "            # calc and print ALS loss every N epochs\n",
    "            print_loss = epoch % 1 == 0  \n",
    "            U , V, err = mf.ALS_v3_weighted(V_sdae, print_loss)\n",
    "            \n",
    "            #print(\"V shape initial\" + str(V.shape))\n",
    "            #V = np.resize(V, (self.num_v , self.k))\n",
    "            #print(\"V shape after\" + str(V.shape))\n",
    "            V = V.T\n",
    "            \n",
    "            auto_losses = []\n",
    "            model_losses = []\n",
    "            for i in range(0 , self.item_infomation_matrix.shape[0] , self.batch_size):\n",
    "                X_train_batch = self.item_infomation_matrix_noise[i:i+self.batch_size]\n",
    "                y_train_batch = self.item_infomation_matrix[i:i+self.batch_size]\n",
    "                \n",
    "                V_batch = V[i:i+self.batch_size]\n",
    "                \n",
    "                _ , my_loss, auto_loss = sess.run([self.optimizer, self.Loss, self.loss_a] , \n",
    "                                       feed_dict={self.model_X_0: X_train_batch , \n",
    "                                                  self.model_X_c: y_train_batch, \n",
    "                                                  self.model_V: V_batch, \n",
    "                                                  self.model_drop_ratio : 0.1})\n",
    "                auto_losses.append(auto_loss)\n",
    "                model_losses.append(my_loss)\n",
    "            \n",
    "            \n",
    "            #summary = sess.run([self.summaries],\n",
    "            #        feed_dict={self.autoencoder_loss: np.mean(auto_losses), \n",
    "            #                   self.model_loss:np.mean(model_losses),\n",
    "            #                   self.als_loss:err})\n",
    "            \n",
    "            summary = tf.Summary();\n",
    "            summary.value.add(tag='Autoencoder Loss', simple_value=np.mean(auto_losses))\n",
    "            summary.value.add(tag='Model Loss', simple_value=np.mean(model_losses))\n",
    "            summary.value.add(tag='ALS Loss', simple_value=err)\n",
    "\n",
    "            train_writer.add_summary(summary, epoch + 1)\n",
    "\n",
    "            \n",
    "            print(\"EPOCH %i MODEL LOSS %f\" % (epoch + 1, np.mean(model_losses)))\n",
    "            print(\"EPOCH %i AUTOENCODER LOSS %f\" % (epoch + 1, np.mean(auto_losses)))\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                os.mkdir('%s/%s/tf/epoch_%s/' % (self.base_dir, self.xp_name, epoch))\n",
    "                os.mkdir('%s/%s/pickles/epoch_%s/' % (self.base_dir, self.xp_name, epoch))\n",
    "                \n",
    "                # save tensorflow model\n",
    "                self.saver.save(sess, '%s/%s/tf/epoch_%s/model_.ckpt' % (self.base_dir, self.xp_name, epoch))\n",
    "                \n",
    "                # save U and V matricies from ALS\n",
    "                with open(r'%s/%s/pickles/epoch_%s/U.pickle'% (self.base_dir, self.xp_name, epoch), 'wb') as handle:\n",
    "                    pickle.dump(U, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                with open(r'%s/%s/pickles/epoch_%s/V.pickle'% (self.base_dir, self.xp_name, epoch), 'wb') as handle:\n",
    "                    pickle.dump(V, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        \n",
    "        sess.close()\n",
    "        return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10668, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_infomation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24303, 10668)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = np.array(rating_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "rm = np.array(\n",
    "    [[1, 2, 3, 7],\n",
    "     [1, 5, 2, 1],\n",
    "     [1, 7, 2, 1],\n",
    "     [1, 2, 3, 4]])\n",
    "#rm = np.mat(normalize(rm, axis=0, norm='l1'))\n",
    "#print(rm)\n",
    "\n",
    "iim = np.array([[0.7, 0.8, 0.9],[ 1, 0.2, 0.3],[0.5, 0.6, 0.7],[0.5, 0.6, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:/Models/master/test5_lr00001_k50_300n/pickles/epoch_45/U.pickle', 'rb') as handle:\n",
    "    U = pickle.load(handle)  \n",
    "    \n",
    "with open(r'D:/Models/master/test5_lr00001_k50_300n/pickles/epoch_45/V.pickle', 'rb') as handle2:\n",
    "    V = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "1 / 100\n",
      "ALS LOSS: 2.898589\n",
      "EPOCH 1 MODEL LOSS 35166.007812\n",
      "EPOCH 1 AUTOENCODER LOSS 29824.156250\n",
      "2 / 100\n",
      "ALS LOSS: 1.448117\n",
      "EPOCH 2 MODEL LOSS 4426.907227\n",
      "EPOCH 2 AUTOENCODER LOSS 61.334709\n",
      "3 / 100\n",
      "ALS LOSS: 0.817276\n",
      "EPOCH 3 MODEL LOSS 3961.845703\n",
      "EPOCH 3 AUTOENCODER LOSS 28.336441\n",
      "4 / 100\n",
      "ALS LOSS: 0.730024\n",
      "EPOCH 4 MODEL LOSS 3677.159424\n",
      "EPOCH 4 AUTOENCODER LOSS 22.104527\n",
      "5 / 100\n",
      "ALS LOSS: 0.714830\n",
      "EPOCH 5 MODEL LOSS 3470.143311\n",
      "EPOCH 5 AUTOENCODER LOSS 20.630169\n",
      "6 / 100\n",
      "ALS LOSS: 0.697931\n",
      "EPOCH 6 MODEL LOSS 3300.103271\n",
      "EPOCH 6 AUTOENCODER LOSS 19.794699\n",
      "7 / 100\n",
      "ALS LOSS: 0.684345\n",
      "EPOCH 7 MODEL LOSS 3154.348877\n",
      "EPOCH 7 AUTOENCODER LOSS 19.099277\n",
      "8 / 100\n",
      "ALS LOSS: 0.676849\n",
      "EPOCH 8 MODEL LOSS 3020.109375\n",
      "EPOCH 8 AUTOENCODER LOSS 18.700705\n",
      "9 / 100\n",
      "ALS LOSS: 0.663997\n",
      "EPOCH 9 MODEL LOSS 2896.240723\n",
      "EPOCH 9 AUTOENCODER LOSS 18.330490\n",
      "10 / 100\n",
      "ALS LOSS: 0.650858\n",
      "EPOCH 10 MODEL LOSS 2777.876953\n",
      "EPOCH 10 AUTOENCODER LOSS 18.120188\n",
      "11 / 100\n",
      "ALS LOSS: 0.637970\n",
      "EPOCH 11 MODEL LOSS 2664.620605\n",
      "EPOCH 11 AUTOENCODER LOSS 17.854092\n",
      "12 / 100\n",
      "ALS LOSS: 0.626245\n",
      "EPOCH 12 MODEL LOSS 2554.588379\n",
      "EPOCH 12 AUTOENCODER LOSS 17.367046\n",
      "13 / 100\n",
      "ALS LOSS: 0.608207\n",
      "EPOCH 13 MODEL LOSS 2447.132080\n",
      "EPOCH 13 AUTOENCODER LOSS 17.291327\n",
      "14 / 100\n",
      "ALS LOSS: 0.599215\n",
      "EPOCH 14 MODEL LOSS 2339.989502\n",
      "EPOCH 14 AUTOENCODER LOSS 17.067745\n",
      "15 / 100\n",
      "ALS LOSS: 0.583292\n",
      "EPOCH 15 MODEL LOSS 2235.966797\n",
      "EPOCH 15 AUTOENCODER LOSS 17.012470\n",
      "16 / 100\n",
      "ALS LOSS: 0.573915\n",
      "EPOCH 16 MODEL LOSS 2133.029541\n",
      "EPOCH 16 AUTOENCODER LOSS 16.765444\n",
      "17 / 100\n",
      "ALS LOSS: 0.559087\n",
      "EPOCH 17 MODEL LOSS 2032.836060\n",
      "EPOCH 17 AUTOENCODER LOSS 16.583981\n",
      "18 / 100\n",
      "ALS LOSS: 0.547258\n",
      "EPOCH 18 MODEL LOSS 1934.343384\n",
      "EPOCH 18 AUTOENCODER LOSS 16.408335\n",
      "19 / 100\n",
      "ALS LOSS: 0.536992\n",
      "EPOCH 19 MODEL LOSS 1837.372437\n",
      "EPOCH 19 AUTOENCODER LOSS 16.350721\n",
      "20 / 100\n",
      "ALS LOSS: 0.526268\n",
      "EPOCH 20 MODEL LOSS 1741.219116\n",
      "EPOCH 20 AUTOENCODER LOSS 16.089926\n",
      "21 / 100\n",
      "ALS LOSS: 0.516763\n",
      "EPOCH 21 MODEL LOSS 1647.008179\n",
      "EPOCH 21 AUTOENCODER LOSS 16.064140\n",
      "22 / 100\n",
      "ALS LOSS: 0.502757\n",
      "EPOCH 22 MODEL LOSS 1556.656372\n",
      "EPOCH 22 AUTOENCODER LOSS 15.881900\n",
      "23 / 100\n",
      "ALS LOSS: 0.496007\n",
      "EPOCH 23 MODEL LOSS 1468.516235\n",
      "EPOCH 23 AUTOENCODER LOSS 15.818082\n",
      "24 / 100\n",
      "ALS LOSS: 0.484134\n",
      "EPOCH 24 MODEL LOSS 1383.301636\n",
      "EPOCH 24 AUTOENCODER LOSS 15.721601\n",
      "25 / 100\n",
      "ALS LOSS: 0.472193\n",
      "EPOCH 25 MODEL LOSS 1302.221313\n",
      "EPOCH 25 AUTOENCODER LOSS 15.640849\n",
      "26 / 100\n",
      "ALS LOSS: 0.464393\n",
      "EPOCH 26 MODEL LOSS 1224.152954\n",
      "EPOCH 26 AUTOENCODER LOSS 15.617402\n",
      "27 / 100\n",
      "ALS LOSS: 0.452968\n",
      "EPOCH 27 MODEL LOSS 1150.228516\n",
      "EPOCH 27 AUTOENCODER LOSS 15.497354\n",
      "28 / 100\n",
      "ALS LOSS: 0.448605\n",
      "EPOCH 28 MODEL LOSS 1079.408569\n",
      "EPOCH 28 AUTOENCODER LOSS 15.468026\n",
      "29 / 100\n",
      "ALS LOSS: 0.439068\n",
      "EPOCH 29 MODEL LOSS 1012.287170\n",
      "EPOCH 29 AUTOENCODER LOSS 15.485813\n",
      "30 / 100\n",
      "ALS LOSS: 0.429132\n",
      "EPOCH 30 MODEL LOSS 949.702271\n",
      "EPOCH 30 AUTOENCODER LOSS 15.399888\n",
      "31 / 100\n",
      "ALS LOSS: 0.419248\n",
      "EPOCH 31 MODEL LOSS 891.184692\n",
      "EPOCH 31 AUTOENCODER LOSS 15.321185\n",
      "32 / 100\n",
      "ALS LOSS: 0.412927\n",
      "EPOCH 32 MODEL LOSS 835.704529\n",
      "EPOCH 32 AUTOENCODER LOSS 15.288391\n",
      "33 / 100\n",
      "ALS LOSS: 0.405670\n",
      "EPOCH 33 MODEL LOSS 784.112732\n",
      "EPOCH 33 AUTOENCODER LOSS 15.245025\n",
      "34 / 100\n",
      "ALS LOSS: 0.399497\n",
      "EPOCH 34 MODEL LOSS 735.236450\n",
      "EPOCH 34 AUTOENCODER LOSS 15.212984\n",
      "35 / 100\n",
      "ALS LOSS: 0.388939\n",
      "EPOCH 35 MODEL LOSS 690.031799\n",
      "EPOCH 35 AUTOENCODER LOSS 15.240865\n",
      "36 / 100\n",
      "ALS LOSS: 0.383330\n",
      "EPOCH 36 MODEL LOSS 648.021912\n",
      "EPOCH 36 AUTOENCODER LOSS 15.163701\n",
      "37 / 100\n",
      "ALS LOSS: 0.378544\n",
      "EPOCH 37 MODEL LOSS 608.330261\n",
      "EPOCH 37 AUTOENCODER LOSS 15.140877\n",
      "38 / 100\n",
      "ALS LOSS: 0.372104\n",
      "EPOCH 38 MODEL LOSS 571.323853\n",
      "EPOCH 38 AUTOENCODER LOSS 15.141821\n",
      "39 / 100\n",
      "ALS LOSS: 0.366932\n",
      "EPOCH 39 MODEL LOSS 536.433838\n",
      "EPOCH 39 AUTOENCODER LOSS 15.089338\n",
      "40 / 100\n",
      "ALS LOSS: 0.361068\n",
      "EPOCH 40 MODEL LOSS 504.016174\n",
      "EPOCH 40 AUTOENCODER LOSS 15.067097\n",
      "41 / 100\n",
      "ALS LOSS: 0.356811\n",
      "EPOCH 41 MODEL LOSS 473.646271\n",
      "EPOCH 41 AUTOENCODER LOSS 15.089312\n",
      "42 / 100\n",
      "ALS LOSS: 0.349763\n",
      "EPOCH 42 MODEL LOSS 445.267578\n",
      "EPOCH 42 AUTOENCODER LOSS 15.047726\n",
      "43 / 100\n",
      "ALS LOSS: 0.347192\n",
      "EPOCH 43 MODEL LOSS 418.482697\n",
      "EPOCH 43 AUTOENCODER LOSS 15.052565\n",
      "44 / 100\n",
      "ALS LOSS: 0.344212\n",
      "EPOCH 44 MODEL LOSS 393.040710\n",
      "EPOCH 44 AUTOENCODER LOSS 15.036419\n",
      "45 / 100\n",
      "ALS LOSS: 0.335980\n",
      "EPOCH 45 MODEL LOSS 370.050537\n",
      "EPOCH 45 AUTOENCODER LOSS 15.014287\n",
      "46 / 100\n",
      "ALS LOSS: 0.336448\n",
      "EPOCH 46 MODEL LOSS 347.660950\n",
      "EPOCH 46 AUTOENCODER LOSS 15.003802\n",
      "47 / 100\n",
      "ALS LOSS: 0.333846\n",
      "EPOCH 47 MODEL LOSS 326.988403\n",
      "EPOCH 47 AUTOENCODER LOSS 15.035826\n",
      "48 / 100\n",
      "ALS LOSS: 0.331349\n",
      "EPOCH 48 MODEL LOSS 307.226227\n",
      "EPOCH 48 AUTOENCODER LOSS 15.019351\n",
      "49 / 100\n",
      "ALS LOSS: 0.327449\n",
      "EPOCH 49 MODEL LOSS 289.275085\n",
      "EPOCH 49 AUTOENCODER LOSS 14.994919\n",
      "50 / 100\n",
      "ALS LOSS: 0.327120\n",
      "EPOCH 50 MODEL LOSS 271.598816\n",
      "EPOCH 50 AUTOENCODER LOSS 14.905275\n",
      "51 / 100\n",
      "ALS LOSS: 0.323249\n",
      "EPOCH 51 MODEL LOSS 255.550903\n",
      "EPOCH 51 AUTOENCODER LOSS 14.873557\n",
      "52 / 100\n",
      "ALS LOSS: 0.322171\n",
      "EPOCH 52 MODEL LOSS 240.431610\n",
      "EPOCH 52 AUTOENCODER LOSS 14.910785\n",
      "53 / 100\n",
      "ALS LOSS: 0.317611\n",
      "EPOCH 53 MODEL LOSS 225.927628\n",
      "EPOCH 53 AUTOENCODER LOSS 14.871916\n",
      "54 / 100\n",
      "ALS LOSS: 0.315971\n",
      "EPOCH 54 MODEL LOSS 212.591110\n",
      "EPOCH 54 AUTOENCODER LOSS 14.863465\n",
      "55 / 100\n",
      "ALS LOSS: 0.314804\n",
      "EPOCH 55 MODEL LOSS 200.417709\n",
      "EPOCH 55 AUTOENCODER LOSS 14.864055\n",
      "56 / 100\n",
      "ALS LOSS: 0.313511\n",
      "EPOCH 56 MODEL LOSS 188.596283\n",
      "EPOCH 56 AUTOENCODER LOSS 14.829829\n",
      "57 / 100\n",
      "ALS LOSS: 0.313270\n",
      "EPOCH 57 MODEL LOSS 177.881897\n",
      "EPOCH 57 AUTOENCODER LOSS 14.852992\n",
      "58 / 100\n",
      "ALS LOSS: 0.312009\n",
      "EPOCH 58 MODEL LOSS 167.233047\n",
      "EPOCH 58 AUTOENCODER LOSS 14.841542\n",
      "59 / 100\n",
      "ALS LOSS: 0.306485\n",
      "EPOCH 59 MODEL LOSS 157.608749\n",
      "EPOCH 59 AUTOENCODER LOSS 14.822929\n",
      "60 / 100\n",
      "ALS LOSS: 0.305727\n",
      "EPOCH 60 MODEL LOSS 148.748917\n",
      "EPOCH 60 AUTOENCODER LOSS 14.855982\n",
      "61 / 100\n",
      "ALS LOSS: 0.304069\n",
      "EPOCH 61 MODEL LOSS 140.423660\n",
      "EPOCH 61 AUTOENCODER LOSS 14.819909\n",
      "62 / 100\n",
      "ALS LOSS: 0.304013\n",
      "EPOCH 62 MODEL LOSS 132.475937\n",
      "EPOCH 62 AUTOENCODER LOSS 14.794097\n",
      "63 / 100\n",
      "ALS LOSS: 0.302841\n",
      "EPOCH 63 MODEL LOSS 124.997963\n",
      "EPOCH 63 AUTOENCODER LOSS 14.804199\n",
      "64 / 100\n",
      "ALS LOSS: 0.300877\n",
      "EPOCH 64 MODEL LOSS 118.143387\n",
      "EPOCH 64 AUTOENCODER LOSS 14.805077\n",
      "65 / 100\n",
      "ALS LOSS: 0.299970\n",
      "EPOCH 65 MODEL LOSS 111.628601\n",
      "EPOCH 65 AUTOENCODER LOSS 14.762988\n",
      "66 / 100\n",
      "ALS LOSS: 0.298128\n",
      "EPOCH 66 MODEL LOSS 105.617538\n",
      "EPOCH 66 AUTOENCODER LOSS 14.766410\n",
      "67 / 100\n",
      "ALS LOSS: 0.296289\n",
      "EPOCH 67 MODEL LOSS 100.069565\n",
      "EPOCH 67 AUTOENCODER LOSS 14.729168\n",
      "68 / 100\n",
      "ALS LOSS: 0.295723\n",
      "EPOCH 68 MODEL LOSS 94.913193\n",
      "EPOCH 68 AUTOENCODER LOSS 14.710420\n",
      "69 / 100\n",
      "ALS LOSS: 0.294714\n",
      "EPOCH 69 MODEL LOSS 89.900604\n",
      "EPOCH 69 AUTOENCODER LOSS 14.702148\n",
      "70 / 100\n",
      "ALS LOSS: 0.292384\n",
      "EPOCH 70 MODEL LOSS 85.505074\n",
      "EPOCH 70 AUTOENCODER LOSS 14.709456\n",
      "71 / 100\n",
      "ALS LOSS: 0.290725\n",
      "EPOCH 71 MODEL LOSS 81.251450\n",
      "EPOCH 71 AUTOENCODER LOSS 14.707192\n",
      "72 / 100\n",
      "ALS LOSS: 0.291405\n",
      "EPOCH 72 MODEL LOSS 77.606735\n",
      "EPOCH 72 AUTOENCODER LOSS 14.709371\n",
      "73 / 100\n",
      "ALS LOSS: 0.290699\n",
      "EPOCH 73 MODEL LOSS 73.672935\n",
      "EPOCH 73 AUTOENCODER LOSS 14.681520\n",
      "74 / 100\n",
      "ALS LOSS: 0.287901\n",
      "EPOCH 74 MODEL LOSS 70.372383\n",
      "EPOCH 74 AUTOENCODER LOSS 14.674646\n",
      "75 / 100\n",
      "ALS LOSS: 0.287950\n",
      "EPOCH 75 MODEL LOSS 67.270142\n",
      "EPOCH 75 AUTOENCODER LOSS 14.656796\n",
      "76 / 100\n",
      "ALS LOSS: 0.288310\n",
      "EPOCH 76 MODEL LOSS 64.267944\n",
      "EPOCH 76 AUTOENCODER LOSS 14.644119\n",
      "77 / 100\n",
      "ALS LOSS: 0.286432\n",
      "EPOCH 77 MODEL LOSS 61.434151\n",
      "EPOCH 77 AUTOENCODER LOSS 14.642429\n",
      "78 / 100\n",
      "ALS LOSS: 0.285888\n",
      "EPOCH 78 MODEL LOSS 58.878151\n",
      "EPOCH 78 AUTOENCODER LOSS 14.505219\n",
      "79 / 100\n",
      "ALS LOSS: 0.285205\n",
      "EPOCH 79 MODEL LOSS 56.426723\n",
      "EPOCH 79 AUTOENCODER LOSS 14.505753\n",
      "80 / 100\n",
      "ALS LOSS: 0.283322\n",
      "EPOCH 80 MODEL LOSS 54.323425\n",
      "EPOCH 80 AUTOENCODER LOSS 14.493122\n",
      "81 / 100\n",
      "ALS LOSS: 0.283150\n",
      "EPOCH 81 MODEL LOSS 52.128052\n",
      "EPOCH 81 AUTOENCODER LOSS 14.488802\n",
      "82 / 100\n",
      "ALS LOSS: 0.281000\n",
      "EPOCH 82 MODEL LOSS 50.264580\n",
      "EPOCH 82 AUTOENCODER LOSS 14.487379\n",
      "83 / 100\n",
      "ALS LOSS: 0.280112\n",
      "EPOCH 83 MODEL LOSS 48.535870\n",
      "EPOCH 83 AUTOENCODER LOSS 14.459379\n",
      "84 / 100\n",
      "ALS LOSS: 0.278586\n",
      "EPOCH 84 MODEL LOSS 46.873497\n",
      "EPOCH 84 AUTOENCODER LOSS 14.454562\n",
      "85 / 100\n",
      "ALS LOSS: 0.279809\n",
      "EPOCH 85 MODEL LOSS 45.441025\n",
      "EPOCH 85 AUTOENCODER LOSS 14.441370\n",
      "86 / 100\n",
      "ALS LOSS: 0.278645\n",
      "EPOCH 86 MODEL LOSS 44.139977\n",
      "EPOCH 86 AUTOENCODER LOSS 14.435294\n",
      "87 / 100\n",
      "ALS LOSS: 0.277148\n",
      "EPOCH 87 MODEL LOSS 42.719540\n",
      "EPOCH 87 AUTOENCODER LOSS 14.417836\n",
      "88 / 100\n",
      "ALS LOSS: 0.276723\n",
      "EPOCH 88 MODEL LOSS 41.481564\n",
      "EPOCH 88 AUTOENCODER LOSS 14.402512\n",
      "89 / 100\n",
      "ALS LOSS: 0.273764\n",
      "EPOCH 89 MODEL LOSS 40.338226\n",
      "EPOCH 89 AUTOENCODER LOSS 14.358494\n",
      "90 / 100\n",
      "ALS LOSS: 0.273455\n",
      "EPOCH 90 MODEL LOSS 39.382591\n",
      "EPOCH 90 AUTOENCODER LOSS 14.353527\n",
      "91 / 100\n",
      "ALS LOSS: 0.273797\n",
      "EPOCH 91 MODEL LOSS 38.447212\n",
      "EPOCH 91 AUTOENCODER LOSS 14.352925\n",
      "92 / 100\n",
      "ALS LOSS: 0.273196\n",
      "EPOCH 92 MODEL LOSS 37.612259\n",
      "EPOCH 92 AUTOENCODER LOSS 14.353334\n",
      "93 / 100\n",
      "ALS LOSS: 0.271772\n",
      "EPOCH 93 MODEL LOSS 36.661015\n",
      "EPOCH 93 AUTOENCODER LOSS 14.338104\n",
      "94 / 100\n",
      "ALS LOSS: 0.271614\n",
      "EPOCH 94 MODEL LOSS 36.013062\n",
      "EPOCH 94 AUTOENCODER LOSS 14.333227\n",
      "95 / 100\n",
      "ALS LOSS: 0.270861\n",
      "EPOCH 95 MODEL LOSS 35.387615\n",
      "EPOCH 95 AUTOENCODER LOSS 14.310520\n",
      "96 / 100\n",
      "ALS LOSS: 0.270109\n",
      "EPOCH 96 MODEL LOSS 34.816963\n",
      "EPOCH 96 AUTOENCODER LOSS 14.310678\n",
      "97 / 100\n",
      "ALS LOSS: 0.269405\n",
      "EPOCH 97 MODEL LOSS 34.107468\n",
      "EPOCH 97 AUTOENCODER LOSS 14.307351\n",
      "98 / 100\n",
      "ALS LOSS: 0.269064\n",
      "EPOCH 98 MODEL LOSS 33.449226\n",
      "EPOCH 98 AUTOENCODER LOSS 14.280388\n",
      "99 / 100\n",
      "ALS LOSS: 0.267663\n",
      "EPOCH 99 MODEL LOSS 33.183475\n",
      "EPOCH 99 AUTOENCODER LOSS 14.284654\n",
      "100 / 100\n",
      "ALS LOSS: 0.268159\n",
      "EPOCH 100 MODEL LOSS 32.693428\n",
      "EPOCH 100 AUTOENCODER LOSS 14.279201\n"
     ]
    }
   ],
   "source": [
    "cdl = CDL(rm, item_infomation_matrix)\n",
    "U, V = cdl.training() #188910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Flushing output cache (0 entries)\n"
     ]
    }
   ],
   "source": [
    "%reset Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/neopux/UHH/datasets/cdl_U_mx_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(U, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(r'/home/neopux/UHH/datasets/cdl_V_mx_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(V, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/neopux/UHH/datasets/cdl_U_mx_train.pickle', 'rb') as handle:\n",
    "    U = pickle.load(handle)  \n",
    "    \n",
    "with open(r'/home/neopux/UHH/datasets/cdl_V_mx_train.pickle', 'rb') as handle2:\n",
    "    V = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24303, 25)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10668, 25)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.dot(U, V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24303, 10668)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24303, 10668)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.03274029, -1.32472413, -1.09501641, ...,  1.29053572,\n",
       "        1.21824796,  2.60169135])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[rm > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2687775334691775"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(rm[rm > 0], preds[rm > 0]) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.826338841575348"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(rm, preds) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Flushing output cache (34 entries)\n"
     ]
    }
   ],
   "source": [
    "%reset Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>asin</th>\n",
       "      <th>0700099867</th>\n",
       "      <th>6050036071</th>\n",
       "      <th>7100027950</th>\n",
       "      <th>7293000936</th>\n",
       "      <th>8176503290</th>\n",
       "      <th>907843905X</th>\n",
       "      <th>9625990674</th>\n",
       "      <th>9861019731</th>\n",
       "      <th>9882155456</th>\n",
       "      <th>B000003SQQ</th>\n",
       "      <th>...</th>\n",
       "      <th>B00J128FPA</th>\n",
       "      <th>B00J226358</th>\n",
       "      <th>B00J6DLPLK</th>\n",
       "      <th>B00J9P3KBS</th>\n",
       "      <th>B00JM3R6M6</th>\n",
       "      <th>B00JQ8YH6A</th>\n",
       "      <th>B00JQHU9RC</th>\n",
       "      <th>B00JXW6GE0</th>\n",
       "      <th>B00KAI3KW2</th>\n",
       "      <th>B00KHECZXO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A00263941WP7WCIL7AKWL</th>\n",
       "      <td>4.289962</td>\n",
       "      <td>4.479144</td>\n",
       "      <td>4.975905</td>\n",
       "      <td>4.668979</td>\n",
       "      <td>4.249168</td>\n",
       "      <td>4.665921</td>\n",
       "      <td>4.739466</td>\n",
       "      <td>4.839571</td>\n",
       "      <td>4.955299</td>\n",
       "      <td>3.684972</td>\n",
       "      <td>...</td>\n",
       "      <td>5.120315</td>\n",
       "      <td>4.357303</td>\n",
       "      <td>4.797302</td>\n",
       "      <td>4.150864</td>\n",
       "      <td>4.637881</td>\n",
       "      <td>4.048742</td>\n",
       "      <td>5.02645</td>\n",
       "      <td>4.182327</td>\n",
       "      <td>4.909996</td>\n",
       "      <td>3.821161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A005481137I9SCAWEF7ON</th>\n",
       "      <td>2.878864</td>\n",
       "      <td>4.109722</td>\n",
       "      <td>4.398296</td>\n",
       "      <td>2.986301</td>\n",
       "      <td>3.570328</td>\n",
       "      <td>4.110752</td>\n",
       "      <td>4.122372</td>\n",
       "      <td>3.985164</td>\n",
       "      <td>4.016250</td>\n",
       "      <td>3.582355</td>\n",
       "      <td>...</td>\n",
       "      <td>4.355327</td>\n",
       "      <td>2.920714</td>\n",
       "      <td>4.032251</td>\n",
       "      <td>2.717914</td>\n",
       "      <td>4.087327</td>\n",
       "      <td>2.863482</td>\n",
       "      <td>2.80838</td>\n",
       "      <td>3.295960</td>\n",
       "      <td>4.778112</td>\n",
       "      <td>2.329751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10668 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "asin                   0700099867  6050036071  7100027950  7293000936  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL    4.289962    4.479144    4.975905    4.668979   \n",
       "A005481137I9SCAWEF7ON    2.878864    4.109722    4.398296    2.986301   \n",
       "\n",
       "asin                   8176503290  907843905X  9625990674  9861019731  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL    4.249168    4.665921    4.739466    4.839571   \n",
       "A005481137I9SCAWEF7ON    3.570328    4.110752    4.122372    3.985164   \n",
       "\n",
       "asin                   9882155456  B000003SQQ     ...      B00J128FPA  \\\n",
       "reviewerID                                        ...                   \n",
       "A00263941WP7WCIL7AKWL    4.955299    3.684972     ...        5.120315   \n",
       "A005481137I9SCAWEF7ON    4.016250    3.582355     ...        4.355327   \n",
       "\n",
       "asin                   B00J226358  B00J6DLPLK  B00J9P3KBS  B00JM3R6M6  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL    4.357303    4.797302    4.150864    4.637881   \n",
       "A005481137I9SCAWEF7ON    2.920714    4.032251    2.717914    4.087327   \n",
       "\n",
       "asin                   B00JQ8YH6A  B00JQHU9RC  B00JXW6GE0  B00KAI3KW2  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL    4.048742     5.02645    4.182327    4.909996   \n",
       "A005481137I9SCAWEF7ON    2.863482     2.80838    3.295960    4.778112   \n",
       "\n",
       "asin                   B00KHECZXO  \n",
       "reviewerID                         \n",
       "A00263941WP7WCIL7AKWL    3.821161  \n",
       "A005481137I9SCAWEF7ON    2.329751  \n",
       "\n",
       "[2 rows x 10668 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df_unmelt = pd.DataFrame(preds, columns = col_cat.categories, index = row_cat.categories)\n",
    "preds_df_unmelt.index.name = 'reviewerID'\n",
    "preds_df_unmelt.columns.name = 'asin'\n",
    "preds_df_unmelt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_val = df_test.copy()\n",
    "#preds_df_unmelt.loc['A00263941WP7WCIL7AKWL', '0700099867']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTextProc</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193452</th>\n",
       "      <td>B005QA98JS</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>This is another set of games that sort of surp...</td>\n",
       "      <td>this set game sort surprise good set this set ...</td>\n",
       "      <td>10 18, 2012</td>\n",
       "      <td>AFXTKAO0CB354</td>\n",
       "      <td>C. Weaver</td>\n",
       "      <td>Aonther Set Of Games That Surprised Me....</td>\n",
       "      <td>1350518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166816</th>\n",
       "      <td>B0043QL2FE</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>4</td>\n",
       "      <td>After playing this game a lot more, I have dec...</td>\n",
       "      <td>after play game lot i decide change review ini...</td>\n",
       "      <td>03 16, 2011</td>\n",
       "      <td>A4E0I88T1MS4O</td>\n",
       "      <td>Fani</td>\n",
       "      <td>Solid improvement from Top Spin 3 but still ha...</td>\n",
       "      <td>1300233600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin helpful  overall  \\\n",
       "193452  B005QA98JS  [1, 1]        4   \n",
       "166816  B0043QL2FE  [4, 7]        4   \n",
       "\n",
       "                                               reviewText  \\\n",
       "193452  This is another set of games that sort of surp...   \n",
       "166816  After playing this game a lot more, I have dec...   \n",
       "\n",
       "                                           reviewTextProc   reviewTime  \\\n",
       "193452  this set game sort surprise good set this set ...  10 18, 2012   \n",
       "166816  after play game lot i decide change review ini...  03 16, 2011   \n",
       "\n",
       "           reviewerID reviewerName  \\\n",
       "193452  AFXTKAO0CB354    C. Weaver   \n",
       "166816  A4E0I88T1MS4O         Fani   \n",
       "\n",
       "                                                  summary  unixReviewTime  \n",
       "193452         Aonther Set Of Games That Surprised Me....      1350518400  \n",
       "166816  Solid improvement from Top Spin 3 but still ha...      1300233600  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_val['value'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(x):\n",
    "    if x['reviewerID'] in preds_df_unmelt.index:\n",
    "        if x['asin'] in preds_df_unmelt.columns:\n",
    "            return preds_df_unmelt.loc[x['reviewerID'], x['asin']]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_val['value'] = df_test_val.apply(get_val, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTextProc</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193452</th>\n",
       "      <td>B005QA98JS</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>This is another set of games that sort of surp...</td>\n",
       "      <td>this set game sort surprise good set this set ...</td>\n",
       "      <td>10 18, 2012</td>\n",
       "      <td>AFXTKAO0CB354</td>\n",
       "      <td>C. Weaver</td>\n",
       "      <td>Aonther Set Of Games That Surprised Me....</td>\n",
       "      <td>1350518400</td>\n",
       "      <td>3.935068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166816</th>\n",
       "      <td>B0043QL2FE</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>4</td>\n",
       "      <td>After playing this game a lot more, I have dec...</td>\n",
       "      <td>after play game lot i decide change review ini...</td>\n",
       "      <td>03 16, 2011</td>\n",
       "      <td>A4E0I88T1MS4O</td>\n",
       "      <td>Fani</td>\n",
       "      <td>Solid improvement from Top Spin 3 but still ha...</td>\n",
       "      <td>1300233600</td>\n",
       "      <td>3.675520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin helpful  overall  \\\n",
       "193452  B005QA98JS  [1, 1]        4   \n",
       "166816  B0043QL2FE  [4, 7]        4   \n",
       "\n",
       "                                               reviewText  \\\n",
       "193452  This is another set of games that sort of surp...   \n",
       "166816  After playing this game a lot more, I have dec...   \n",
       "\n",
       "                                           reviewTextProc   reviewTime  \\\n",
       "193452  this set game sort surprise good set this set ...  10 18, 2012   \n",
       "166816  after play game lot i decide change review ini...  03 16, 2011   \n",
       "\n",
       "           reviewerID reviewerName  \\\n",
       "193452  AFXTKAO0CB354    C. Weaver   \n",
       "166816  A4E0I88T1MS4O         Fani   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "193452         Aonther Set Of Games That Surprised Me....      1350518400   \n",
       "166816  Solid improvement from Top Spin 3 but still ha...      1300233600   \n",
       "\n",
       "           value  \n",
       "193452  3.935068  \n",
       "166816  3.675520  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69534, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69534, 11)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTextProc</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193452</th>\n",
       "      <td>B005QA98JS</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>This is another set of games that sort of surp...</td>\n",
       "      <td>this set game sort surprise good set this set ...</td>\n",
       "      <td>10 18, 2012</td>\n",
       "      <td>AFXTKAO0CB354</td>\n",
       "      <td>C. Weaver</td>\n",
       "      <td>Aonther Set Of Games That Surprised Me....</td>\n",
       "      <td>1350518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166816</th>\n",
       "      <td>B0043QL2FE</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>4</td>\n",
       "      <td>After playing this game a lot more, I have dec...</td>\n",
       "      <td>after play game lot i decide change review ini...</td>\n",
       "      <td>03 16, 2011</td>\n",
       "      <td>A4E0I88T1MS4O</td>\n",
       "      <td>Fani</td>\n",
       "      <td>Solid improvement from Top Spin 3 but still ha...</td>\n",
       "      <td>1300233600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin helpful  overall  \\\n",
       "193452  B005QA98JS  [1, 1]        4   \n",
       "166816  B0043QL2FE  [4, 7]        4   \n",
       "\n",
       "                                               reviewText  \\\n",
       "193452  This is another set of games that sort of surp...   \n",
       "166816  After playing this game a lot more, I have dec...   \n",
       "\n",
       "                                           reviewTextProc   reviewTime  \\\n",
       "193452  this set game sort surprise good set this set ...  10 18, 2012   \n",
       "166816  after play game lot i decide change review ini...  03 16, 2011   \n",
       "\n",
       "           reviewerID reviewerName  \\\n",
       "193452  AFXTKAO0CB354    C. Weaver   \n",
       "166816  A4E0I88T1MS4O         Fani   \n",
       "\n",
       "                                                  summary  unixReviewTime  \n",
       "193452         Aonther Set Of Games That Surprised Me....      1350518400  \n",
       "166816  Solid improvement from Top Spin 3 but still ha...      1300233600  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[~df_test_val.value.isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1504624368435534"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(df_test[~df_test_val.value.isnull()].overall, df_test_val[~df_test_val.value.isnull()].value) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833793696260032"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(df_test[~df_test_val.value.isnull()].overall, df_test_val[~df_test_val.value.isnull()].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
