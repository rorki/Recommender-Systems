{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.sparse import csr_matrix\n",
    "#init random seed\n",
    "np.random.seed(5)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict, namedtuple\n",
    "from experiment_out_utils import precision_recall_at_k_4df, write_to_csv, XPData, XPRow, write_row\n",
    "import itertools as it\n",
    "from matrix_factorization_with_als import MF\n",
    "from model_out_utils import make_out_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out dir of experiment:  D:/Models/thesis/sdae/sdae_optimized/\n",
      "Out dir of U, V matricies:  D:/Models/thesis/sdae/sdae_optimized/pickles/\n",
      "Out dir of model parameters:  D:/Models/thesis/sdae/sdae_optimized/tf/\n"
     ]
    }
   ],
   "source": [
    "### create all necessary dirs for output ###\n",
    "\n",
    "XP_PATH, U_V_PATH, MODEL_PATH = make_out_dirs(model_name='sdae', xp_name='sdae_optimized') \n",
    "print(\"Out dir of experiment: \", XP_PATH)\n",
    "print(\"Out dir of U, V matricies: \", U_V_PATH)\n",
    "print(\"Out dir of model parameters: \", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json('/home/neopux/UHH/datasets/Video_Games_5_proc.json')\n",
    "df = pd.read_json(r'D:\\Datasets\\amazon_reviews\\processed\\reviews_Video_Games_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTextProc</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>summaryProc</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[8, 12]</td>\n",
       "      <td>1</td>\n",
       "      <td>Installing the game was a struggle (because of...</td>\n",
       "      <td>instal game struggle game window live bugs).so...</td>\n",
       "      <td>07 9, 2012</td>\n",
       "      <td>A2HD75EMZR8QLN</td>\n",
       "      <td>123</td>\n",
       "      <td>Pay to unlock content? I don't think so.</td>\n",
       "      <td>pay unlock content i not think</td>\n",
       "      <td>1341792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0700099867</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like rally cars get this game you will ...</td>\n",
       "      <td>if like rally car game fun it orient 34;europe...</td>\n",
       "      <td>06 30, 2013</td>\n",
       "      <td>A3UR8NLLY1ZHCX</td>\n",
       "      <td>Alejandro Henao \"Electronic Junky\"</td>\n",
       "      <td>Good rally game</td>\n",
       "      <td>good rally game</td>\n",
       "      <td>1372550400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  helpful  overall  \\\n",
       "0  0700099867  [8, 12]        1   \n",
       "1  0700099867   [0, 0]        4   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  Installing the game was a struggle (because of...   \n",
       "1  If you like rally cars get this game you will ...   \n",
       "\n",
       "                                      reviewTextProc   reviewTime  \\\n",
       "0  instal game struggle game window live bugs).so...   07 9, 2012   \n",
       "1  if like rally car game fun it orient 34;europe...  06 30, 2013   \n",
       "\n",
       "       reviewerID                        reviewerName  \\\n",
       "0  A2HD75EMZR8QLN                                 123   \n",
       "1  A3UR8NLLY1ZHCX  Alejandro Henao \"Electronic Junky\"   \n",
       "\n",
       "                                    summary                     summaryProc  \\\n",
       "0  Pay to unlock content? I don't think so.  pay unlock content i not think   \n",
       "1                           Good rally game                 good rally game   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1341792000  \n",
       "1      1372550400  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.3, stratify=df['reviewerID'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = df_train.groupby('asin').reviewTextProc.agg(' '.join)\n",
    "test_reviews = df_test.groupby('asin').reviewTextProc.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summaries = df_train.groupby('asin').summaryProc.agg(' '.join)\n",
    "test_summaries = df_test.groupby('asin').summaryProc.agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_rev = train_summaries + train_reviews\n",
    "total_test_rev = test_summaries + test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "item_infomation_matrix = vectorizer.fit_transform(total_train_rev.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10668, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(item_infomation_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ham', 4059),\n",
       " ('halt', 4058),\n",
       " ('halo2', 4057),\n",
       " ('halo', 4056),\n",
       " ('hallway', 4055),\n",
       " ('halloween', 4054),\n",
       " ('hallmark', 4053),\n",
       " ('hall', 4052),\n",
       " ('halfway', 4051),\n",
       " ('halftime', 4050),\n",
       " ('halflife', 4049),\n",
       " ('half', 4048),\n",
       " ('halen', 4047),\n",
       " ('hale', 4046),\n",
       " ('hairstyle', 4045),\n",
       " ('haired', 4044),\n",
       " ('hair', 4043),\n",
       " ('hail', 4042),\n",
       " ('hahaha', 4041),\n",
       " ('haha', 4040)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.vocabulary_.items(), reverse=True)[5940:5960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artowrk',\n",
       " 'ganer',\n",
       " 'dispite',\n",
       " 'creat',\n",
       " 'fighterbasicaly',\n",
       " 'lk',\n",
       " 'dsbasic',\n",
       " 'bora',\n",
       " 'gintaroo',\n",
       " 'classicfrogger']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.stop_words_)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_infomation_matrix = np.array(item_infomation_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0700099867', '6050036071', '7100027950', '7293000936', '8176503290',\n",
       "       '907843905X', '9625990674', '9861019731', '9882155456', 'B000003SQQ',\n",
       "       ...\n",
       "       'B00J128FPA', 'B00J226358', 'B00J6DLPLK', 'B00J9P3KBS', 'B00JM3R6M6',\n",
       "       'B00JQ8YH6A', 'B00JQHU9RC', 'B00JXW6GE0', 'B00KAI3KW2', 'B00KHECZXO'],\n",
       "      dtype='object', name='asin', length=10668)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_rev.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df_train.pivot(index = 'reviewerID', columns = 'asin', values = 'overall')\n",
    "pivoted = pivoted.fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>asin</th>\n",
       "      <th>0700099867</th>\n",
       "      <th>6050036071</th>\n",
       "      <th>7100027950</th>\n",
       "      <th>7293000936</th>\n",
       "      <th>8176503290</th>\n",
       "      <th>907843905X</th>\n",
       "      <th>9625990674</th>\n",
       "      <th>9861019731</th>\n",
       "      <th>9882155456</th>\n",
       "      <th>B000003SQQ</th>\n",
       "      <th>...</th>\n",
       "      <th>B00J128FPA</th>\n",
       "      <th>B00J226358</th>\n",
       "      <th>B00J6DLPLK</th>\n",
       "      <th>B00J9P3KBS</th>\n",
       "      <th>B00JM3R6M6</th>\n",
       "      <th>B00JQ8YH6A</th>\n",
       "      <th>B00JQHU9RC</th>\n",
       "      <th>B00JXW6GE0</th>\n",
       "      <th>B00KAI3KW2</th>\n",
       "      <th>B00KHECZXO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A00263941WP7WCIL7AKWL</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A005481137I9SCAWEF7ON</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A01588502N52TNG1BP7WG</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A01803182IUSFNIFF5984</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A023090719X7MTBCLM19B</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10668 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "asin                   0700099867  6050036071  7100027950  7293000936  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL         0.0         0.0         0.0         0.0   \n",
       "A005481137I9SCAWEF7ON         0.0         0.0         0.0         0.0   \n",
       "A01588502N52TNG1BP7WG         0.0         0.0         0.0         0.0   \n",
       "A01803182IUSFNIFF5984         0.0         0.0         0.0         0.0   \n",
       "A023090719X7MTBCLM19B         0.0         0.0         0.0         0.0   \n",
       "\n",
       "asin                   8176503290  907843905X  9625990674  9861019731  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL         0.0         0.0         0.0         0.0   \n",
       "A005481137I9SCAWEF7ON         0.0         0.0         0.0         0.0   \n",
       "A01588502N52TNG1BP7WG         0.0         0.0         0.0         0.0   \n",
       "A01803182IUSFNIFF5984         0.0         0.0         0.0         0.0   \n",
       "A023090719X7MTBCLM19B         0.0         0.0         0.0         0.0   \n",
       "\n",
       "asin                   9882155456  B000003SQQ     ...      B00J128FPA  \\\n",
       "reviewerID                                        ...                   \n",
       "A00263941WP7WCIL7AKWL         0.0         0.0     ...             0.0   \n",
       "A005481137I9SCAWEF7ON         0.0         0.0     ...             0.0   \n",
       "A01588502N52TNG1BP7WG         0.0         0.0     ...             0.0   \n",
       "A01803182IUSFNIFF5984         0.0         0.0     ...             0.0   \n",
       "A023090719X7MTBCLM19B         0.0         0.0     ...             0.0   \n",
       "\n",
       "asin                   B00J226358  B00J6DLPLK  B00J9P3KBS  B00JM3R6M6  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL         0.0         0.0         0.0         0.0   \n",
       "A005481137I9SCAWEF7ON         0.0         0.0         0.0         0.0   \n",
       "A01588502N52TNG1BP7WG         0.0         0.0         0.0         0.0   \n",
       "A01803182IUSFNIFF5984         0.0         0.0         0.0         0.0   \n",
       "A023090719X7MTBCLM19B         0.0         0.0         0.0         0.0   \n",
       "\n",
       "asin                   B00JQ8YH6A  B00JQHU9RC  B00JXW6GE0  B00KAI3KW2  \\\n",
       "reviewerID                                                              \n",
       "A00263941WP7WCIL7AKWL         0.0         0.0         0.0         0.0   \n",
       "A005481137I9SCAWEF7ON         0.0         0.0         0.0         0.0   \n",
       "A01588502N52TNG1BP7WG         0.0         0.0         0.0         0.0   \n",
       "A01803182IUSFNIFF5984         0.0         0.0         0.0         0.0   \n",
       "A023090719X7MTBCLM19B         0.0         0.0         0.0         0.0   \n",
       "\n",
       "asin                   B00KHECZXO  \n",
       "reviewerID                         \n",
       "A00263941WP7WCIL7AKWL         0.0  \n",
       "A005481137I9SCAWEF7ON         0.0  \n",
       "A01588502N52TNG1BP7WG         0.0  \n",
       "A01803182IUSFNIFF5984         0.0  \n",
       "A023090719X7MTBCLM19B         0.0  \n",
       "\n",
       "[5 rows x 10668 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = pivoted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10668"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pivoted.columns == total_train_rev.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking Noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(corruption_level, shape):\n",
    "    return np.random.binomial(1, 1 - corruption_level, shape)\n",
    "\n",
    "def add_noise(x , corruption_level ):\n",
    "    mask_ = mask(corruption_level , x.shape)\n",
    "    print(\"Mask shape: \" + str(mask_.shape))\n",
    "    x = np.multiply(x, mask_)\n",
    "    print(\"Noising completed..:\" + str(x.shape))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDAE & CDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDL():\n",
    "    def __init__(self , rating_matrix , item_infomation_matrix, out_path = None, k=10, \n",
    "                 epochs=50, batch_size=32, lr=0.0001, hidden_size=25, matrix_noise = 0.3,\n",
    "                drop_ratio=0.1, lambda_w = 1, lambda_n = 1, lambda_v = 1, lambda_u = 1):\n",
    "        self.out_path = out_path\n",
    "        \n",
    "        self.k = k\n",
    "        self.n_input = item_infomation_matrix.shape[1] # dimensionality of text representations - 1000\n",
    "        self.n_hidden1 = hidden_size\n",
    "        self.n_hidden2 = self.k\n",
    "        \n",
    "        # lambdas for loss calc\n",
    "        self.lambda_w = lambda_w\n",
    "        self.lambda_n = lambda_n\n",
    "        self.lambda_v = lambda_v\n",
    "        self.lambda_u = lambda_u\n",
    "        \n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.num_u = rating_matrix.shape[0]\n",
    "        self.num_v = rating_matrix.shape[1]\n",
    "        intializer = tf.variance_scaling_initializer()\n",
    "        self.non_zero_idx = rating_matrix > 0\n",
    "        \n",
    "        self.Weights = { \n",
    "            'w1' : tf.Variable(intializer([self.n_input, self.n_hidden1]), dtype=tf.float32),\n",
    "            'w2' : tf.Variable(intializer([self.n_hidden1, self.n_hidden2]), dtype=tf.float32),\n",
    "            'w3' : tf.Variable(intializer([self.n_hidden2, self.n_hidden1]), dtype=tf.float32),\n",
    "            'w4' : tf.Variable(intializer([self.n_hidden1, self.n_input]), dtype=tf.float32)   \n",
    "        }\n",
    "        self.Biases = {\n",
    "            'b1' : tf.Variable(tf.random_normal( [self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b2' : tf.Variable(tf.random_normal( [self.n_hidden2] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b3' : tf.Variable(tf.random_normal( [self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b4' : tf.Variable(tf.random_normal( [self.n_input] , mean=0.0, stddev=1 / self.lambda_w ))\n",
    "            #'b1' : tf.Variable(tf.zeros(self.n_hidden1)),\n",
    "            #'b2' : tf.Variable(tf.zeros(self.n_hidden2)),\n",
    "            #'b3' : tf.Variable(tf.zeros(self.n_hidden1)),\n",
    "            #'b4' : tf.Variable(tf.zeros(self.n_input))\n",
    "        }\n",
    "        \n",
    "        self.item_infomation_matrix = item_infomation_matrix\n",
    "        self.item_infomation_matrix_noise = add_noise(self.item_infomation_matrix , matrix_noise)\n",
    "        self.rating_matrix = rating_matrix\n",
    "    \n",
    "        self.build_model()\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def encoder(self , x , drop_ratio):\n",
    "        w1 = self.Weights['w1']\n",
    "        b1 = self.Biases['b1']\n",
    "        L1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "        L1 = tf.nn.dropout( L1 , keep_prob= 1 - drop_ratio)\n",
    "        \n",
    "        w2 = self.Weights['w2']\n",
    "        b2 = self.Biases['b2']\n",
    "        L2 = tf.nn.relu(tf.matmul(L1, w2) + b2)\n",
    "        L2 = tf.nn.dropout(L2 , keep_prob= 1 - drop_ratio)\n",
    "        return L2\n",
    "    \n",
    "    def decoder(self , x , drop_ratio):\n",
    "        w3 = self.Weights['w3']\n",
    "        b3 = self.Biases['b3']\n",
    "        L3 = tf.nn.relu(tf.matmul(x, w3) + b3)\n",
    "        L3 = tf.nn.dropout(L3 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        w4 = self.Weights['w4']\n",
    "        b4 = self.Biases['b4']\n",
    "        L4 = tf.nn.relu(tf.matmul(L3, w4) + b4)\n",
    "        L4 = tf.nn.dropout(L4 , keep_prob= 1 - drop_ratio)\n",
    "        return L4\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model_X_0 = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.model_X_c = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        \n",
    "        self.model_V = tf.placeholder(tf.float32 , shape=(None , self.k))\n",
    "        self.model_drop_ratio = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.V_sdae = self.encoder(self.model_X_0 , self.model_drop_ratio)\n",
    "        self.y_pred = self.decoder(self.V_sdae , self.model_drop_ratio)\n",
    "        \n",
    "        self.Regularization = tf.reduce_sum([tf.nn.l2_loss(w) + tf.nn.l2_loss(b) \n",
    "                                             for w,b in zip(self.Weights.values() , self.Biases.values())])\n",
    "        loss_r = 1/2 * self.lambda_w * self.Regularization\n",
    "        self.loss_a = 1/2 * self.lambda_n * tf.reduce_sum(tf.pow(self.model_X_c - self.y_pred , 2))\n",
    "        loss_v =1/2 * self.lambda_v * tf.reduce_sum(tf.pow(self.model_V - self.V_sdae , 2))\n",
    "        \n",
    "        self.Loss = loss_r + self.loss_a + loss_v\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.Loss)\n",
    "        \n",
    "    \n",
    "    def training(self):\n",
    "        sess = tf.Session()\n",
    "        \n",
    "        ## define dirs for tensorboard if needed\n",
    "        if self.out_path != None:\n",
    "            train_writer = tf.summary.FileWriter('%s/tf/train' % self.out_path, sess.graph)\n",
    "            test_writer = tf.summary.FileWriter('%s/tf/test' % self.out_path, sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        mf = MF(self.rating_matrix, self.k, self.lambda_u, self.lambda_v)\n",
    "        \n",
    "        for epoch in range(0, self.epochs):\n",
    "            print(\"EPOCH %s / %s\" % (epoch + 1, self.epochs))\n",
    "            \n",
    "            V_sdae = sess.run(self.V_sdae , feed_dict={self.model_X_0 : self.item_infomation_matrix_noise , \n",
    "                                                       self.model_drop_ratio : self.drop_ratio})\n",
    "            # calc and print ALS loss every N epochs\n",
    "            U , V, beta_u, beta_v, err = mf.ALS_v3_weighted(V_sdae)\n",
    "            V = V.T\n",
    "            \n",
    "            auto_losses = []\n",
    "            model_losses = []\n",
    "            for i in range(0 , self.item_infomation_matrix.shape[0] , self.batch_size):\n",
    "                X_train_batch = self.item_infomation_matrix_noise[i : i+self.batch_size]\n",
    "                y_train_batch = self.item_infomation_matrix[i : i+self.batch_size]\n",
    "                \n",
    "                V_batch = V[i : i + self.batch_size]\n",
    "                _ , my_loss, auto_loss = sess.run([self.optimizer, self.Loss, self.loss_a] , \n",
    "                                       feed_dict={self.model_X_0: X_train_batch , \n",
    "                                                  self.model_X_c: y_train_batch, \n",
    "                                                  self.model_V: V_batch, \n",
    "                                                  self.model_drop_ratio : self.drop_ratio})\n",
    "                auto_losses.append(auto_loss)\n",
    "                model_losses.append(my_loss)\n",
    "            \n",
    "            print(\"ALS LOSS %s\" % err) \n",
    "            print(\"MODEL LOSS %s\" % np.mean(model_losses))\n",
    "            print(\"AUTOENCODER LOSS %s\" % np.mean(auto_losses))          \n",
    "            \n",
    "            # save log files\n",
    "            if self.out_path != None:\n",
    "                # dump summaries\n",
    "                summary = tf.Summary();\n",
    "                summary.value.add(tag='Autoencoder Loss', simple_value=np.mean(auto_losses))\n",
    "                summary.value.add(tag='Model Loss', simple_value=np.mean(model_losses))\n",
    "                summary.value.add(tag='ALS Loss', simple_value=err)\n",
    "                train_writer.add_summary(summary, epoch + 1)\n",
    "                \n",
    "                # dump model and pickles\n",
    "                if epoch % 5 == 0:\n",
    "                    os.mkdir('%s/tf/epoch_%s/' % (self.out_path, epoch))\n",
    "                    os.mkdir('%s/pickles/epoch_%s/' % (self.out_path, epoch))\n",
    "\n",
    "                    # save tensorflow model\n",
    "                    self.saver.save(sess, '%s/tf/epoch_%s/model_.ckpt' % (self.out_path, epoch))\n",
    "\n",
    "                    # save U and V matricies from ALS\n",
    "                    with open('%s/pickles/epoch_%s/U.pickle' % (self.out_path, epoch), 'wb') as handle:\n",
    "                        pickle.dump(U, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    with open('%s/pickles/epoch_%s/V.pickle' % (self.out_path, epoch), 'wb') as handle:\n",
    "                        pickle.dump(V, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        \n",
    "        sess.close()\n",
    "        return U, V, beta_u, beta_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10668, 10000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_infomation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24303, 10668)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing output cache (7 entries)\n"
     ]
    }
   ],
   "source": [
    "%reset Out\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24303, 10668)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 200\n",
      "ALS LOSS 3.1659064065156843\n",
      "MODEL LOSS 39533.33\n",
      "AUTOENCODER LOSS 1360.0447\n",
      "EPOCH 2 / 200\n",
      "ALS LOSS 0.8736638385130221\n",
      "MODEL LOSS 20005.062\n",
      "AUTOENCODER LOSS 227.5444\n",
      "EPOCH 3 / 200\n",
      "ALS LOSS 0.5354408584089498\n",
      "MODEL LOSS 10214.416\n",
      "AUTOENCODER LOSS 168.69966\n",
      "EPOCH 4 / 200\n",
      "ALS LOSS 0.4900248494778793\n",
      "MODEL LOSS 5108.576\n",
      "AUTOENCODER LOSS 152.96867\n",
      "EPOCH 5 / 200\n",
      "ALS LOSS 0.47711767256384413\n",
      "MODEL LOSS 2513.5674\n",
      "AUTOENCODER LOSS 145.79938\n",
      "EPOCH 6 / 200\n",
      "ALS LOSS 0.47193161435337044\n",
      "MODEL LOSS 1249.1621\n",
      "AUTOENCODER LOSS 142.03293\n",
      "EPOCH 7 / 200\n",
      "ALS LOSS 0.46824485147297806\n",
      "MODEL LOSS 665.503\n",
      "AUTOENCODER LOSS 140.12273\n",
      "EPOCH 8 / 200\n",
      "ALS LOSS 0.4656987390313292\n",
      "MODEL LOSS 411.85226\n",
      "AUTOENCODER LOSS 138.95624\n",
      "EPOCH 9 / 200\n",
      "ALS LOSS 0.46436917536735606\n",
      "MODEL LOSS 306.28183\n",
      "AUTOENCODER LOSS 138.41791\n",
      "EPOCH 10 / 200\n",
      "ALS LOSS 0.4633576799474289\n",
      "MODEL LOSS 262.21158\n",
      "AUTOENCODER LOSS 138.13274\n",
      "EPOCH 11 / 200\n",
      "ALS LOSS 0.4621037964839852\n",
      "MODEL LOSS 240.9773\n",
      "AUTOENCODER LOSS 137.95999\n",
      "EPOCH 12 / 200\n",
      "ALS LOSS 0.461245629130745\n",
      "MODEL LOSS 228.6745\n",
      "AUTOENCODER LOSS 137.8498\n",
      "EPOCH 13 / 200\n",
      "ALS LOSS 0.4605487784820453\n",
      "MODEL LOSS 220.89291\n",
      "AUTOENCODER LOSS 137.74985\n",
      "EPOCH 14 / 200\n",
      "ALS LOSS 0.45986707732534277\n",
      "MODEL LOSS 215.19534\n",
      "AUTOENCODER LOSS 137.68771\n",
      "EPOCH 15 / 200\n",
      "ALS LOSS 0.4591548768206913\n",
      "MODEL LOSS 211.17482\n",
      "AUTOENCODER LOSS 137.57704\n",
      "EPOCH 16 / 200\n",
      "ALS LOSS 0.4587444309367415\n",
      "MODEL LOSS 208.04068\n",
      "AUTOENCODER LOSS 137.58052\n",
      "EPOCH 17 / 200\n",
      "ALS LOSS 0.45815018870150676\n",
      "MODEL LOSS 205.6755\n",
      "AUTOENCODER LOSS 137.60002\n",
      "EPOCH 18 / 200\n",
      "ALS LOSS 0.4578813307869984\n",
      "MODEL LOSS 203.67749\n",
      "AUTOENCODER LOSS 137.55559\n",
      "EPOCH 19 / 200\n",
      "ALS LOSS 0.4572391216730067\n",
      "MODEL LOSS 202.29388\n",
      "AUTOENCODER LOSS 137.6412\n",
      "EPOCH 20 / 200\n",
      "ALS LOSS 0.45682419691749887\n",
      "MODEL LOSS 201.03076\n",
      "AUTOENCODER LOSS 137.60867\n",
      "EPOCH 21 / 200\n",
      "ALS LOSS 0.45652012264253267\n",
      "MODEL LOSS 199.94124\n",
      "AUTOENCODER LOSS 137.5787\n",
      "EPOCH 22 / 200\n",
      "ALS LOSS 0.4563208007714299\n",
      "MODEL LOSS 199.24106\n",
      "AUTOENCODER LOSS 137.59877\n",
      "EPOCH 23 / 200\n",
      "ALS LOSS 0.4560163232652186\n",
      "MODEL LOSS 198.12823\n",
      "AUTOENCODER LOSS 137.51285\n",
      "EPOCH 24 / 200\n",
      "ALS LOSS 0.45574975408696544\n",
      "MODEL LOSS 197.40804\n",
      "AUTOENCODER LOSS 137.47876\n",
      "EPOCH 25 / 200\n",
      "ALS LOSS 0.4554276783552329\n",
      "MODEL LOSS 196.63354\n",
      "AUTOENCODER LOSS 137.35019\n",
      "EPOCH 26 / 200\n",
      "ALS LOSS 0.45520561501022083\n",
      "MODEL LOSS 196.17876\n",
      "AUTOENCODER LOSS 137.43022\n",
      "EPOCH 27 / 200\n",
      "ALS LOSS 0.454868297455199\n",
      "MODEL LOSS 195.71585\n",
      "AUTOENCODER LOSS 137.412\n",
      "EPOCH 28 / 200\n",
      "ALS LOSS 0.4549535935487866\n",
      "MODEL LOSS 195.21571\n",
      "AUTOENCODER LOSS 137.41107\n",
      "EPOCH 29 / 200\n",
      "ALS LOSS 0.45457432048523605\n",
      "MODEL LOSS 194.5779\n",
      "AUTOENCODER LOSS 137.34564\n",
      "EPOCH 30 / 200\n",
      "ALS LOSS 0.454407488182776\n",
      "MODEL LOSS 194.10019\n",
      "AUTOENCODER LOSS 137.40094\n",
      "EPOCH 31 / 200\n",
      "ALS LOSS 0.4542376611097773\n",
      "MODEL LOSS 193.80418\n",
      "AUTOENCODER LOSS 137.33331\n",
      "EPOCH 32 / 200\n",
      "ALS LOSS 0.4541419931919267\n",
      "MODEL LOSS 193.37073\n",
      "AUTOENCODER LOSS 137.29204\n",
      "EPOCH 33 / 200\n",
      "ALS LOSS 0.45405821622011466\n",
      "MODEL LOSS 192.86871\n",
      "AUTOENCODER LOSS 137.2663\n",
      "EPOCH 34 / 200\n",
      "ALS LOSS 0.4538395442421485\n",
      "MODEL LOSS 192.53017\n",
      "AUTOENCODER LOSS 137.28563\n",
      "EPOCH 35 / 200\n",
      "ALS LOSS 0.45348646125302694\n",
      "MODEL LOSS 192.20044\n",
      "AUTOENCODER LOSS 137.26794\n",
      "EPOCH 36 / 200\n",
      "ALS LOSS 0.4533582152147935\n",
      "MODEL LOSS 191.8636\n",
      "AUTOENCODER LOSS 137.19746\n",
      "EPOCH 37 / 200\n",
      "ALS LOSS 0.45333486699620607\n",
      "MODEL LOSS 191.64366\n",
      "AUTOENCODER LOSS 137.2021\n",
      "EPOCH 38 / 200\n",
      "ALS LOSS 0.45332673752003694\n",
      "MODEL LOSS 191.22931\n",
      "AUTOENCODER LOSS 137.17755\n",
      "EPOCH 39 / 200\n",
      "ALS LOSS 0.4530726190019719\n",
      "MODEL LOSS 191.05698\n",
      "AUTOENCODER LOSS 137.2027\n",
      "EPOCH 40 / 200\n",
      "ALS LOSS 0.45319417939961865\n",
      "MODEL LOSS 190.75484\n",
      "AUTOENCODER LOSS 137.22116\n",
      "EPOCH 41 / 200\n",
      "ALS LOSS 0.45301001982508027\n",
      "MODEL LOSS 190.59477\n",
      "AUTOENCODER LOSS 137.20322\n",
      "EPOCH 42 / 200\n",
      "ALS LOSS 0.4529133664176756\n",
      "MODEL LOSS 190.27565\n",
      "AUTOENCODER LOSS 137.1501\n",
      "EPOCH 43 / 200\n",
      "ALS LOSS 0.4527794737110889\n",
      "MODEL LOSS 190.15518\n",
      "AUTOENCODER LOSS 137.1753\n",
      "EPOCH 44 / 200\n",
      "ALS LOSS 0.45270898430539674\n",
      "MODEL LOSS 189.90887\n",
      "AUTOENCODER LOSS 137.10535\n",
      "EPOCH 45 / 200\n",
      "ALS LOSS 0.45258302832742936\n",
      "MODEL LOSS 189.83575\n",
      "AUTOENCODER LOSS 137.20299\n",
      "EPOCH 46 / 200\n",
      "ALS LOSS 0.4525559115987822\n",
      "MODEL LOSS 189.69786\n",
      "AUTOENCODER LOSS 137.24591\n",
      "EPOCH 47 / 200\n",
      "ALS LOSS 0.4524696641221618\n",
      "MODEL LOSS 189.44014\n",
      "AUTOENCODER LOSS 137.13843\n",
      "EPOCH 48 / 200\n",
      "ALS LOSS 0.45231415072633274\n",
      "MODEL LOSS 189.23586\n",
      "AUTOENCODER LOSS 137.15842\n",
      "EPOCH 49 / 200\n",
      "ALS LOSS 0.45233224402646194\n",
      "MODEL LOSS 189.20047\n",
      "AUTOENCODER LOSS 137.08685\n",
      "EPOCH 50 / 200\n",
      "ALS LOSS 0.45218929623575177\n",
      "MODEL LOSS 188.98854\n",
      "AUTOENCODER LOSS 137.10324\n",
      "EPOCH 51 / 200\n",
      "ALS LOSS 0.4521219905771241\n",
      "MODEL LOSS 188.9516\n",
      "AUTOENCODER LOSS 137.20406\n",
      "EPOCH 52 / 200\n",
      "ALS LOSS 0.4520898298542878\n",
      "MODEL LOSS 188.8938\n",
      "AUTOENCODER LOSS 137.1698\n",
      "EPOCH 53 / 200\n",
      "ALS LOSS 0.45204533665444857\n",
      "MODEL LOSS 188.68219\n",
      "AUTOENCODER LOSS 137.14317\n",
      "EPOCH 54 / 200\n",
      "ALS LOSS 0.45194900509975683\n",
      "MODEL LOSS 188.60547\n",
      "AUTOENCODER LOSS 137.21066\n",
      "EPOCH 55 / 200\n",
      "ALS LOSS 0.4519260049752477\n",
      "MODEL LOSS 188.62439\n",
      "AUTOENCODER LOSS 137.2098\n",
      "EPOCH 56 / 200\n",
      "ALS LOSS 0.45182127820713325\n",
      "MODEL LOSS 188.51337\n",
      "AUTOENCODER LOSS 137.19249\n",
      "EPOCH 57 / 200\n",
      "ALS LOSS 0.4518102689305421\n",
      "MODEL LOSS 188.40651\n",
      "AUTOENCODER LOSS 137.19548\n",
      "EPOCH 58 / 200\n",
      "ALS LOSS 0.4516999972856326\n",
      "MODEL LOSS 188.33252\n",
      "AUTOENCODER LOSS 137.1471\n",
      "EPOCH 59 / 200\n",
      "ALS LOSS 0.45180306716936885\n",
      "MODEL LOSS 188.26509\n",
      "AUTOENCODER LOSS 137.20966\n",
      "EPOCH 60 / 200\n",
      "ALS LOSS 0.45178882905501844\n",
      "MODEL LOSS 188.2449\n",
      "AUTOENCODER LOSS 137.16751\n",
      "EPOCH 61 / 200\n",
      "ALS LOSS 0.4516700987956516\n",
      "MODEL LOSS 188.11276\n",
      "AUTOENCODER LOSS 137.17705\n",
      "EPOCH 62 / 200\n",
      "ALS LOSS 0.4517644780808035\n",
      "MODEL LOSS 187.9539\n",
      "AUTOENCODER LOSS 137.07175\n",
      "EPOCH 63 / 200\n",
      "ALS LOSS 0.4515858066480791\n",
      "MODEL LOSS 187.88148\n",
      "AUTOENCODER LOSS 137.07314\n",
      "EPOCH 64 / 200\n",
      "ALS LOSS 0.4517044771874409\n",
      "MODEL LOSS 187.92384\n",
      "AUTOENCODER LOSS 137.18048\n",
      "EPOCH 65 / 200\n",
      "ALS LOSS 0.45156525415558724\n",
      "MODEL LOSS 187.78874\n",
      "AUTOENCODER LOSS 137.09761\n",
      "EPOCH 66 / 200\n",
      "ALS LOSS 0.451614954336553\n",
      "MODEL LOSS 187.7666\n",
      "AUTOENCODER LOSS 137.17657\n",
      "EPOCH 67 / 200\n",
      "ALS LOSS 0.45138984927911785\n",
      "MODEL LOSS 187.65\n",
      "AUTOENCODER LOSS 137.10054\n",
      "EPOCH 68 / 200\n",
      "ALS LOSS 0.4514143697621303\n",
      "MODEL LOSS 187.76495\n",
      "AUTOENCODER LOSS 137.22664\n",
      "EPOCH 69 / 200\n",
      "ALS LOSS 0.45143693580390265\n",
      "MODEL LOSS 187.59251\n",
      "AUTOENCODER LOSS 137.11668\n",
      "EPOCH 70 / 200\n",
      "ALS LOSS 0.45129045495877324\n",
      "MODEL LOSS 187.59569\n",
      "AUTOENCODER LOSS 137.13707\n",
      "EPOCH 71 / 200\n",
      "ALS LOSS 0.4514674221791146\n",
      "MODEL LOSS 187.55159\n",
      "AUTOENCODER LOSS 137.22273\n",
      "EPOCH 72 / 200\n",
      "ALS LOSS 0.4514109214405405\n",
      "MODEL LOSS 187.435\n",
      "AUTOENCODER LOSS 137.1446\n",
      "EPOCH 73 / 200\n",
      "ALS LOSS 0.4514324962133338\n",
      "MODEL LOSS 187.37376\n",
      "AUTOENCODER LOSS 137.12762\n",
      "EPOCH 74 / 200\n",
      "ALS LOSS 0.45133064166673803\n",
      "MODEL LOSS 187.4169\n",
      "AUTOENCODER LOSS 137.18997\n",
      "EPOCH 75 / 200\n",
      "ALS LOSS 0.4513564613806181\n",
      "MODEL LOSS 187.3584\n",
      "AUTOENCODER LOSS 137.06357\n",
      "EPOCH 76 / 200\n",
      "ALS LOSS 0.45133298147455436\n",
      "MODEL LOSS 187.38756\n",
      "AUTOENCODER LOSS 137.13924\n",
      "EPOCH 77 / 200\n",
      "ALS LOSS 0.45137958657961874\n",
      "MODEL LOSS 187.29591\n",
      "AUTOENCODER LOSS 137.23886\n",
      "EPOCH 78 / 200\n",
      "ALS LOSS 0.4512097040644584\n",
      "MODEL LOSS 187.15952\n",
      "AUTOENCODER LOSS 137.17105\n",
      "EPOCH 79 / 200\n",
      "ALS LOSS 0.45118705545156107\n",
      "MODEL LOSS 187.20628\n",
      "AUTOENCODER LOSS 137.16278\n",
      "EPOCH 80 / 200\n",
      "ALS LOSS 0.4511892196997236\n",
      "MODEL LOSS 187.13298\n",
      "AUTOENCODER LOSS 137.17867\n",
      "EPOCH 81 / 200\n",
      "ALS LOSS 0.4511662003566075\n",
      "MODEL LOSS 187.17487\n",
      "AUTOENCODER LOSS 137.18028\n",
      "EPOCH 82 / 200\n",
      "ALS LOSS 0.45113093528975445\n",
      "MODEL LOSS 187.07611\n",
      "AUTOENCODER LOSS 137.16211\n",
      "EPOCH 83 / 200\n",
      "ALS LOSS 0.45109079966528126\n",
      "MODEL LOSS 186.988\n",
      "AUTOENCODER LOSS 137.0806\n",
      "EPOCH 84 / 200\n",
      "ALS LOSS 0.45115205402084285\n",
      "MODEL LOSS 187.01309\n",
      "AUTOENCODER LOSS 137.12567\n",
      "EPOCH 85 / 200\n",
      "ALS LOSS 0.45120288577200085\n",
      "MODEL LOSS 187.01\n",
      "AUTOENCODER LOSS 137.17836\n",
      "EPOCH 86 / 200\n",
      "ALS LOSS 0.45112813558100406\n",
      "MODEL LOSS 186.97104\n",
      "AUTOENCODER LOSS 137.18542\n",
      "EPOCH 87 / 200\n",
      "ALS LOSS 0.4511856225089695\n",
      "MODEL LOSS 186.9985\n",
      "AUTOENCODER LOSS 137.21259\n",
      "EPOCH 88 / 200\n",
      "ALS LOSS 0.451087189669861\n",
      "MODEL LOSS 187.01466\n",
      "AUTOENCODER LOSS 137.186\n",
      "EPOCH 89 / 200\n",
      "ALS LOSS 0.4510259444469485\n",
      "MODEL LOSS 186.86243\n",
      "AUTOENCODER LOSS 137.07527\n",
      "EPOCH 90 / 200\n",
      "ALS LOSS 0.45095179623527853\n",
      "MODEL LOSS 186.86856\n",
      "AUTOENCODER LOSS 137.09639\n",
      "EPOCH 91 / 200\n",
      "ALS LOSS 0.4510339681865648\n",
      "MODEL LOSS 186.86299\n",
      "AUTOENCODER LOSS 137.12218\n",
      "EPOCH 92 / 200\n",
      "ALS LOSS 0.4510888667189232\n",
      "MODEL LOSS 186.82875\n",
      "AUTOENCODER LOSS 137.09554\n",
      "EPOCH 93 / 200\n",
      "ALS LOSS 0.45096498135162677\n",
      "MODEL LOSS 186.8328\n",
      "AUTOENCODER LOSS 137.08643\n",
      "EPOCH 94 / 200\n",
      "ALS LOSS 0.4509760772807442\n",
      "MODEL LOSS 186.77148\n",
      "AUTOENCODER LOSS 137.09227\n",
      "EPOCH 95 / 200\n",
      "ALS LOSS 0.45099763358461875\n",
      "MODEL LOSS 186.73102\n",
      "AUTOENCODER LOSS 137.08125\n",
      "EPOCH 96 / 200\n",
      "ALS LOSS 0.45105114644241484\n",
      "MODEL LOSS 186.75104\n",
      "AUTOENCODER LOSS 137.12071\n",
      "EPOCH 97 / 200\n",
      "ALS LOSS 0.4510862560683673\n",
      "MODEL LOSS 186.73854\n",
      "AUTOENCODER LOSS 137.13144\n",
      "EPOCH 98 / 200\n",
      "ALS LOSS 0.4509497200339091\n",
      "MODEL LOSS 186.78648\n",
      "AUTOENCODER LOSS 137.16432\n",
      "EPOCH 99 / 200\n",
      "ALS LOSS 0.45096197873598065\n",
      "MODEL LOSS 186.69695\n",
      "AUTOENCODER LOSS 137.10072\n",
      "EPOCH 100 / 200\n",
      "ALS LOSS 0.45088909341592015\n",
      "MODEL LOSS 186.79184\n",
      "AUTOENCODER LOSS 137.20668\n",
      "EPOCH 101 / 200\n",
      "ALS LOSS 0.4508939083430391\n",
      "MODEL LOSS 186.67206\n",
      "AUTOENCODER LOSS 137.09885\n",
      "EPOCH 102 / 200\n",
      "ALS LOSS 0.4509304144342545\n",
      "MODEL LOSS 186.71442\n",
      "AUTOENCODER LOSS 137.13039\n",
      "EPOCH 103 / 200\n",
      "ALS LOSS 0.45099607485769505\n",
      "MODEL LOSS 186.7203\n",
      "AUTOENCODER LOSS 137.12906\n",
      "EPOCH 104 / 200\n",
      "ALS LOSS 0.450912098354262\n",
      "MODEL LOSS 186.68056\n",
      "AUTOENCODER LOSS 137.11005\n",
      "EPOCH 105 / 200\n",
      "ALS LOSS 0.4508918617548193\n",
      "MODEL LOSS 186.71225\n",
      "AUTOENCODER LOSS 137.14351\n",
      "EPOCH 106 / 200\n",
      "ALS LOSS 0.45086955827445857\n",
      "MODEL LOSS 186.653\n",
      "AUTOENCODER LOSS 137.13615\n",
      "EPOCH 107 / 200\n",
      "ALS LOSS 0.45096633005034636\n",
      "MODEL LOSS 186.68971\n",
      "AUTOENCODER LOSS 137.15793\n",
      "EPOCH 108 / 200\n",
      "ALS LOSS 0.4509638979176838\n",
      "MODEL LOSS 186.61568\n",
      "AUTOENCODER LOSS 137.16196\n",
      "EPOCH 109 / 200\n",
      "ALS LOSS 0.45089564923205655\n",
      "MODEL LOSS 186.66151\n",
      "AUTOENCODER LOSS 137.15958\n",
      "EPOCH 110 / 200\n",
      "ALS LOSS 0.45092630949845497\n",
      "MODEL LOSS 186.66158\n",
      "AUTOENCODER LOSS 137.17055\n",
      "EPOCH 111 / 200\n",
      "ALS LOSS 0.4509122737295835\n",
      "MODEL LOSS 186.56613\n",
      "AUTOENCODER LOSS 137.13646\n",
      "EPOCH 112 / 200\n",
      "ALS LOSS 0.4510131258644501\n",
      "MODEL LOSS 186.5516\n",
      "AUTOENCODER LOSS 137.16594\n",
      "EPOCH 113 / 200\n",
      "ALS LOSS 0.4509284992668603\n",
      "MODEL LOSS 186.64684\n",
      "AUTOENCODER LOSS 137.15723\n",
      "EPOCH 114 / 200\n",
      "ALS LOSS 0.4509590694331455\n",
      "MODEL LOSS 186.52625\n",
      "AUTOENCODER LOSS 137.14845\n",
      "EPOCH 115 / 200\n",
      "ALS LOSS 0.45084977851692454\n",
      "MODEL LOSS 186.56189\n",
      "AUTOENCODER LOSS 137.12247\n",
      "EPOCH 116 / 200\n",
      "ALS LOSS 0.4508811731147689\n",
      "MODEL LOSS 186.52472\n",
      "AUTOENCODER LOSS 137.11095\n",
      "EPOCH 117 / 200\n",
      "ALS LOSS 0.45088863889870695\n",
      "MODEL LOSS 186.4236\n",
      "AUTOENCODER LOSS 137.08621\n",
      "EPOCH 118 / 200\n",
      "ALS LOSS 0.4508494745940956\n",
      "MODEL LOSS 186.56664\n",
      "AUTOENCODER LOSS 137.18707\n",
      "EPOCH 119 / 200\n",
      "ALS LOSS 0.45073956340153787\n",
      "MODEL LOSS 186.5736\n",
      "AUTOENCODER LOSS 137.13809\n",
      "EPOCH 120 / 200\n",
      "ALS LOSS 0.4508560157267526\n",
      "MODEL LOSS 186.46527\n",
      "AUTOENCODER LOSS 137.10574\n",
      "EPOCH 121 / 200\n",
      "ALS LOSS 0.45077412830689967\n",
      "MODEL LOSS 186.63057\n",
      "AUTOENCODER LOSS 137.24568\n",
      "EPOCH 122 / 200\n",
      "ALS LOSS 0.4508345738921843\n",
      "MODEL LOSS 186.4846\n",
      "AUTOENCODER LOSS 137.11984\n",
      "EPOCH 123 / 200\n",
      "ALS LOSS 0.45084706665464297\n",
      "MODEL LOSS 186.49187\n",
      "AUTOENCODER LOSS 137.14629\n",
      "EPOCH 124 / 200\n",
      "ALS LOSS 0.4508293784081123\n",
      "MODEL LOSS 186.49387\n",
      "AUTOENCODER LOSS 137.16867\n",
      "EPOCH 125 / 200\n",
      "ALS LOSS 0.450820787864738\n",
      "MODEL LOSS 186.54694\n",
      "AUTOENCODER LOSS 137.18764\n",
      "EPOCH 126 / 200\n",
      "ALS LOSS 0.4508338452711941\n",
      "MODEL LOSS 186.51555\n",
      "AUTOENCODER LOSS 137.18198\n",
      "EPOCH 127 / 200\n",
      "ALS LOSS 0.4508471054100719\n",
      "MODEL LOSS 186.4658\n",
      "AUTOENCODER LOSS 137.166\n",
      "EPOCH 128 / 200\n",
      "ALS LOSS 0.4507632558054817\n",
      "MODEL LOSS 186.44543\n",
      "AUTOENCODER LOSS 137.1493\n",
      "EPOCH 129 / 200\n",
      "ALS LOSS 0.45071074086808965\n",
      "MODEL LOSS 186.44545\n",
      "AUTOENCODER LOSS 137.10883\n",
      "EPOCH 130 / 200\n",
      "ALS LOSS 0.4507828036256662\n",
      "MODEL LOSS 186.58527\n",
      "AUTOENCODER LOSS 137.23091\n",
      "EPOCH 131 / 200\n",
      "ALS LOSS 0.45080885392541387\n",
      "MODEL LOSS 186.40393\n",
      "AUTOENCODER LOSS 137.12056\n",
      "EPOCH 132 / 200\n",
      "ALS LOSS 0.45081226177661793\n",
      "MODEL LOSS 186.44774\n",
      "AUTOENCODER LOSS 137.12602\n",
      "EPOCH 133 / 200\n",
      "ALS LOSS 0.45081232842375407\n",
      "MODEL LOSS 186.40558\n",
      "AUTOENCODER LOSS 137.14691\n",
      "EPOCH 134 / 200\n",
      "ALS LOSS 0.45072058182126257\n",
      "MODEL LOSS 186.41086\n",
      "AUTOENCODER LOSS 137.18372\n",
      "EPOCH 135 / 200\n",
      "ALS LOSS 0.45060372196726073\n",
      "MODEL LOSS 186.60362\n",
      "AUTOENCODER LOSS 137.27992\n",
      "EPOCH 136 / 200\n",
      "ALS LOSS 0.4506809664837072\n",
      "MODEL LOSS 186.3373\n",
      "AUTOENCODER LOSS 137.09724\n",
      "EPOCH 137 / 200\n",
      "ALS LOSS 0.45076862315956123\n",
      "MODEL LOSS 186.38469\n",
      "AUTOENCODER LOSS 137.12154\n",
      "EPOCH 138 / 200\n",
      "ALS LOSS 0.4506874214085199\n",
      "MODEL LOSS 186.45311\n",
      "AUTOENCODER LOSS 137.14755\n",
      "EPOCH 139 / 200\n",
      "ALS LOSS 0.45067309214408907\n",
      "MODEL LOSS 186.41576\n",
      "AUTOENCODER LOSS 137.14128\n",
      "EPOCH 140 / 200\n",
      "ALS LOSS 0.4507563107900781\n",
      "MODEL LOSS 186.43277\n",
      "AUTOENCODER LOSS 137.17784\n",
      "EPOCH 141 / 200\n",
      "ALS LOSS 0.45067293797005836\n",
      "MODEL LOSS 186.3494\n",
      "AUTOENCODER LOSS 137.10936\n",
      "EPOCH 142 / 200\n",
      "ALS LOSS 0.4506809364537149\n",
      "MODEL LOSS 186.4317\n",
      "AUTOENCODER LOSS 137.18703\n",
      "EPOCH 143 / 200\n",
      "ALS LOSS 0.45073107581915367\n",
      "MODEL LOSS 186.43999\n",
      "AUTOENCODER LOSS 137.22748\n",
      "EPOCH 144 / 200\n",
      "ALS LOSS 0.45074655345027015\n",
      "MODEL LOSS 186.3599\n",
      "AUTOENCODER LOSS 137.18053\n",
      "EPOCH 145 / 200\n",
      "ALS LOSS 0.450594639359994\n",
      "MODEL LOSS 186.35059\n",
      "AUTOENCODER LOSS 137.16977\n",
      "EPOCH 146 / 200\n",
      "ALS LOSS 0.4507014826176899\n",
      "MODEL LOSS 186.32382\n",
      "AUTOENCODER LOSS 137.11914\n",
      "EPOCH 147 / 200\n",
      "ALS LOSS 0.4506150816459253\n",
      "MODEL LOSS 186.38492\n",
      "AUTOENCODER LOSS 137.17578\n",
      "EPOCH 148 / 200\n",
      "ALS LOSS 0.45059692694168885\n",
      "MODEL LOSS 186.34183\n",
      "AUTOENCODER LOSS 137.12944\n",
      "EPOCH 149 / 200\n",
      "ALS LOSS 0.45076736005991347\n",
      "MODEL LOSS 186.33589\n",
      "AUTOENCODER LOSS 137.16028\n",
      "EPOCH 150 / 200\n",
      "ALS LOSS 0.4507919355263979\n",
      "MODEL LOSS 186.28368\n",
      "AUTOENCODER LOSS 137.1503\n",
      "EPOCH 151 / 200\n",
      "ALS LOSS 0.45064604134409775\n",
      "MODEL LOSS 186.38358\n",
      "AUTOENCODER LOSS 137.17194\n",
      "EPOCH 152 / 200\n",
      "ALS LOSS 0.4505600534951961\n",
      "MODEL LOSS 186.34583\n",
      "AUTOENCODER LOSS 137.20027\n",
      "EPOCH 153 / 200\n",
      "ALS LOSS 0.45069751590017687\n",
      "MODEL LOSS 186.31389\n",
      "AUTOENCODER LOSS 137.16403\n",
      "EPOCH 154 / 200\n",
      "ALS LOSS 0.45067147864219287\n",
      "MODEL LOSS 186.3352\n",
      "AUTOENCODER LOSS 137.15813\n",
      "EPOCH 155 / 200\n",
      "ALS LOSS 0.45067370508582\n",
      "MODEL LOSS 186.25534\n",
      "AUTOENCODER LOSS 137.0718\n",
      "EPOCH 156 / 200\n",
      "ALS LOSS 0.4506821838631131\n",
      "MODEL LOSS 186.23885\n",
      "AUTOENCODER LOSS 137.10136\n",
      "EPOCH 157 / 200\n",
      "ALS LOSS 0.4506408434219582\n",
      "MODEL LOSS 186.26875\n",
      "AUTOENCODER LOSS 137.10547\n",
      "EPOCH 158 / 200\n",
      "ALS LOSS 0.45058799169652713\n",
      "MODEL LOSS 186.31302\n",
      "AUTOENCODER LOSS 137.16846\n",
      "EPOCH 159 / 200\n",
      "ALS LOSS 0.4506465270155527\n",
      "MODEL LOSS 186.25407\n",
      "AUTOENCODER LOSS 137.16954\n",
      "EPOCH 160 / 200\n",
      "ALS LOSS 0.45064983101027234\n",
      "MODEL LOSS 186.20673\n",
      "AUTOENCODER LOSS 137.12114\n",
      "EPOCH 161 / 200\n",
      "ALS LOSS 0.45065058110125567\n",
      "MODEL LOSS 186.2086\n",
      "AUTOENCODER LOSS 137.09953\n",
      "EPOCH 162 / 200\n",
      "ALS LOSS 0.45065735981407073\n",
      "MODEL LOSS 186.29137\n",
      "AUTOENCODER LOSS 137.18898\n",
      "EPOCH 163 / 200\n",
      "ALS LOSS 0.4506048397788005\n",
      "MODEL LOSS 186.21852\n",
      "AUTOENCODER LOSS 137.08441\n",
      "EPOCH 164 / 200\n",
      "ALS LOSS 0.4506462707905016\n",
      "MODEL LOSS 186.2626\n",
      "AUTOENCODER LOSS 137.1357\n",
      "EPOCH 165 / 200\n",
      "ALS LOSS 0.45069490257232814\n",
      "MODEL LOSS 186.28584\n",
      "AUTOENCODER LOSS 137.19594\n",
      "EPOCH 166 / 200\n",
      "ALS LOSS 0.45053425373917183\n",
      "MODEL LOSS 186.24062\n",
      "AUTOENCODER LOSS 137.1143\n",
      "EPOCH 167 / 200\n",
      "ALS LOSS 0.4506231364649884\n",
      "MODEL LOSS 186.27817\n",
      "AUTOENCODER LOSS 137.1368\n",
      "EPOCH 168 / 200\n",
      "ALS LOSS 0.4506793839570855\n",
      "MODEL LOSS 186.33066\n",
      "AUTOENCODER LOSS 137.20525\n",
      "EPOCH 169 / 200\n",
      "ALS LOSS 0.45061197657302465\n",
      "MODEL LOSS 186.25287\n",
      "AUTOENCODER LOSS 137.21715\n",
      "EPOCH 170 / 200\n",
      "ALS LOSS 0.450542372348638\n",
      "MODEL LOSS 186.28467\n",
      "AUTOENCODER LOSS 137.15\n",
      "EPOCH 171 / 200\n",
      "ALS LOSS 0.4505715105143333\n",
      "MODEL LOSS 186.20505\n",
      "AUTOENCODER LOSS 137.10649\n",
      "EPOCH 172 / 200\n",
      "ALS LOSS 0.4505822110033545\n",
      "MODEL LOSS 186.2389\n",
      "AUTOENCODER LOSS 137.18088\n",
      "EPOCH 173 / 200\n",
      "ALS LOSS 0.45054255164156015\n",
      "MODEL LOSS 186.203\n",
      "AUTOENCODER LOSS 137.13103\n",
      "EPOCH 174 / 200\n",
      "ALS LOSS 0.45056425614001766\n",
      "MODEL LOSS 186.14713\n",
      "AUTOENCODER LOSS 137.1064\n",
      "EPOCH 175 / 200\n",
      "ALS LOSS 0.4505281777024773\n",
      "MODEL LOSS 186.1895\n",
      "AUTOENCODER LOSS 137.12848\n",
      "EPOCH 176 / 200\n",
      "ALS LOSS 0.45053048542563656\n",
      "MODEL LOSS 186.19911\n",
      "AUTOENCODER LOSS 137.17601\n",
      "EPOCH 177 / 200\n",
      "ALS LOSS 0.45051262393224084\n",
      "MODEL LOSS 186.19511\n",
      "AUTOENCODER LOSS 137.14389\n",
      "EPOCH 178 / 200\n",
      "ALS LOSS 0.45048310013161597\n",
      "MODEL LOSS 186.26567\n",
      "AUTOENCODER LOSS 137.13712\n",
      "EPOCH 179 / 200\n",
      "ALS LOSS 0.4504747074089424\n",
      "MODEL LOSS 186.25359\n",
      "AUTOENCODER LOSS 137.11726\n",
      "EPOCH 180 / 200\n",
      "ALS LOSS 0.4505713179392402\n",
      "MODEL LOSS 186.27946\n",
      "AUTOENCODER LOSS 137.12866\n",
      "EPOCH 181 / 200\n",
      "ALS LOSS 0.4506187125998483\n",
      "MODEL LOSS 186.175\n",
      "AUTOENCODER LOSS 137.13007\n",
      "EPOCH 182 / 200\n",
      "ALS LOSS 0.45056714776398216\n",
      "MODEL LOSS 186.27892\n",
      "AUTOENCODER LOSS 137.1758\n",
      "EPOCH 183 / 200\n",
      "ALS LOSS 0.4504792439238162\n",
      "MODEL LOSS 186.17087\n",
      "AUTOENCODER LOSS 137.07602\n",
      "EPOCH 184 / 200\n",
      "ALS LOSS 0.4505466505137078\n",
      "MODEL LOSS 186.25989\n",
      "AUTOENCODER LOSS 137.18378\n",
      "EPOCH 185 / 200\n",
      "ALS LOSS 0.45050581106226195\n",
      "MODEL LOSS 186.31396\n",
      "AUTOENCODER LOSS 137.17136\n",
      "EPOCH 186 / 200\n",
      "ALS LOSS 0.45052346668528304\n",
      "MODEL LOSS 186.25253\n",
      "AUTOENCODER LOSS 137.15097\n",
      "EPOCH 187 / 200\n",
      "ALS LOSS 0.45053867203317277\n",
      "MODEL LOSS 186.29253\n",
      "AUTOENCODER LOSS 137.19868\n",
      "EPOCH 188 / 200\n",
      "ALS LOSS 0.45047632334999976\n",
      "MODEL LOSS 186.20966\n",
      "AUTOENCODER LOSS 137.14636\n",
      "EPOCH 189 / 200\n",
      "ALS LOSS 0.45052097335846375\n",
      "MODEL LOSS 186.26054\n",
      "AUTOENCODER LOSS 137.14566\n",
      "EPOCH 190 / 200\n",
      "ALS LOSS 0.45055264041375226\n",
      "MODEL LOSS 186.19026\n",
      "AUTOENCODER LOSS 137.13531\n",
      "EPOCH 191 / 200\n",
      "ALS LOSS 0.4505918407512877\n",
      "MODEL LOSS 186.15958\n",
      "AUTOENCODER LOSS 137.12663\n",
      "EPOCH 192 / 200\n",
      "ALS LOSS 0.45058567519771736\n",
      "MODEL LOSS 186.2174\n",
      "AUTOENCODER LOSS 137.14392\n",
      "EPOCH 193 / 200\n",
      "ALS LOSS 0.4505368267269256\n",
      "MODEL LOSS 186.29192\n",
      "AUTOENCODER LOSS 137.24126\n",
      "EPOCH 194 / 200\n",
      "ALS LOSS 0.45058636029813004\n",
      "MODEL LOSS 186.24385\n",
      "AUTOENCODER LOSS 137.17055\n",
      "EPOCH 195 / 200\n",
      "ALS LOSS 0.45054242623518\n",
      "MODEL LOSS 186.199\n",
      "AUTOENCODER LOSS 137.14224\n",
      "EPOCH 196 / 200\n",
      "ALS LOSS 0.4505655507910153\n",
      "MODEL LOSS 186.20157\n",
      "AUTOENCODER LOSS 137.17696\n",
      "EPOCH 197 / 200\n",
      "ALS LOSS 0.450460586101962\n",
      "MODEL LOSS 186.09799\n",
      "AUTOENCODER LOSS 137.08907\n",
      "EPOCH 198 / 200\n",
      "ALS LOSS 0.45045896410577757\n",
      "MODEL LOSS 186.18852\n",
      "AUTOENCODER LOSS 137.15466\n",
      "EPOCH 199 / 200\n",
      "ALS LOSS 0.4505142416337486\n",
      "MODEL LOSS 186.22638\n",
      "AUTOENCODER LOSS 137.178\n",
      "EPOCH 200 / 200\n",
      "ALS LOSS 0.450539080296222\n",
      "MODEL LOSS 186.09479\n",
      "AUTOENCODER LOSS 137.11716\n"
     ]
    }
   ],
   "source": [
    "cdl = CDL(rating_matrix, item_infomation_matrix, out_path=XP_PATH, k=50, hidden_size=150, \n",
    "          matrix_noise=0.3, drop_ratio=0.1, epochs=200, lambda_w=20, lambda_v=100, \n",
    "          lambda_u=0.1, lambda_n=10)\n",
    "U, V, beta_u, beta_v = cdl.training() #188910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: 24303 x 50\n",
      "V shape: 10668 x 50\n",
      "beta_u shape: 24303\n",
      "beta_v shape: 10668\n"
     ]
    }
   ],
   "source": [
    "## dump U and V matricies to pickle files\n",
    "print(\"U shape: %s x %s\" % U.shape)\n",
    "print(\"V shape: %s x %s\" % V.shape)\n",
    "\n",
    "print(\"beta_u shape: %s\" % beta_u.shape)\n",
    "print(\"beta_v shape: %s\" % beta_v.shape)\n",
    "\n",
    "with open(U_V_PATH + 'U_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(U, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(U_V_PATH + 'V_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(V, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(U_V_PATH + 'beta_u_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(beta_u, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(U_V_PATH + 'beta_v_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(beta_v, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    rm = np.array(\n",
    "        [[1, 2, 3, 7, 8],\n",
    "         [1, 5, 2, 1, 3],\n",
    "         [1, 7, 2, 1, 10],\n",
    "         [1, 2, 3, 4, 50]])\n",
    "    iim = np.array([[0.7, 0.8, 0.9],[ 1, 0.2, 0.3],[0.5, 0.6, 0.7],[0.5, 0.6, 0.7],[0.1, 0.2, 1.0]])\n",
    "    cdl = CDL(rm, iim, k=3, hidden_size=20, lr=0.0001, epochs=50)\n",
    "    U, V, beta_u, beta_v = cdl.training() #188910\n",
    "    \n",
    "    print('## U and V matricies ##')\n",
    "    print(U)\n",
    "    print(V)\n",
    "    \n",
    "    print('## U and V biased ##')\n",
    "    print(beta_u)\n",
    "    print(beta_v)\n",
    "    \n",
    "    print('## Predictions ##')\n",
    "    predictions = np.dot(U, V.T) + beta_u.reshape(-1, 1) + beta_v.reshape(1, -1)\n",
    "        \n",
    "    print(predictions)\n",
    "    \n",
    "    print('## Initial matrix ##')\n",
    "    print(rm)\n",
    "    \n",
    "    return rm, U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (5, 3)\n",
      "Noising completed..:(5, 3)\n",
      "EPOCH 1 / 50\n",
      "ALS LOSS 1.2046472436371132\n",
      "MODEL LOSS 70.18561\n",
      "AUTOENCODER LOSS 3.908306\n",
      "EPOCH 2 / 50\n",
      "ALS LOSS 0.7107355986833231\n",
      "MODEL LOSS 54.93246\n",
      "AUTOENCODER LOSS 3.9942102\n",
      "EPOCH 3 / 50\n",
      "ALS LOSS 0.47965679562842073\n",
      "MODEL LOSS 57.021286\n",
      "AUTOENCODER LOSS 4.6897564\n",
      "EPOCH 4 / 50\n",
      "ALS LOSS 0.43347562379434734\n",
      "MODEL LOSS 54.94312\n",
      "AUTOENCODER LOSS 4.1818175\n",
      "EPOCH 5 / 50\n",
      "ALS LOSS 0.41045281871592165\n",
      "MODEL LOSS 52.2169\n",
      "AUTOENCODER LOSS 3.5133886\n",
      "EPOCH 6 / 50\n",
      "ALS LOSS 0.4222943511838082\n",
      "MODEL LOSS 54.221687\n",
      "AUTOENCODER LOSS 3.9598656\n",
      "EPOCH 7 / 50\n",
      "ALS LOSS 0.3965419194918399\n",
      "MODEL LOSS 53.519455\n",
      "AUTOENCODER LOSS 3.5970554\n",
      "EPOCH 8 / 50\n",
      "ALS LOSS 0.3636557769409453\n",
      "MODEL LOSS 53.911064\n",
      "AUTOENCODER LOSS 2.9565032\n",
      "EPOCH 9 / 50\n",
      "ALS LOSS 0.4192090592784522\n",
      "MODEL LOSS 54.902596\n",
      "AUTOENCODER LOSS 4.3583302\n",
      "EPOCH 10 / 50\n",
      "ALS LOSS 0.3871567019835682\n",
      "MODEL LOSS 54.66372\n",
      "AUTOENCODER LOSS 3.700183\n",
      "EPOCH 11 / 50\n",
      "ALS LOSS 0.39751770303337386\n",
      "MODEL LOSS 54.620716\n",
      "AUTOENCODER LOSS 4.455678\n",
      "EPOCH 12 / 50\n",
      "ALS LOSS 0.4539292362249477\n",
      "MODEL LOSS 52.447548\n",
      "AUTOENCODER LOSS 3.318207\n",
      "EPOCH 13 / 50\n",
      "ALS LOSS 0.36740222821432145\n",
      "MODEL LOSS 53.224632\n",
      "AUTOENCODER LOSS 3.4375806\n",
      "EPOCH 14 / 50\n",
      "ALS LOSS 0.3898055564421202\n",
      "MODEL LOSS 54.746937\n",
      "AUTOENCODER LOSS 4.04059\n",
      "EPOCH 15 / 50\n",
      "ALS LOSS 0.3827678993168763\n",
      "MODEL LOSS 53.923397\n",
      "AUTOENCODER LOSS 3.5313249\n",
      "EPOCH 16 / 50\n",
      "ALS LOSS 0.3975624762506108\n",
      "MODEL LOSS 54.04081\n",
      "AUTOENCODER LOSS 4.992781\n",
      "EPOCH 17 / 50\n",
      "ALS LOSS 0.40658917991250465\n",
      "MODEL LOSS 53.986412\n",
      "AUTOENCODER LOSS 4.6615\n",
      "EPOCH 18 / 50\n",
      "ALS LOSS 0.37287442213505306\n",
      "MODEL LOSS 54.280422\n",
      "AUTOENCODER LOSS 4.2721586\n",
      "EPOCH 19 / 50\n",
      "ALS LOSS 0.389812284841781\n",
      "MODEL LOSS 52.99608\n",
      "AUTOENCODER LOSS 3.45398\n",
      "EPOCH 20 / 50\n",
      "ALS LOSS 0.4287288340507735\n",
      "MODEL LOSS 55.47823\n",
      "AUTOENCODER LOSS 4.620693\n",
      "EPOCH 21 / 50\n",
      "ALS LOSS 0.41092323978552237\n",
      "MODEL LOSS 52.61683\n",
      "AUTOENCODER LOSS 3.4477897\n",
      "EPOCH 22 / 50\n",
      "ALS LOSS 0.3900680314040781\n",
      "MODEL LOSS 52.23166\n",
      "AUTOENCODER LOSS 3.3076432\n",
      "EPOCH 23 / 50\n",
      "ALS LOSS 0.3625940934529672\n",
      "MODEL LOSS 56.908463\n",
      "AUTOENCODER LOSS 5.7948384\n",
      "EPOCH 24 / 50\n",
      "ALS LOSS 0.41246117262869386\n",
      "MODEL LOSS 53.222473\n",
      "AUTOENCODER LOSS 3.559743\n",
      "EPOCH 25 / 50\n",
      "ALS LOSS 0.3796314998269984\n",
      "MODEL LOSS 53.446724\n",
      "AUTOENCODER LOSS 3.4101472\n",
      "EPOCH 26 / 50\n",
      "ALS LOSS 0.38154322150252834\n",
      "MODEL LOSS 54.040016\n",
      "AUTOENCODER LOSS 2.7555144\n",
      "EPOCH 27 / 50\n",
      "ALS LOSS 0.40144324759684075\n",
      "MODEL LOSS 52.42375\n",
      "AUTOENCODER LOSS 3.8238873\n",
      "EPOCH 28 / 50\n",
      "ALS LOSS 0.37900305590058386\n",
      "MODEL LOSS 53.323223\n",
      "AUTOENCODER LOSS 3.48909\n",
      "EPOCH 29 / 50\n",
      "ALS LOSS 0.3920359599066403\n",
      "MODEL LOSS 50.90966\n",
      "AUTOENCODER LOSS 2.8763626\n",
      "EPOCH 30 / 50\n",
      "ALS LOSS 0.36987940459576907\n",
      "MODEL LOSS 51.120804\n",
      "AUTOENCODER LOSS 3.2659197\n",
      "EPOCH 31 / 50\n",
      "ALS LOSS 0.37107877349878615\n",
      "MODEL LOSS 51.918686\n",
      "AUTOENCODER LOSS 3.5624309\n",
      "EPOCH 32 / 50\n",
      "ALS LOSS 0.436857184516748\n",
      "MODEL LOSS 50.272545\n",
      "AUTOENCODER LOSS 2.7302663\n",
      "EPOCH 33 / 50\n",
      "ALS LOSS 0.36976033011488824\n",
      "MODEL LOSS 52.11522\n",
      "AUTOENCODER LOSS 2.557641\n",
      "EPOCH 34 / 50\n",
      "ALS LOSS 0.3850831932387499\n",
      "MODEL LOSS 53.20571\n",
      "AUTOENCODER LOSS 3.9121046\n",
      "EPOCH 35 / 50\n",
      "ALS LOSS 0.38926368946379375\n",
      "MODEL LOSS 52.127495\n",
      "AUTOENCODER LOSS 2.2882845\n",
      "EPOCH 36 / 50\n",
      "ALS LOSS 0.3776858100527731\n",
      "MODEL LOSS 52.513927\n",
      "AUTOENCODER LOSS 4.7963533\n",
      "EPOCH 37 / 50\n",
      "ALS LOSS 0.376465094248401\n",
      "MODEL LOSS 54.57531\n",
      "AUTOENCODER LOSS 3.5958948\n",
      "EPOCH 38 / 50\n",
      "ALS LOSS 0.425153914872325\n",
      "MODEL LOSS 50.39634\n",
      "AUTOENCODER LOSS 3.0894408\n",
      "EPOCH 39 / 50\n",
      "ALS LOSS 0.41813535660252615\n",
      "MODEL LOSS 50.611675\n",
      "AUTOENCODER LOSS 3.2095509\n",
      "EPOCH 40 / 50\n",
      "ALS LOSS 0.4041529795347897\n",
      "MODEL LOSS 53.793423\n",
      "AUTOENCODER LOSS 4.321288\n",
      "EPOCH 41 / 50\n",
      "ALS LOSS 0.4288291188498908\n",
      "MODEL LOSS 52.38485\n",
      "AUTOENCODER LOSS 4.1392984\n",
      "EPOCH 42 / 50\n",
      "ALS LOSS 0.43709973050045975\n",
      "MODEL LOSS 51.74031\n",
      "AUTOENCODER LOSS 3.4415605\n",
      "EPOCH 43 / 50\n",
      "ALS LOSS 0.38083762195941007\n",
      "MODEL LOSS 50.35656\n",
      "AUTOENCODER LOSS 2.8269858\n",
      "EPOCH 44 / 50\n",
      "ALS LOSS 0.391082748107017\n",
      "MODEL LOSS 51.084023\n",
      "AUTOENCODER LOSS 3.747398\n",
      "EPOCH 45 / 50\n",
      "ALS LOSS 0.390651427070944\n",
      "MODEL LOSS 53.35649\n",
      "AUTOENCODER LOSS 2.8139086\n",
      "EPOCH 46 / 50\n",
      "ALS LOSS 0.39765372231028423\n",
      "MODEL LOSS 49.517532\n",
      "AUTOENCODER LOSS 3.2340178\n",
      "EPOCH 47 / 50\n",
      "ALS LOSS 0.3839281213933135\n",
      "MODEL LOSS 52.28772\n",
      "AUTOENCODER LOSS 5.1709347\n",
      "EPOCH 48 / 50\n",
      "ALS LOSS 0.3888654144691977\n",
      "MODEL LOSS 51.318684\n",
      "AUTOENCODER LOSS 3.9621942\n",
      "EPOCH 49 / 50\n",
      "ALS LOSS 0.3667855549637301\n",
      "MODEL LOSS 51.134094\n",
      "AUTOENCODER LOSS 2.9501274\n",
      "EPOCH 50 / 50\n",
      "ALS LOSS 0.43043058847748905\n",
      "MODEL LOSS 50.71234\n",
      "AUTOENCODER LOSS 4.4432464\n",
      "## U and V matricies ##\n",
      "[[-0.0922356  -0.49859609  2.07652929]\n",
      " [ 1.14245349  0.28609908  0.01784949]\n",
      " [ 1.53235778  1.22540724  0.40715635]\n",
      " [-1.00201208  5.47996279  3.81003181]]\n",
      "[[ 0.87095557 -0.01173411  0.49622267]\n",
      " [ 2.49756425  0.41562221  0.17232498]\n",
      " [ 1.10866652 -0.08458112  1.04929586]\n",
      " [-0.03115225 -0.81835596  1.86993594]\n",
      " [ 0.03697252  5.62121863  4.41293643]]\n",
      "## U and V biased ##\n",
      "[0.7835553  0.44061305 0.29892654 0.48821537]\n",
      "[-0.48873308  1.25098157  0.03256904  0.90983667  0.91596998]\n",
      "## Predictions ##\n",
      "[[ 1.25076059  1.95478277  2.93493123  5.98727116  8.0569892 ]\n",
      " [ 0.9524064   4.66693065  1.73431285  1.11410625  3.08581656]\n",
      " [ 1.33247014  5.95653977  2.3539505   0.91956378  9.95658874]\n",
      " [ 0.95309595  2.17076525  2.94423638  4.06922214 48.98463552]]\n",
      "## Initial matrix ##\n",
      "[[ 1  2  3  7  8]\n",
      " [ 1  5  2  1  3]\n",
      " [ 1  7  2  1 10]\n",
      " [ 1  2  3  4 50]]\n"
     ]
    }
   ],
   "source": [
    "# test run\n",
    "rating_matrix, U, V = test_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing output cache (0 entries)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('./optimiz/'):\n",
    "    os.mkdir('./optimiz/')\n",
    "    \n",
    "%reset Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hidden_size\": [150],\n",
    "    \"k\": [50],\n",
    "    \"matrix_noise\": [0.3],\n",
    "    \"drop_ratio\": [0.1],\n",
    "    \"lambda_u\": [0.01, 0.1],\n",
    "    \"lambda_v\": [100],\n",
    "    \"lambda_w\": [20, 35 ,50],\n",
    "    \"lambda_n\": [10, 0.1]\n",
    "}\n",
    "\n",
    "#lr=0.0001, hidden_size=25, matrix_noise = 0.3, drop_ratio=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(params)\n",
    "combinations = list(it.product(*(params[key] for key in sorted_keys)))\n",
    "df_test_val = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of combinations: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of combinations: %s\" % len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 20}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.2122202412205842\n",
      "MODEL LOSS 39437.574\n",
      "AUTOENCODER LOSS 1314.8956\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.5641612811289409\n",
      "MODEL LOSS 19955.096\n",
      "AUTOENCODER LOSS 222.55614\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.2654116926018575\n",
      "MODEL LOSS 10176.011\n",
      "AUTOENCODER LOSS 167.86546\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.22408585672816306\n",
      "MODEL LOSS 5079.91\n",
      "AUTOENCODER LOSS 152.37798\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.2119584844837788\n",
      "MODEL LOSS 2489.6902\n",
      "AUTOENCODER LOSS 145.43913\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.20476335668733642\n",
      "MODEL LOSS 1226.5087\n",
      "AUTOENCODER LOSS 141.91673\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.20002158483741656\n",
      "MODEL LOSS 642.65576\n",
      "AUTOENCODER LOSS 140.1111\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.19625925818535642\n",
      "MODEL LOSS 387.84445\n",
      "AUTOENCODER LOSS 139.0429\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.19262101184915964\n",
      "MODEL LOSS 280.17197\n",
      "AUTOENCODER LOSS 138.41402\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.18926901698927712\n",
      "MODEL LOSS 234.55235\n",
      "AUTOENCODER LOSS 138.1636\n",
      "MSE (non zero, train set): 0.18926901698927712\n",
      "MAE (non zero, train set): 0.13595532744417507\n",
      "MSE (test set): 1.184975255887487\n",
      "MAE (test set): 0.8884653110989902\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 20}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 35}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.2013536515488097\n",
      "MODEL LOSS 66388.445\n",
      "AUTOENCODER LOSS 591.03827\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.5551523836832006\n",
      "MODEL LOSS 33769.312\n",
      "AUTOENCODER LOSS 204.20432\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.24988088200054262\n",
      "MODEL LOSS 16986.28\n",
      "AUTOENCODER LOSS 151.15942\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.20683965145778443\n",
      "MODEL LOSS 8275.669\n",
      "AUTOENCODER LOSS 141.38023\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.19263417863263405\n",
      "MODEL LOSS 3882.854\n",
      "AUTOENCODER LOSS 139.04657\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.18579919500450667\n",
      "MODEL LOSS 1770.4133\n",
      "AUTOENCODER LOSS 138.12769\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.18142563014619317\n",
      "MODEL LOSS 817.40063\n",
      "AUTOENCODER LOSS 137.79807\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.17821654041713098\n",
      "MODEL LOSS 419.1637\n",
      "AUTOENCODER LOSS 137.60236\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.17586095628159942\n",
      "MODEL LOSS 265.4276\n",
      "AUTOENCODER LOSS 137.52751\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.17409294897542346\n",
      "MODEL LOSS 208.83702\n",
      "AUTOENCODER LOSS 137.40227\n",
      "MSE (non zero, train set): 0.17409294897542346\n",
      "MAE (non zero, train set): 0.12369208685037683\n",
      "MSE (test set): 1.1870318183947308\n",
      "MAE (test set): 0.888333149847921\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 35}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 50}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.2389225852921024\n",
      "MODEL LOSS 93743.63\n",
      "AUTOENCODER LOSS 299.65802\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.5542354093296402\n",
      "MODEL LOSS 47746.66\n",
      "AUTOENCODER LOSS 154.0806\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.24434259784805665\n",
      "MODEL LOSS 23931.838\n",
      "AUTOENCODER LOSS 141.95515\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.19856005296409943\n",
      "MODEL LOSS 11573.235\n",
      "AUTOENCODER LOSS 139.6555\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.18377159700552972\n",
      "MODEL LOSS 5354.68\n",
      "AUTOENCODER LOSS 138.65567\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.17710825220164794\n",
      "MODEL LOSS 2376.22\n",
      "AUTOENCODER LOSS 138.08339\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.17334705108616286\n",
      "MODEL LOSS 1041.3646\n",
      "AUTOENCODER LOSS 137.76527\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.17085658251447097\n",
      "MODEL LOSS 489.89288\n",
      "AUTOENCODER LOSS 137.62387\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.16929377809345006\n",
      "MODEL LOSS 281.48483\n",
      "AUTOENCODER LOSS 137.4882\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.1679085175664804\n",
      "MODEL LOSS 208.43942\n",
      "AUTOENCODER LOSS 137.4698\n",
      "MSE (non zero, train set): 0.1679085175664804\n",
      "MAE (non zero, train set): 0.1183442852571743\n",
      "MSE (test set): 1.1926778004740675\n",
      "MAE (test set): 0.8917341362704042\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 50}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 20}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.1620217464021754\n",
      "MODEL LOSS 39798.992\n",
      "AUTOENCODER LOSS 1477.1569\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.862306142159999\n",
      "MODEL LOSS 20162.613\n",
      "AUTOENCODER LOSS 228.69368\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.5338940437025856\n",
      "MODEL LOSS 10337.131\n",
      "AUTOENCODER LOSS 169.64194\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.49149083969233154\n",
      "MODEL LOSS 5204.865\n",
      "AUTOENCODER LOSS 154.09267\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.4796809991900304\n",
      "MODEL LOSS 2589.274\n",
      "AUTOENCODER LOSS 146.59607\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.47417953493783843\n",
      "MODEL LOSS 1311.5724\n",
      "AUTOENCODER LOSS 142.5764\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.4707020184945491\n",
      "MODEL LOSS 716.9991\n",
      "AUTOENCODER LOSS 140.46196\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.4690580019995887\n",
      "MODEL LOSS 453.5078\n",
      "AUTOENCODER LOSS 139.24591\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.4671001108013494\n",
      "MODEL LOSS 340.28513\n",
      "AUTOENCODER LOSS 138.73349\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.46628266583661354\n",
      "MODEL LOSS 289.22885\n",
      "AUTOENCODER LOSS 138.39017\n",
      "MSE (non zero, train set): 0.46628266583661354\n",
      "MAE (non zero, train set): 0.3464103763098676\n",
      "MSE (test set): 1.1252311061183657\n",
      "MAE (test set): 0.8566652107318268\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 20}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 35}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.1718365890720417\n",
      "MODEL LOSS 66148.79\n",
      "AUTOENCODER LOSS 560.0478\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.8837008435665867\n",
      "MODEL LOSS 33647.754\n",
      "AUTOENCODER LOSS 203.14989\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.5367434677287396\n",
      "MODEL LOSS 16921.285\n",
      "AUTOENCODER LOSS 151.39981\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.4878742602588825\n",
      "MODEL LOSS 8241.952\n",
      "AUTOENCODER LOSS 141.4942\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.47420440447356826\n",
      "MODEL LOSS 3869.288\n",
      "AUTOENCODER LOSS 139.1088\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.4682439348755853\n",
      "MODEL LOSS 1769.7075\n",
      "AUTOENCODER LOSS 138.23401\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.46487065900440644\n",
      "MODEL LOSS 824.9268\n",
      "AUTOENCODER LOSS 137.88373\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.46271213318561444\n",
      "MODEL LOSS 431.69144\n",
      "AUTOENCODER LOSS 137.69333\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.4608788570001418\n",
      "MODEL LOSS 280.7613\n",
      "AUTOENCODER LOSS 137.56161\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.4595868022056533\n",
      "MODEL LOSS 226.2384\n",
      "AUTOENCODER LOSS 137.5793\n",
      "MSE (non zero, train set): 0.4595868022056533\n",
      "MAE (non zero, train set): 0.3390827081502695\n",
      "MSE (test set): 1.1321918709965402\n",
      "MAE (test set): 0.8630292761192074\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 35}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 50}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.188698272655973\n",
      "MODEL LOSS 93745.69\n",
      "AUTOENCODER LOSS 317.06396\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.8919344114470724\n",
      "MODEL LOSS 47753.867\n",
      "AUTOENCODER LOSS 154.91068\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.5423028445593301\n",
      "MODEL LOSS 23955.066\n",
      "AUTOENCODER LOSS 142.00464\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.4879428160413318\n",
      "MODEL LOSS 11597.368\n",
      "AUTOENCODER LOSS 139.65253\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.47332778595632036\n",
      "MODEL LOSS 5376.5312\n",
      "AUTOENCODER LOSS 138.75743\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.46728503966943263\n",
      "MODEL LOSS 2396.13\n",
      "AUTOENCODER LOSS 138.24402\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.4638630902894745\n",
      "MODEL LOSS 1060.0939\n",
      "AUTOENCODER LOSS 137.82953\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.4617472571315722\n",
      "MODEL LOSS 508.28528\n",
      "AUTOENCODER LOSS 137.63449\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.46011786776314345\n",
      "MODEL LOSS 299.9815\n",
      "AUTOENCODER LOSS 137.50648\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.4588998799765463\n",
      "MODEL LOSS 227.09892\n",
      "AUTOENCODER LOSS 137.43652\n",
      "MSE (non zero, train set): 0.4588998799765463\n",
      "MAE (non zero, train set): 0.3377824730993184\n",
      "MSE (test set): 1.1354028415561601\n",
      "MAE (test set): 0.866900017861667\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 10, 'lambda_w': 50}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 20}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.2230396896988034\n",
      "MODEL LOSS 37623.17\n",
      "AUTOENCODER LOSS 22.074432\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.5513142885493226\n",
      "MODEL LOSS 19184.668\n",
      "AUTOENCODER LOSS 7.6188235\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.2597740762679186\n",
      "MODEL LOSS 9604.657\n",
      "AUTOENCODER LOSS 3.5680103\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.2163707331491368\n",
      "MODEL LOSS 4631.493\n",
      "AUTOENCODER LOSS 2.2087326\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.20138205632867864\n",
      "MODEL LOSS 2129.2476\n",
      "AUTOENCODER LOSS 1.7382888\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.1931989176362504\n",
      "MODEL LOSS 930.6554\n",
      "AUTOENCODER LOSS 1.5777957\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.1877153773866199\n",
      "MODEL LOSS 393.16327\n",
      "AUTOENCODER LOSS 1.5237845\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.1841174607934378\n",
      "MODEL LOSS 170.62883\n",
      "AUTOENCODER LOSS 1.5066891\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.1812471622573656\n",
      "MODEL LOSS 85.65551\n",
      "AUTOENCODER LOSS 1.5002589\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.17886078676607522\n",
      "MODEL LOSS 55.160168\n",
      "AUTOENCODER LOSS 1.4977106\n",
      "MSE (non zero, train set): 0.17886078676607522\n",
      "MAE (non zero, train set): 0.12770023318732182\n",
      "MSE (test set): 1.1894000085560037\n",
      "MAE (test set): 0.8904438536461169\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 20}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 35}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.2271247614618543\n",
      "MODEL LOSS 65442.9\n",
      "AUTOENCODER LOSS 7.3141694\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.5511421177571716\n",
      "MODEL LOSS 33323.08\n",
      "AUTOENCODER LOSS 2.3642159\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.25176120326119417\n",
      "MODEL LOSS 16650.049\n",
      "AUTOENCODER LOSS 1.670059\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.2070934643448269\n",
      "MODEL LOSS 8000.985\n",
      "AUTOENCODER LOSS 1.5541532\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.19206466365575026\n",
      "MODEL LOSS 3652.9185\n",
      "AUTOENCODER LOSS 1.5352023\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.18496485424983178\n",
      "MODEL LOSS 1573.052\n",
      "AUTOENCODER LOSS 1.5318742\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.1806516625873734\n",
      "MODEL LOSS 642.1303\n",
      "AUTOENCODER LOSS 1.5312955\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.1777448431937212\n",
      "MODEL LOSS 258.08047\n",
      "AUTOENCODER LOSS 1.5309073\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.17537901544789944\n",
      "MODEL LOSS 113.21822\n",
      "AUTOENCODER LOSS 1.5304805\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.17339945198514065\n",
      "MODEL LOSS 62.290993\n",
      "AUTOENCODER LOSS 1.5300967\n",
      "MSE (non zero, train set): 0.17339945198514065\n",
      "MAE (non zero, train set): 0.12318821340254808\n",
      "MSE (test set): 1.190397534631879\n",
      "MAE (test set): 0.8911576404153689\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 35}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 50}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.2177218203462554\n",
      "MODEL LOSS 93440.734\n",
      "AUTOENCODER LOSS 4.6058054\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.5523519057337353\n",
      "MODEL LOSS 47599.887\n",
      "AUTOENCODER LOSS 1.8307798\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.24730646654190583\n",
      "MODEL LOSS 23787.86\n",
      "AUTOENCODER LOSS 1.5856494\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.20104995847267595\n",
      "MODEL LOSS 11427.239\n",
      "AUTOENCODER LOSS 1.554618\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.18632680284750014\n",
      "MODEL LOSS 5207.9214\n",
      "AUTOENCODER LOSS 1.5475131\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.1794374809792487\n",
      "MODEL LOSS 2230.2495\n",
      "AUTOENCODER LOSS 1.5456543\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.17552962961092627\n",
      "MODEL LOSS 896.9923\n",
      "AUTOENCODER LOSS 1.5454127\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.17295072172580966\n",
      "MODEL LOSS 347.57797\n",
      "AUTOENCODER LOSS 1.5453597\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.1710984440814866\n",
      "MODEL LOSS 141.14136\n",
      "AUTOENCODER LOSS 1.5447751\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.1697821004888609\n",
      "MODEL LOSS 69.619064\n",
      "AUTOENCODER LOSS 1.5442988\n",
      "MSE (non zero, train set): 0.1697821004888609\n",
      "MAE (non zero, train set): 0.12008270801930042\n",
      "MSE (test set): 1.1909396806497636\n",
      "MAE (test set): 0.8905462004825075\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.01, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 50}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 20}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.170881267392892\n",
      "MODEL LOSS 37512.617\n",
      "AUTOENCODER LOSS 23.152636\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.8526371356588154\n",
      "MODEL LOSS 19131.695\n",
      "AUTOENCODER LOSS 7.892814\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.5348507141809792\n",
      "MODEL LOSS 9589.114\n",
      "AUTOENCODER LOSS 3.6748354\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.4927950790210573\n",
      "MODEL LOSS 4638.1235\n",
      "AUTOENCODER LOSS 2.2449944\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.4807970774165923\n",
      "MODEL LOSS 2148.5798\n",
      "AUTOENCODER LOSS 1.7467786\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.47496765936971086\n",
      "MODEL LOSS 956.4835\n",
      "AUTOENCODER LOSS 1.5773776\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.470910410642332\n",
      "MODEL LOSS 423.23422\n",
      "AUTOENCODER LOSS 1.5224749\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.46846280114912137\n",
      "MODEL LOSS 202.4601\n",
      "AUTOENCODER LOSS 1.5044996\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.4661029642413651\n",
      "MODEL LOSS 118.139114\n",
      "AUTOENCODER LOSS 1.4988204\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.46458087543352594\n",
      "MODEL LOSS 88.31549\n",
      "AUTOENCODER LOSS 1.4952728\n",
      "MSE (non zero, train set): 0.46458087543352594\n",
      "MAE (non zero, train set): 0.34449341813988466\n",
      "MSE (test set): 1.1292965075993975\n",
      "MAE (test set): 0.8605404783890671\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 20}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 35}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.1794705535103365\n",
      "MODEL LOSS 65453.92\n",
      "AUTOENCODER LOSS 7.5408607\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.8803127274899804\n",
      "MODEL LOSS 33356.523\n",
      "AUTOENCODER LOSS 2.3858185\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.5368589977962372\n",
      "MODEL LOSS 16689.791\n",
      "AUTOENCODER LOSS 1.6789513\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.48838133069462075\n",
      "MODEL LOSS 8038.8667\n",
      "AUTOENCODER LOSS 1.5625753\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.4748342386531798\n",
      "MODEL LOSS 3686.7107\n",
      "AUTOENCODER LOSS 1.5379872\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.46879536547622774\n",
      "MODEL LOSS 1602.5703\n",
      "AUTOENCODER LOSS 1.5328249\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.46516129777285486\n",
      "MODEL LOSS 668.8601\n",
      "AUTOENCODER LOSS 1.5319549\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.4630714303617807\n",
      "MODEL LOSS 282.82108\n",
      "AUTOENCODER LOSS 1.5309366\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.4615576419206827\n",
      "MODEL LOSS 136.67406\n",
      "AUTOENCODER LOSS 1.5275903\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.4604328328612535\n",
      "MODEL LOSS 85.49452\n",
      "AUTOENCODER LOSS 1.5266479\n",
      "MSE (non zero, train set): 0.4604328328612535\n",
      "MAE (non zero, train set): 0.3401110628446858\n",
      "MSE (test set): 1.130590169915485\n",
      "MAE (test set): 0.8612118955153655\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 35}\n",
      "Start testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 50}\n",
      "Mask shape: (10668, 10000)\n",
      "Noising completed..:(10668, 10000)\n",
      "EPOCH 1 / 10\n",
      "ALS LOSS 3.1854573651256506\n",
      "MODEL LOSS 93538.664\n",
      "AUTOENCODER LOSS 3.5443847\n",
      "EPOCH 2 / 10\n",
      "ALS LOSS 0.8915553941342146\n",
      "MODEL LOSS 47662.88\n",
      "AUTOENCODER LOSS 1.7024859\n",
      "EPOCH 3 / 10\n",
      "ALS LOSS 0.5406575317525197\n",
      "MODEL LOSS 23832.586\n",
      "AUTOENCODER LOSS 1.570195\n",
      "EPOCH 4 / 10\n",
      "ALS LOSS 0.48754556831434487\n",
      "MODEL LOSS 11457.323\n",
      "AUTOENCODER LOSS 1.5520651\n",
      "EPOCH 5 / 10\n",
      "ALS LOSS 0.4733686977281646\n",
      "MODEL LOSS 5230.996\n",
      "AUTOENCODER LOSS 1.5485593\n",
      "EPOCH 6 / 10\n",
      "ALS LOSS 0.46753343913210066\n",
      "MODEL LOSS 2250.742\n",
      "AUTOENCODER LOSS 1.5477335\n",
      "EPOCH 7 / 10\n",
      "ALS LOSS 0.4640818235245687\n",
      "MODEL LOSS 917.1863\n",
      "AUTOENCODER LOSS 1.5482117\n",
      "EPOCH 8 / 10\n",
      "ALS LOSS 0.46189306983535117\n",
      "MODEL LOSS 368.05753\n",
      "AUTOENCODER LOSS 1.5463332\n",
      "EPOCH 9 / 10\n",
      "ALS LOSS 0.46048091013763215\n",
      "MODEL LOSS 161.60411\n",
      "AUTOENCODER LOSS 1.544639\n",
      "EPOCH 10 / 10\n",
      "ALS LOSS 0.4591908296425465\n",
      "MODEL LOSS 90.140366\n",
      "AUTOENCODER LOSS 1.5441979\n",
      "MSE (non zero, train set): 0.4591908296425465\n",
      "MAE (non zero, train set): 0.3383943581687919\n",
      "MSE (test set): 1.1341768572778297\n",
      "MAE (test set): 0.8658015083048138\n",
      "Srop testing hyper params:  {'k': 50, 'lambda_u': 0.1, 'lambda_v': 100, 'hidden_size': 150, 'drop_ratio': 0.1, 'matrix_noise': 0.3, 'lambda_n': 0.1, 'lambda_w': 50}\n"
     ]
    }
   ],
   "source": [
    "write_row('./optimiz/cdl_sdae_7_10epochs.csv', sorted_keys + ['mse_train', 'mae_train', 'mse_test', 'mae_test'])\n",
    "for ps in combinations:\n",
    "    tf.reset_default_graph()\n",
    "    hyper_params = dict(zip(sorted_keys, ps))\n",
    "    \n",
    "    print(\"Start testing hyper params: \", hyper_params)\n",
    "    cdl = CDL(rating_matrix, item_infomation_matrix, out_path=None, epochs=10, **hyper_params)\n",
    "    U, V, beta_u, beta_v = cdl.training() #188910\n",
    "    \n",
    "    predictions = np.dot(U, V.T) + beta_u.reshape(-1, 1) + beta_v.reshape(1, -1)\n",
    "    \n",
    "    # predictions on train set\n",
    "    train_mse = mean_squared_error(rating_matrix[rating_matrix > 0], predictions[rating_matrix > 0]) ** 0.5\n",
    "    train_mae = mean_absolute_error(rating_matrix[rating_matrix > 0], predictions[rating_matrix > 0])\n",
    "    \n",
    "    print(\"MSE (non zero, train set): %s\" % train_mse)\n",
    "    print(\"MAE (non zero, train set): %s\" % train_mae)\n",
    "    \n",
    "    # predictions on test set\n",
    "    preds_df_unmelt = pd.DataFrame(predictions, columns = pivoted.columns, index = pivoted.index)\n",
    "    preds_df_unmelt.index.name = 'reviewerID'\n",
    "    preds_df_unmelt.columns.name = 'asin'\n",
    "    \n",
    "    df_test_val['value'] = df_test_val.apply(get_val, axis = 1)\n",
    "    mse = mean_squared_error(df_test[~df_test_val.value.isnull()].overall, df_test_val[~df_test_val.value.isnull()].value) ** 0.5\n",
    "    mae = mean_absolute_error(df_test[~df_test_val.value.isnull()].overall, df_test_val[~df_test_val.value.isnull()].value)\n",
    "    \n",
    "    del preds_df_unmelt\n",
    "    del predictions\n",
    "    \n",
    "    print(\"MSE (test set): %s\" % mse)\n",
    "    print(\"MAE (test set): %s\" % mae)\n",
    "    \n",
    "    print(\"Srop testing hyper params: \", hyper_params)\n",
    "    \n",
    "    # write to file\n",
    "    write_row('./optimiz/cdl_sdae_7_10epochs.csv', [hyper_params[k] for k in sorted_keys] + [train_mse, train_mae, mse, mae] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
